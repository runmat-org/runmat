{
  "builtin": "rand",
  "model": null,
  "prompt": "Builtin: rand\nPreferred Codex model: gpt-5-codex\n\n===== docs/ARCHITECTURE.md =====\n# RunMat Architecture\n\nRunMat is a high-performance, MATLAB-compatible runtime built in Rust. Its design is heavily inspired by modern high-performance language runtimes like the V8 JavaScript engine, adapting concepts like tiered execution and snapshotting to the specific needs of a numerical computing environment.\n\nThis document provides a comprehensive overview of the RunMat architecture. It is intended for current and future contributors who wish to understand the internal workings of the system, its design principles, and how its various components fit together.\n\n## Core Philosophy\n\nThe architecture is guided by a few key principles:\n\n1.  **Performance from the Ground Up**: Every component is designed with performance in mind. This includes a tiered JIT compiler, a high-performance generational garbage collector, zero-copy data structures where possible, and optional integration with native BLAS/LAPACK libraries.\n2.  **Safety and Modularity**: By leveraging Rust's safety guarantees and a highly modular crate-based architecture, we aim for a robust and maintainable system. Each core component (parser, interpreter, JIT, GC) is an independent crate, enabling focused development and testing.\n3.  **V8-Inspired Tiered Execution**: We believe that the optimal execution strategy for a dynamic language involves multiple tiers. Code begins execution in a simple, fast-to-start interpreter. \"Hot\" code paths are then identified and promoted to an optimizing JIT compiler for native-level performance.\n4.  **Fast Startup**: A key pain point of traditional MATLAB environments is slow startup time. RunMat addresses this with a sophisticated snapshotting system that pre-compiles and serializes the entire standard library into a binary blob that can be loaded into memory nearly instantaneously.\n5.  **Excellent Ergonomics**: From the world-class interactive plotting library to the helpful REPL and comprehensive configuration system, the goal is to provide a powerful and pleasant user experience.\n\n---\n\n## The Execution Pipeline: A Code's Journey\n\nThe lifecycle of a piece of MATLAB code in RunMat involves a multi-stage pipeline, transforming it from a raw string into optimized native machine code.\n\n```mermaid\ngraph TD\n    A[MATLAB Code String] -->|runmat-lexer| B(Token Stream);\n    B -->|runmat-parser| C(Abstract Syntax Tree - AST);\n    C -->|runmat-hir| D(High-level IR - HIR);\n    D -->|runmat-ignition| E(Bytecode);\n    E -->|Ignition Interpreter| F[Execution & Profiling];\n    F -->|Hot Code via runmat-turbine| G(Cranelift IR);\n    G -->|Turbine JIT Compiler| H[Native Machine Code];\n    H -->|JIT Execution| I[Optimized Execution];\n    F -->|Cold Code| J[Result];\n    I --> J;\n\n    subgraph \"1. Frontend Parsing\"\n        A\n        B\n        C\n    end\n\n    subgraph \"2. Semantic Analysis\"\n        D\n    end\n\n    subgraph \"3. Tier-1: Ignition (Interpreter)\"\n        E\n        F\n    end\n\n    subgraph \"4. Tier-2: Turbine (JIT)\"\n        G\n        H\n        I\n    end\n```\n\n### 1. Frontend: Lexer & Parser (`runmat-lexer`, `runmat-parser`)\n\n-   **Lexing**: The process begins in `runmat-lexer`. Using the `logos` library, the raw source code string is broken down into a stream of `Token`s. This stage is highly optimized for raw speed, handling keywords, operators, literals, and stripping comments and whitespace.\n-   **Parsing**: The `runmat-parser` crate consumes the token stream and constructs an **Abstract Syntax Tree (AST)**. The AST is a direct, hierarchical representation of the code's structure, but it lacks semantic understanding.\n\n### 2. Semantic Analysis: High-Level IR (`runmat-hir`)\n\nThe AST is \"lowered\" into a **High-level Intermediate Representation (HIR)**. This is a critical step for semantic analysis in a language like MATLAB, which has complex scoping rules.\n-   **Variable Resolution**: The `runmat-hir` crate walks the AST and resolves all variable identifiers. It builds and manages scopes (global, function, loop), replacing string-based names with unique `VarId`s. This ensures that every variable access is unambiguous from this point forward.\n-   **Basic Type Inference**: While the system is dynamic, the HIR pass performs a preliminary type inference, annotating expressions with types like `Scalar` or `Matrix`. This provides early information for later optimization stages.\n\n### 3. Tier-1 Execution: Ignition (`runmat-ignition`)\n\nIgnition is the baseline execution engine. It serves two roles:\n-   **Compiler**: It takes the HIR and compiles it into a simple, portable **Bytecode**.\n-   **Interpreter**: It can directly execute this bytecode. All code begins its life running in the Ignition interpreter. The interpreter is designed for fast startup and low overhead.\n\n### 4. Tier-2 Execution: Turbine (`runmat-turbine`)\n\nTurbine is the optimizing JIT (Just-In-Time) compiler, forming the top performance tier.\n-   **Hotspot Profiling**: As code runs in Ignition, the `HotspotProfiler` in Turbine tracks the execution frequency of bytecode sequences.\n-   **JIT Compilation**: When a function or loop is identified as \"hot\" (i.e., executed many times), Turbine's `BytecodeCompiler` takes over. It translates the hot bytecode into **Cranelift IR**, a machine-independent representation.\n-   **Optimization & Code Generation**: Cranelift performs numerous optimizations on this IR before compiling it down to native machine code for the host architecture (e.g., x86-64, AArch64).\n-   **Optimized Execution**: The resulting native code is cached. Subsequent calls to the hot function will bypass the interpreter entirely and execute the highly-optimized machine code directly, yielding significant performance gains. This tiered approach ensures that RunMat spends time optimizing only the code that matters most.\n\n---\n\n## Memory Management: `runmat-gc`\n\nRunMat features a sophisticated, high-performance, generational garbage collector. This avoids the manual memory management pitfalls of languages like C++ and provides better performance for numerical workloads than simple reference counting.\n\n-   **Generational Collection**: The heap is divided into multiple generations. New, short-lived objects are allocated in a \"young\" generation, which is collected frequently and quickly (a minor GC). Objects that survive multiple minor GCs are promoted to an \"old\" generation, which is collected less frequently (a major GC). This strategy is highly effective for typical MATLAB workloads where many temporary matrices are created and quickly discarded.\n-   **Safe Handles (`GcPtr<T>`, `GcHandle`)**: To ensure safety and prevent dangling pointers, the GC system uses safe handles. The `HighPerformanceGC` employs a central object table, and user code interacts with objects via a `GcHandle` (a unique ID). This handle-based system is safer than raw pointers, as the GC can move objects in memory without invalidating user code.\n-   **Write Barriers**: To efficiently track pointers from the old generation to the young generation, the GC implements write barriers. This is crucial for ensuring that a minor GC can run without scanning the entire old-generation heap.\n-   **Configurability**: The GC is highly tunable through `GcConfig`, with presets for different workloads like `LowLatency` and `HighThroughput`.\n\n---\n\n## The Runtime and Standard Library\n\nThe power of RunMat comes from its extensive and performant standard library, which is managed by a trio of crates.\n\n```mermaid\ngraph TD\n    A[\"MATLAB Code: y = sin(x)\"] --> B{Execution Engine}\n    B -->|call builtin sin| C{Runtime Dispatch}\n    C -->|inventory lookup| D[\"Builtin Registry\"]\n    D -->|finds sin| E[\"sin_builtin (runtime)\"]\n    subgraph Crates\n        F[\"runmat-macros: runtime_builtin macro\"]\n        G[\"runmat-builtins: Value & Builtin types\"]\n        H[\"runmat-runtime: function implementations\"]\n    end\n    E -->|implemented in| H\n    E -->|registered via| F\n    F -->|uses| G\n```\n\n-   **`runmat-builtins`**: This is the foundational crate. It defines the core `Value` enum, which represents all possible data types in the RunMat runtime (integers, floats, strings, matrices, etc.). It also defines the `Builtin` struct and uses the `inventory` crate to create a global, self-registering list of all available standard library functions.\n-   **`runmat-macros`**: To make extending the standard library trivial, this crate provides the `#[runtime_builtin]` procedural macro. A developer can simply write a standard Rust function that operates on native types (e.g., `fn sin(x: f64) -> f64`) and annotate it with `#[runtime_builtin(name = \"sin\")]`. The macro automatically generates the wrapper code to handle `Value` type conversions and registers the function with the `inventory` system.\n-   **`runmat-runtime`**: This is where the actual implementations of the standard library functions reside. It includes:\n    -   Basic mathematical functions.\n    -   Matrix manipulation and comparison operators.\n    -   Wrappers for the plotting library.\n    -   **BLAS/LAPACK Integration**: Through the optional `blas-lapack` feature, this crate provides high-performance linear algebra operations by linking against native libraries like Apple's Accelerate framework or OpenBLAS.\n\n---\n\n## Fast Startup: `runmat-snapshot`\n\nThe snapshot system is a key architectural feature for providing a fast, responsive user experience.\n\n-   **The Problem**: A rich standard library requires parsing and compiling hundreds of `.m` files, which can lead to slow startup times.\n-   **The Solution**: At build time, the `SnapshotBuilder` executes the entire pipeline for all standard library components—parsing, HIR generation, and bytecode compilation.\n-   **Serialization**: The resulting state (HIR, bytecode, builtin metadata, etc.) is serialized into a single, optimized binary file (`.snapshot`).\n-   **Fast Loading**: At runtime, the `SnapshotLoader` can load this binary blob. By using memory-mapping (`mmap`), the snapshot can be brought into the process's address space almost instantly, with zero copy overhead. The runtime can then use the pre-compiled components directly, skipping the entire parsing and compilation pipeline for the standard library and achieving startup times in milliseconds.\n\n---\n\n## Visualization: `runmat-plot`\n\nRunMat includes a \"world-class\" interactive plotting library designed to rival and exceed the capabilities of MATLAB's Handle Graphics.\n\n-   **GPU-Accelerated**: The entire rendering pipeline is built on `wgpu`, a modern, cross-platform graphics API. This provides smooth, high-performance rendering for complex 2D and 3D scenes.\n-   **Layered Architecture**:\n    -   **Core (`renderer`, `scene`, `camera`)**: A low-level engine that manages `wgpu` pipelines, a scene graph for organizing renderable objects, and an interactive camera.\n    -   **Plots (`LinePlot`, `SurfacePlot`, etc.)**: High-level, user-facing structs that encapsulate the data and styling for a specific plot type.\n    -   **GUI (`PlotWindow`, `PlotOverlay`)**: An interactive window built with `winit` and `egui` that provides camera controls, zooming, panning, and UI overlays like axes, grids, and titles.\n-   **MATLAB-Compatible API**: Provides a familiar API (`plot()`, `scatter()`, `surf()`) for ease of use.\n-   **Jupyter Integration**: Can render plots as static images or interactive HTML widgets directly within Jupyter notebooks.\n-   **Modern Theming**: A professional and configurable styling system with beautiful presets like `ModernDark`.\n\n---\n\n## Contributing to RunMat\n\nThe modular architecture is designed to make contributions straightforward. Here are some common ways to extend the system:\n\n-   **Adding a New Builtin Function**:\n    1.  Navigate to `crates/runmat-runtime`.\n    2.  Find the appropriate module (e.g., `matrix.rs`, `blas.rs`).\n    3.  Write a standard Rust function that takes and returns native Rust types.\n    4.  Add the `#[runtime_builtin(name = \"your_matlab_name\")]` attribute.\n    5.  The macro and `inventory` system will handle the rest.\n\n-   **Improving the JIT Compiler**:\n    1.  The core logic is in `crates/runmat-turbine/src/compiler.rs`.\n    2.  Focus on the `compile_instructions` function, which translates `runmat_ignition::Instr` into Cranelift IR.\n    3.  You can add new instruction handlers or optimize existing ones by emitting more efficient Cranelift IR sequences. For example, recognizing patterns that can be mapped to specific machine instructions (e.g., fused multiply-add).\n\n-   **Adding a New Plot Type**:\n    1.  Create a new file in `crates/runmat-plot/src/plots/`, e.g., `my_plot.rs`.\n    2.  Define a struct `MyPlot` that holds the plot's data and styling options.\n    3.  Implement a `render_data(&mut self) -> RenderData` method on your struct. This method is responsible for generating the `Vec<Vertex>` and `Vec<u32>` (if using an index buffer) that the `wgpu` renderer will consume.\n    4.  Add your plot to the `PlotElement` enum in `crates/runmat-plot/src/plots/figure.rs` and add a corresponding `add_my_plot` method to the `Figure` struct.\n\n===== crates/runmat-accelerate/README.md =====\n## RunMat Accelerate\n\n### Purpose\n`runmat-accelerate` provides the high-level acceleration layer that integrates GPU backends with the language runtime. It implements provider(s) for `runmat-accelerate-api` so that `gpuArray`, `gather`, and (todo) accelerated math and linear algebra can execute on devices transparently where appropriate.\n\n### Architecture\n- Depends on `runmat-accelerate-api` to register an `AccelProvider` implementation at startup.\n- Backends (e.g., `wgpu`, `cuda`, `rocm`, `metal`, `vulkan`, `opencl`) are feature-gated. Only one provider is registered globally, but a future multi-device planner can fan out.\n- `Planner` decides when to run ops on CPU vs GPU (size thresholds, op types, fusion opportunities). `Accelerator` exposes ergonomic entry points used by the runtime or higher layers.\n\n### Autograd and default optimization (planned)\n- Tensor/Matrix operations will participate in reverse-mode autograd by default. The runtime records a compact tape of primitive ops; gradients are computed by chaining primitive derivatives (no provider changes required).\n- The planner and JIT will fuse common elementwise chains and simple BLAS sequences to reduce temporaries and host↔device transfers automatically.\n- For providers that expose fused kernels, the planner can route differentiated graphs to those paths, improving both forward and backward performance.\n\n### What it provides today\n- A scaffolding `Accelerator` with elementwise add routing: choose CPU path (delegating to `runmat-runtime`) or GPU path (via provider methods). The GPU path currently uses upload/compute/download placeholders and is ready to be backed by a real backend.\n- Integration points for `gpuArray`/`gather`: when a provider is registered, runtime builtins route through the provider API defined in `runmat-accelerate-api`.\n\n### How it fits with the runtime\n- The MATLAB-facing builtins (`gpuArray`, `gather`) live in `runmat-runtime` for consistency with all other builtins. They call into `runmat-accelerate-api::provider()`, which is implemented and registered by this crate.\n- This separation avoids dependency cycles and keeps the language surface centralized while enabling pluggable backends.\n\n### Backends\n- `wgpu` (feature: `wgpu`) is the first cross-vendor target. CUDA/ROCm/Metal/Vulkan/OpenCL are planned (features already stubbed).\n- Backend responsibilities:\n  - Allocate/free buffers, handle host↔device transfers\n  - Provide kernels for core ops (elementwise, transpose, matmul/GEMM)\n  - Report device information (for planner decisions)\n\n### Current state\n- Compiles and wires through to the runtime via the API layer.\n- CPU fallback path fully functional; GPU path ready for provider implementation.\n\n### Roadmap\n- Implement an in-process provider with a buffer registry (proof-of-concept) to make `gpuArray`/`gather` round-trip actual data without copying through a real device yet.\n- Implement first real backend (likely `wgpu`): upload/download, elementwise add/mul/div/pow, transpose, matmul, with simple planner thresholds.\n- Add streams/queues, memory pools, pinned/unified buffers, and multi-device support.\n- Planner cost model and operator fusion (elementwise chains and simple BLAS fusions).\n\n### Example usage\nThe provider is registered at process startup (REPL/CLI/app). Once registered, MATLAB-like code can use:\n```matlab\nG = gpuArray(A);      % move tensor to device\nH = G + 2;            % elementwise add (planner may choose GPU path)\nR = gather(H);        % bring results back to host\n```\n\n### Native acceleration\n\nRunMat will power native acceleration for RunMat. It lets users avoid needing to use the gpuArray/gather builtins and instead use the native acceleration API:\n\n```matlab\n% Example: Large matrix multiplication and elementwise operations\nA = randn(10000, 10000);   % Large matrix\nB = randn(10000, 10000);\n\n% Normally, in MATLAB you'd need to explicitly use gpuArray:\n%   G = gpuArray(A);\n%   H = G .* B;           % Elementwise multiply on GPU\n%   S = sum(H, 2);\n%   R = gather(S);\n\n% With RunMat accelerate, the planner can transparently move data to the GPU\n% and back as needed, so you can just write:\nH = A .* B;           % Planner may choose GPU for large ops\nS = sum(H, 2);        % Fused and executed on device if beneficial\n\n% Results are automatically brought back to host as needed\ndisp(S(1:10));        % Print first 10 results\n\n% The planner/JIT will optimize transfers and fuse operations for best performance.\n\n```\n\n### Device info (gpuDevice)\n- `gpuDevice()` returns a structured value with details about the active provider/device when available. Fields include `device_id`, `name`, `vendor`, optional `memory_bytes`, and optional `backend`.\n\n```matlab\ninfo = gpuDevice();\n% Example output (in-process provider):\n%   struct with fields:\n%       device_id: 0\n%            name: 'InProcess'\n%          vendor: 'RunMat'\n%      backend: 'inprocess'\n```\n\n===== crates/runmat-accelerate/src/lib.rs =====\n//! RunMat Accelerate: GPU Acceleration Abstraction Layer\n//!\n//! Goals:\n//! - Provide a backend-agnostic API surface that maps RunMat operations to GPU kernels.\n//! - Support multiple backends via features (CUDA, ROCm, Metal, Vulkan, OpenCL, wgpu).\n//! - Allow zero-copy interop with `runmat-builtins::Matrix` where possible.\n//! - Defer actual kernel authoring to backend crates/modules; this crate defines traits and wiring.\n//!\n//! This is scaffolding only; implementations will land after interpreter/JIT semantics are complete.\n\nuse once_cell::sync::Lazy;\nuse runmat_builtins::{Tensor, Value};\nuse std::path::PathBuf;\nuse std::sync::RwLock;\n\npub mod fusion;\npub mod fusion_exec;\npub mod fusion_residency;\npub mod graph;\npub mod native_auto;\npub mod simple_provider;\n#[cfg(feature = \"wgpu\")]\npub mod wgpu_backend;\npub use fusion::*;\npub use graph::*;\npub use native_auto::{\n    is_sink, prepare_builtin_args, promote_binary, promote_reduction_args, promote_unary, BinaryOp,\n    ReductionOp, UnaryOp,\n};\n#[cfg(feature = \"wgpu\")]\nuse runmat_accelerate_api::AccelProvider;\nuse serde::{Deserialize, Serialize};\n#[cfg(feature = \"wgpu\")]\nuse wgpu::PowerPreference;\n\n/// Preferred acceleration provider selection\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"kebab-case\")]\npub enum AccelerateProviderPreference {\n    Auto,\n    Wgpu,\n    InProcess,\n}\n\nimpl Default for AccelerateProviderPreference {\n    fn default() -> Self {\n        Self::Auto\n    }\n}\n\n/// Power preference used when initializing a WGPU backend\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"kebab-case\")]\npub enum AccelPowerPreference {\n    Auto,\n    HighPerformance,\n    LowPower,\n}\n\nimpl Default for AccelPowerPreference {\n    fn default() -> Self {\n        Self::Auto\n    }\n}\n\n/// Logging verbosity for auto-offload promotion decisions.\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"kebab-case\")]\npub enum AutoOffloadLogLevel {\n    Off,\n    Info,\n    Trace,\n}\n\nimpl Default for AutoOffloadLogLevel {\n    fn default() -> Self {\n        AutoOffloadLogLevel::Trace\n    }\n}\n\n/// Configuration passed to the native auto-offload planner.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AutoOffloadOptions {\n    pub enabled: bool,\n    pub calibrate: bool,\n    #[serde(default)]\n    pub profile_path: Option<PathBuf>,\n    #[serde(default)]\n    pub log_level: AutoOffloadLogLevel,\n}\n\nimpl Default for AutoOffloadOptions {\n    fn default() -> Self {\n        Self {\n            enabled: true,\n            calibrate: true,\n            profile_path: None,\n            log_level: AutoOffloadLogLevel::Trace,\n        }\n    }\n}\n\nstatic AUTO_OFFLOAD_OPTIONS: Lazy<RwLock<AutoOffloadOptions>> =\n    Lazy::new(|| RwLock::new(AutoOffloadOptions::default()));\n\nstatic RESIDENCY_HOOKS: Lazy<()> = Lazy::new(|| {\n    runmat_accelerate_api::register_residency_clear(fusion_residency::clear);\n});\n\npub(crate) fn ensure_residency_hooks() {\n    Lazy::force(&RESIDENCY_HOOKS);\n}\n\npub fn configure_auto_offload(options: AutoOffloadOptions) {\n    if let Ok(mut guard) = AUTO_OFFLOAD_OPTIONS.write() {\n        *guard = options;\n    }\n}\n\npub(crate) fn auto_offload_options() -> AutoOffloadOptions {\n    AUTO_OFFLOAD_OPTIONS\n        .read()\n        .map(|guard| guard.clone())\n        .unwrap_or_default()\n}\n\n/// Initialization options for selecting and configuring the acceleration provider.\n#[derive(Debug, Clone)]\npub struct AccelerateInitOptions {\n    pub enabled: bool,\n    pub provider: AccelerateProviderPreference,\n    pub allow_inprocess_fallback: bool,\n    pub wgpu_power_preference: AccelPowerPreference,\n    pub wgpu_force_fallback_adapter: bool,\n    pub auto_offload: AutoOffloadOptions,\n}\n\nimpl Default for AccelerateInitOptions {\n    fn default() -> Self {\n        Self {\n            enabled: true,\n            provider: AccelerateProviderPreference::Auto,\n            allow_inprocess_fallback: true,\n            wgpu_power_preference: AccelPowerPreference::Auto,\n            wgpu_force_fallback_adapter: false,\n            auto_offload: AutoOffloadOptions::default(),\n        }\n    }\n}\n\n/// Initialize the global acceleration provider using the supplied options.\npub fn initialize_acceleration_provider_with(options: &AccelerateInitOptions) {\n    configure_auto_offload(options.auto_offload.clone());\n\n    if runmat_accelerate_api::provider().is_some() {\n        return;\n    }\n\n    if !options.enabled {\n        if options.allow_inprocess_fallback {\n            simple_provider::register_inprocess_provider();\n            log::info!(\n                \"RunMat Accelerate: acceleration disabled; using in-process provider for compatibility\"\n            );\n        } else {\n            log::info!(\"RunMat Accelerate: acceleration disabled; no provider registered\");\n        }\n        return;\n    }\n\n    #[allow(unused_mut)]\n    let mut registered = false;\n\n    #[cfg(feature = \"wgpu\")]\n    {\n        if !registered\n            && matches!(\n                options.provider,\n                AccelerateProviderPreference::Auto | AccelerateProviderPreference::Wgpu\n            )\n        {\n            let wgpu_options = wgpu_backend::WgpuProviderOptions {\n                power_preference: match options.wgpu_power_preference {\n                    AccelPowerPreference::Auto => PowerPreference::HighPerformance,\n                    AccelPowerPreference::HighPerformance => PowerPreference::HighPerformance,\n                    AccelPowerPreference::LowPower => PowerPreference::LowPower,\n                },\n                force_fallback_adapter: options.wgpu_force_fallback_adapter,\n            };\n\n            match wgpu_backend::register_wgpu_provider(wgpu_options) {\n                Ok(provider) => {\n                    registered = true;\n                    let info = provider.device_info_struct();\n                    let backend = info.backend.as_deref().unwrap_or(\"unknown\");\n                    log::info!(\n                        \"RunMat Accelerate: using WGPU provider {} (vendor: {}, backend: {})\",\n                        info.name,\n                        info.vendor,\n                        backend\n                    );\n                }\n                Err(err) => {\n                    log::warn!(\n                        \"RunMat Accelerate: failed to initialize WGPU provider, falling back: {err}\"\n                    );\n                }\n            }\n        }\n    }\n\n    #[cfg(not(feature = \"wgpu\"))]\n    {\n        if matches!(options.provider, AccelerateProviderPreference::Wgpu) {\n            log::warn!(\n                \"RunMat Accelerate: WGPU provider requested but crate built without 'wgpu' feature\"\n            );\n        }\n    }\n\n    if !registered {\n        if options.allow_inprocess_fallback\n            || matches!(options.provider, AccelerateProviderPreference::InProcess)\n        {\n            simple_provider::register_inprocess_provider();\n            log::info!(\"RunMat Accelerate: using in-process acceleration provider\");\n        } else {\n            log::warn!(\"RunMat Accelerate: no acceleration provider registered\");\n        }\n    }\n}\n\n/// Initialize the acceleration provider using default options.\npub fn initialize_acceleration_provider() {\n    initialize_acceleration_provider_with(&AccelerateInitOptions::default());\n}\n\n/// High-level device kind. Concrete selection is provided by backend.\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum DeviceKind {\n    Cpu,\n    Cuda,\n    Rocm,\n    Metal,\n    Vulkan,\n    OpenCl,\n    Wgpu,\n}\n\n/// Device descriptor used for selection and capabilities query.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeviceInfo {\n    pub kind: DeviceKind,\n    pub name: String,\n    pub vendor: String,\n    pub memory_bytes: Option<u64>,\n    pub compute_capability: Option<String>,\n}\n\n/// Abstract buffer that may reside on device or be host-pinned.\npub trait BufferHandle: Send + Sync {\n    fn len(&self) -> usize;\n    fn is_empty(&self) -> bool {\n        self.len() == 0\n    }\n}\n\n/// Abstract matrix allocated on a device backend.\npub trait DeviceMatrix: Send + Sync {\n    fn rows(&self) -> usize;\n    fn cols(&self) -> usize;\n    fn as_buffer(&self) -> &dyn BufferHandle;\n}\n\n/// Core backend interface that concrete backends must implement.\npub trait AccelerateBackend: Send + Sync {\n    fn device_info(&self) -> DeviceInfo;\n\n    // Memory\n    fn upload_matrix(&self, host: &Tensor) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn download_matrix(&self, dev: &dyn DeviceMatrix) -> anyhow::Result<Tensor>;\n\n    // Elementwise\n    fn elem_add(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn elem_sub(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn elem_mul(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn elem_div(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn elem_pow(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n\n    // Linear algebra (future): matmul, transpose, BLAS/LAPACK analogs\n    fn matmul(\n        &self,\n        a: &dyn DeviceMatrix,\n        b: &dyn DeviceMatrix,\n    ) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n    fn transpose(&self, a: &dyn DeviceMatrix) -> anyhow::Result<Box<dyn DeviceMatrix>>;\n}\n\n/// Planner determines whether to execute on CPU or a selected backend.\n/// This will eventually consult sizes, heuristics, and device availability.\n#[derive(Default)]\npub struct Planner {\n    backend: Option<Box<dyn AccelerateBackend>>,\n}\n\nimpl Planner {\n    pub fn new(backend: Option<Box<dyn AccelerateBackend>>) -> Self {\n        Self { backend }\n    }\n\n    pub fn device(&self) -> Option<&dyn AccelerateBackend> {\n        self.backend.as_deref()\n    }\n\n    /// Example decision hook: execute elementwise add on GPU if large enough.\n    pub fn choose_elem_add(&self, a: &Tensor, b: &Tensor) -> ExecutionTarget {\n        if let Some(bk) = &self.backend {\n            if a.data.len() >= 1 << 16 && a.rows() == b.rows() && a.cols() == b.cols() {\n                return ExecutionTarget::Gpu(bk.device_info());\n            }\n        }\n        ExecutionTarget::Cpu\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ExecutionTarget {\n    Cpu,\n    Gpu(DeviceInfo),\n}\n\n/// High-level façade for accelerated operations, falling back to `runmat-runtime`.\npub struct Accelerator {\n    planner: Planner,\n}\n\nimpl Accelerator {\n    pub fn new(planner: Planner) -> Self {\n        Self { planner }\n    }\n\n    pub fn elementwise_add(&self, a: &Value, b: &Value) -> anyhow::Result<Value> {\n        match (a, b) {\n            (Value::Tensor(ma), Value::Tensor(mb)) => match self.planner.choose_elem_add(ma, mb) {\n                ExecutionTarget::Cpu => {\n                    runmat_runtime::elementwise_add(a, b).map_err(|e| anyhow::anyhow!(e))\n                }\n                ExecutionTarget::Gpu(_) => {\n                    let bk = self\n                        .planner\n                        .device()\n                        .ok_or_else(|| anyhow::anyhow!(\"no backend\"))?;\n                    let da = bk.upload_matrix(ma)?;\n                    let db = bk.upload_matrix(mb)?;\n                    let dc = bk.elem_add(da.as_ref(), db.as_ref())?;\n                    let out = bk.download_matrix(dc.as_ref())?;\n                    Ok(Value::Tensor(out))\n                }\n            },\n            (Value::GpuTensor(ga), Value::GpuTensor(gb)) => {\n                // Placeholder: assume same device; in practice look up buffers by id\n                // Fallback to CPU until device registry is implemented\n                let ha = self.gather_handle(ga)?;\n                let hb = self.gather_handle(gb)?;\n                self.elementwise_add(&ha, &hb)\n            }\n            (Value::GpuTensor(ga), other) => {\n                let ha = self.gather_handle(ga)?;\n                self.elementwise_add(&ha, other)\n            }\n            (other, Value::GpuTensor(gb)) => {\n                let hb = self.gather_handle(gb)?;\n                self.elementwise_add(other, &hb)\n            }\n            _ => runmat_runtime::elementwise_add(a, b).map_err(|e| anyhow::anyhow!(e)),\n        }\n    }\n\n    fn gather_handle(&self, h: &runmat_accelerate_api::GpuTensorHandle) -> anyhow::Result<Value> {\n        if let Some(p) = runmat_accelerate_api::provider() {\n            let ht = p.download(h).map_err(|e| anyhow::anyhow!(e))?;\n            let t = Tensor::new(ht.data, ht.shape).map_err(|e| anyhow::anyhow!(e))?;\n            Ok(Value::Tensor(t))\n        } else {\n            // Fallback to zeros with same shape if no provider is registered\n            let shape = h.shape.clone();\n            let total: usize = shape.iter().product();\n            let zeros = Tensor::new(vec![0.0; total], shape).map_err(|e| anyhow::anyhow!(e))?;\n            Ok(Value::Tensor(zeros))\n        }\n    }\n}\n\n===== crates/runmat-accelerate/src/native_auto.rs =====\nuse std::collections::HashMap;\nuse std::env;\nuse std::fs;\nuse std::path::PathBuf;\nuse std::time::{Duration, Instant};\n\nuse crate::{auto_offload_options, fusion::active_fusion, fusion_residency, AutoOffloadLogLevel};\nuse anyhow::{anyhow, Result};\nuse log::{debug, info, trace};\nuse once_cell::sync::OnceCell;\nuse runmat_accelerate_api::{AccelProvider, HostTensorView};\nuse runmat_builtins::{builtin_functions, AccelTag, Tensor, Value};\nuse runmat_runtime::gather_if_needed;\nuse serde::Deserialize;\n\nconst DEFAULT_CPU_ELEM_PER_ELEM: f64 = 1.0e-7;\nconst DEFAULT_CPU_REDUCTION_PER_ELEM: f64 = 1.2e-7;\nconst DEFAULT_CPU_MATMUL_PER_FLOP: f64 = 2.5e-11;\n\n#[derive(Clone, Copy, Debug)]\npub enum BinaryOp {\n    Elementwise,\n    MatMul,\n}\n\n#[derive(Clone, Copy, Debug)]\npub enum UnaryOp {\n    Generic,\n    Transpose,\n}\n\n#[derive(Clone, Copy, Debug)]\npub enum ReductionOp {\n    Sum,\n    Mean,\n    Min,\n    Max,\n}\n\n#[derive(Debug, Clone)]\nstruct ThresholdConfig {\n    unary_min_elems: usize,\n    binary_min_elems: usize,\n    reduction_min_elems: usize,\n    matmul_min_flops: usize,\n    cpu_elem_per_elem: f64,\n    cpu_reduction_per_elem: f64,\n    cpu_matmul_per_flop: f64,\n}\n\nimpl Default for ThresholdConfig {\n    fn default() -> Self {\n        Self {\n            unary_min_elems: 4_096,\n            binary_min_elems: 4_096,\n            reduction_min_elems: 4_096,\n            matmul_min_flops: 1_000_000, // roughly 100x100x100\n            cpu_elem_per_elem: DEFAULT_CPU_ELEM_PER_ELEM,\n            cpu_reduction_per_elem: DEFAULT_CPU_REDUCTION_PER_ELEM,\n            cpu_matmul_per_flop: DEFAULT_CPU_MATMUL_PER_FLOP,\n        }\n    }\n}\n\npub struct NativeAutoOffload {\n    provider: &'static dyn AccelProvider,\n    thresholds: ThresholdConfig,\n    enabled: bool,\n}\n\nstatic GLOBAL: OnceCell<Option<NativeAutoOffload>> = OnceCell::new();\nstatic PROFILE_MODEL: OnceCell<Option<ProfileCostModel>> = OnceCell::new();\n\nfn env_bool(key: &str) -> Option<bool> {\n    env::var(key).ok().and_then(|v| parse_bool(&v))\n}\n\nfn parse_bool(s: &str) -> Option<bool> {\n    match s.trim().to_ascii_lowercase().as_str() {\n        \"1\" | \"true\" | \"yes\" | \"on\" => Some(true),\n        \"0\" | \"false\" | \"no\" | \"off\" => Some(false),\n        _ => None,\n    }\n}\n\nfn log_promotion<F>(builder: F)\nwhere\n    F: FnOnce() -> String,\n{\n    match auto_offload_options().log_level {\n        AutoOffloadLogLevel::Off => {}\n        AutoOffloadLogLevel::Info => info!(\"{}\", builder()),\n        AutoOffloadLogLevel::Trace => trace!(\"{}\", builder()),\n    }\n}\n\nfn update_cpu_cost(slot: &mut f64, candidate: f64) {\n    if candidate.is_finite() && candidate > 0.0 && candidate < *slot {\n        *slot = candidate;\n    }\n}\n\nfn value_len(value: &Value) -> Option<usize> {\n    match value {\n        Value::Tensor(t) => Some(t.data.len()),\n        Value::GpuTensor(handle) => Some(handle.shape.iter().product()),\n        Value::Num(_) | Value::Bool(_) | Value::Int(_) => Some(1),\n        Value::Complex(_, _) => Some(1),\n        _ => None,\n    }\n}\n\nfn element_count_pair(a: &Value, b: &Value) -> Option<usize> {\n    let la = value_len(a)?;\n    let lb = value_len(b)?;\n    Some(la.max(lb))\n}\n\npub fn global() -> Option<&'static NativeAutoOffload> {\n    GLOBAL.get_or_init(|| initialize()).as_ref()\n}\n\nfn initialize() -> Option<NativeAutoOffload> {\n    if !auto_enabled() {\n        return None;\n    }\n    let provider = runmat_accelerate_api::provider()?;\n    let mut config = ThresholdConfig::default();\n    apply_env_overrides(&mut config);\n    if calibrate_enabled() {\n        if let Err(err) = auto_calibrate(provider, &mut config) {\n            debug!(\"Native auto-offload calibration failed: {err}\");\n        }\n    }\n    let model_status = if profile_cost_model().is_some() {\n        \"profile\"\n    } else {\n        \"fallback\"\n    };\n    info!(\n        \"Native auto-offload thresholds: unary={} binary={} reduction={} matmul_flops={} (model: {})\",\n        config.unary_min_elems,\n        config.binary_min_elems,\n        config.reduction_min_elems,\n        config.matmul_min_flops,\n        model_status\n    );\n    Some(NativeAutoOffload::new(provider, config))\n}\n\nimpl NativeAutoOffload {\n    fn new(provider: &'static dyn AccelProvider, thresholds: ThresholdConfig) -> Self {\n        let enabled = true;\n        Self {\n            provider,\n            thresholds,\n            enabled,\n        }\n    }\n\n    fn promote_tensor_if_large(&self, value: &Value, threshold: usize) -> Result<Value> {\n        match value {\n            Value::GpuTensor(_) => Ok(value.clone()),\n            Value::Tensor(t) => {\n                if t.data.len() >= threshold && threshold > 0 {\n                    log_promotion(|| {\n                        format!(\n                            \"Promoting tensor to GPU (len={}, threshold={})\",\n                            t.data.len(),\n                            threshold\n                        )\n                    });\n                    self.tensor_to_gpu(t)\n                } else {\n                    Ok(value.clone())\n                }\n            }\n            _ => Ok(value.clone()),\n        }\n    }\n\n    fn tensor_to_gpu(&self, tensor: &Tensor) -> Result<Value> {\n        let view = HostTensorView {\n            data: &tensor.data,\n            shape: &tensor.shape,\n        };\n        let handle = self\n            .provider\n            .upload(&view)\n            .map_err(|e| anyhow!(e.to_string()))?;\n        Ok(Value::GpuTensor(handle))\n    }\n\n    fn promote_binary(&self, op: BinaryOp, a: &Value, b: &Value) -> Result<(Value, Value)> {\n        if !self.enabled {\n            return Ok((a.clone(), b.clone()));\n        }\n        match op {\n            BinaryOp::Elementwise => {\n                let mut elems = element_count_pair(a, b).unwrap_or(0);\n                if let Some(active) = active_fusion() {\n                    if active.kind.is_elementwise() {\n                        if let Some(ec) = active.element_count {\n                            elems = ec;\n                        }\n                    }\n                }\n                let should_gpu = self\n                    .should_gpu_elementwise(elems)\n                    .unwrap_or(elems >= self.thresholds.binary_min_elems);\n                if should_gpu {\n                    log_promotion(|| format!(\"Elementwise offload accepted ({} elems)\", elems));\n                }\n                let threshold = if should_gpu {\n                    1\n                } else {\n                    self.thresholds.binary_min_elems\n                };\n                let a_p = self.promote_tensor_if_large(a, threshold)?;\n                let b_p = self.promote_tensor_if_large(b, threshold)?;\n                Ok((a_p, b_p))\n            }\n            BinaryOp::MatMul => {\n                if let (Some((ra, ca)), Some((rb, cb))) = (tensor_rows_cols(a), tensor_rows_cols(b))\n                {\n                    if ca != rb {\n                        return Ok((a.clone(), b.clone()));\n                    }\n                    let flops = ra * ca * cb;\n                    let should_gpu = self\n                        .should_gpu_matmul(flops)\n                        .unwrap_or(flops >= self.thresholds.matmul_min_flops);\n                    if should_gpu {\n                        log_promotion(|| {\n                            format!(\n                                \"Promoting matmul operands (flops={}, threshold={})\",\n                                flops, self.thresholds.matmul_min_flops\n                            )\n                        });\n                        let a_p = self.promote_tensor_if_large(a, 1)?;\n                        let b_p = self.promote_tensor_if_large(b, 1)?;\n                        return Ok((a_p, b_p));\n                    }\n                }\n                Ok((a.clone(), b.clone()))\n            }\n        }\n    }\n\n    fn promote_unary(&self, _op: UnaryOp, v: &Value) -> Result<Value> {\n        if !self.enabled {\n            return Ok(v.clone());\n        }\n        let threshold = self.thresholds.unary_min_elems;\n        self.promote_tensor_if_large(v, threshold)\n    }\n\n    fn promote_reduction(&self, _op: ReductionOp, args: &[Value]) -> Result<Vec<Value>> {\n        if !self.enabled || args.is_empty() {\n            return Ok(args.to_vec());\n        }\n        let elems = value_len(&args[0]).unwrap_or(0);\n        let should_gpu = self\n            .should_gpu_reduction(elems)\n            .unwrap_or(elems >= self.thresholds.reduction_min_elems);\n        if should_gpu {\n            log_promotion(|| format!(\"Reduction offload accepted ({} elems)\", elems));\n        }\n        let threshold = if should_gpu {\n            1\n        } else {\n            self.thresholds.reduction_min_elems\n        };\n        let mut out = Vec::with_capacity(args.len());\n        if let Some(first) = args.first() {\n            out.push(self.promote_tensor_if_large(first, threshold)?);\n            for rest in &args[1..] {\n                out.push(rest.clone());\n            }\n        }\n        Ok(out)\n    }\n\n    fn prepare_builtin(&self, name: &str, args: &[Value]) -> Result<Vec<Value>> {\n        if !self.enabled {\n            return Ok(args.to_vec());\n        }\n        if let Some(policy) = builtin_policy(name) {\n            if policy.is_sink {\n                return gather_args(args);\n            }\n\n            let mut processed = args.to_vec();\n\n            if policy\n                .accel_tags\n                .iter()\n                .any(|tag| matches!(tag, AccelTag::Reduction))\n            {\n                log_promotion(|| format!(\"Promoting builtin '{}' as reduction\", name));\n                return self.promote_reduction(ReductionOp::Sum, args);\n            }\n\n            if policy\n                .accel_tags\n                .iter()\n                .any(|tag| matches!(tag, AccelTag::MatMul))\n                && processed.len() >= 2\n            {\n                log_promotion(|| format!(\"Promoting builtin '{}' as matmul\", name));\n                let (a_p, b_p) =\n                    self.promote_binary(BinaryOp::MatMul, &processed[0], &processed[1])?;\n                processed[0] = a_p;\n                processed[1] = b_p;\n                return Ok(processed);\n            }\n\n            if policy\n                .accel_tags\n                .iter()\n                .any(|tag| matches!(tag, AccelTag::Elementwise))\n                && processed.len() >= 2\n            {\n                log_promotion(|| format!(\"Promoting builtin '{}' as elementwise\", name));\n                let (a_p, b_p) =\n                    self.promote_binary(BinaryOp::Elementwise, &processed[0], &processed[1])?;\n                processed[0] = a_p;\n                processed[1] = b_p;\n                return Ok(processed);\n            }\n\n            if let Some(first) = processed.first_mut() {\n                if policy\n                    .accel_tags\n                    .iter()\n                    .any(|tag| matches!(tag, AccelTag::Transpose))\n                {\n                    log_promotion(|| format!(\"Promoting builtin '{}' as transpose\", name));\n                    *first = self.promote_unary(UnaryOp::Transpose, first)?;\n                    return Ok(processed);\n                }\n\n                if policy\n                    .accel_tags\n                    .iter()\n                    .any(|tag| matches!(tag, AccelTag::Unary))\n                {\n                    log_promotion(|| format!(\"Promoting builtin '{}' as unary\", name));\n                    *first = self.promote_unary(UnaryOp::Generic, first)?;\n                    return Ok(processed);\n                }\n            }\n        }\n        Ok(args.to_vec())\n    }\n\n    fn should_gpu_elementwise(&self, elements: usize) -> Option<bool> {\n        if let Some(active) = active_fusion() {\n            if active.kind.is_elementwise() {\n                return Some(true);\n            }\n        }\n        if elements == 0 {\n            return Some(false);\n        }\n        let cpu_cost = self.thresholds.cpu_elem_per_elem * elements as f64;\n        profile_cost_model()\n            .and_then(|model| model.estimate_elemwise(elements))\n            .map(|gpu| gpu.as_secs_f64() * 0.95 < cpu_cost)\n    }\n\n    fn should_gpu_reduction(&self, elements: usize) -> Option<bool> {\n        if elements == 0 {\n            return Some(false);\n        }\n        let cpu_cost = self.thresholds.cpu_reduction_per_elem * elements as f64;\n        profile_cost_model()\n            .and_then(|model| model.estimate_reduction(elements))\n            .map(|gpu| gpu.as_secs_f64() * 0.95 < cpu_cost)\n    }\n\n    fn should_gpu_matmul(&self, flops: usize) -> Option<bool> {\n        if flops == 0 {\n            return Some(false);\n        }\n        let cpu_cost = self.thresholds.cpu_matmul_per_flop * flops as f64;\n        profile_cost_model()\n            .and_then(|model| model.estimate_matmul_flops(flops))\n            .map(|gpu| gpu.as_secs_f64() * 0.95 < cpu_cost)\n    }\n}\n\nfn tensor_rows_cols(value: &Value) -> Option<(usize, usize)> {\n    match value {\n        Value::Tensor(t) => Some((t.rows(), t.cols())),\n        Value::GpuTensor(handle) => {\n            if handle.shape.len() == 2 {\n                Some((handle.shape[0], handle.shape[1]))\n            } else {\n                None\n            }\n        }\n        _ => None,\n    }\n}\n\nfn gather_args(args: &[Value]) -> Result<Vec<Value>> {\n    let mut out = Vec::with_capacity(args.len());\n    for value in args {\n        if let Value::GpuTensor(handle) = value {\n            fusion_residency::clear(handle);\n        }\n        out.push(gather_if_needed(value).map_err(|e| anyhow!(e))?);\n    }\n    Ok(out)\n}\n\n#[derive(Clone, Copy)]\nstruct BuiltinPolicy {\n    accel_tags: &'static [AccelTag],\n    is_sink: bool,\n}\n\nstatic BUILTIN_POLICIES: OnceCell<HashMap<String, BuiltinPolicy>> = OnceCell::new();\n\nfn builtin_policy(name: &str) -> Option<BuiltinPolicy> {\n    let map = BUILTIN_POLICIES.get_or_init(|| {\n        let mut map = HashMap::new();\n        for func in builtin_functions() {\n            map.insert(\n                func.name.to_ascii_lowercase(),\n                BuiltinPolicy {\n                    accel_tags: func.accel_tags,\n                    is_sink: func.is_sink,\n                },\n            );\n        }\n        map\n    });\n    map.get(&name.to_ascii_lowercase()).copied()\n}\n\nfn auto_enabled() -> bool {\n    if let Some(flag) = env_bool(\"RUNMAT_ACCEL_AUTO_OFFLOAD\") {\n        return flag;\n    }\n    auto_offload_options().enabled\n}\n\nfn calibrate_enabled() -> bool {\n    if let Some(flag) = env_bool(\"RUNMAT_ACCEL_CALIBRATE\") {\n        return flag;\n    }\n    auto_offload_options().calibrate\n}\n\nfn apply_env_overrides(cfg: &mut ThresholdConfig) {\n    if let Some(val) = env_usize(\"RUNMAT_ACCEL_THRESHOLD_UNARY\") {\n        cfg.unary_min_elems = val;\n    }\n    if let Some(val) = env_usize(\"RUNMAT_ACCEL_THRESHOLD_ELEMWISE\") {\n        cfg.binary_min_elems = val;\n    }\n    if let Some(val) = env_usize(\"RUNMAT_ACCEL_THRESHOLD_REDUCTION\") {\n        cfg.reduction_min_elems = val;\n    }\n    if let Some(val) = env_usize(\"RUNMAT_ACCEL_THRESHOLD_MATMUL\") {\n        cfg.matmul_min_flops = val;\n    }\n    if let Some(val) = env_usize(\"RUNMAT_ACCEL_THRESHOLD_ALL\") {\n        cfg.unary_min_elems = val;\n        cfg.binary_min_elems = val;\n        cfg.reduction_min_elems = val;\n    }\n}\n\nfn env_usize(key: &str) -> Option<usize> {\n    env::var(key).ok().and_then(|v| v.parse::<usize>().ok())\n}\n\nfn auto_calibrate(provider: &'static dyn AccelProvider, cfg: &mut ThresholdConfig) -> Result<()> {\n    if let Some(elem_threshold) = calibrate_elemwise(provider, cfg).transpose()? {\n        cfg.binary_min_elems = elem_threshold;\n        cfg.unary_min_elems = cfg.unary_min_elems.min(elem_threshold);\n    }\n    if let Some(red_threshold) = calibrate_reduction(provider, cfg).transpose()? {\n        cfg.reduction_min_elems = red_threshold;\n    }\n    if let Some(matmul_threshold) = calibrate_matmul(provider, cfg).transpose()? {\n        cfg.matmul_min_flops = matmul_threshold;\n    }\n    Ok(())\n}\n\nfn calibrate_elemwise(\n    provider: &'static dyn AccelProvider,\n    cfg: &mut ThresholdConfig,\n) -> Option<Result<usize>> {\n    let sizes = [256usize, 1_024, 4_096, 16_384, 65_536];\n    for size in sizes {\n        match compare_elemwise(provider, size, &mut cfg.cpu_elem_per_elem) {\n            Ok(Some(true)) => return Some(Ok(size)),\n            Ok(Some(false)) => continue,\n            Ok(None) => return None,\n            Err(e) => return Some(Err(e)),\n        }\n    }\n    Some(Ok(usize::MAX))\n}\n\nfn compare_elemwise(\n    provider: &'static dyn AccelProvider,\n    elements: usize,\n    cpu_cost_slot: &mut f64,\n) -> Result<Option<bool>> {\n    if elements == 0 {\n        return Ok(Some(false));\n    }\n    let data: Vec<f64> = (0..elements).map(|i| i as f64).collect();\n    let tensor = Tensor::new(data.clone(), vec![elements, 1]).map_err(|e| anyhow!(e))?;\n    let a = Value::Tensor(tensor.clone());\n    let b = Value::Tensor(tensor.clone());\n    let cpu_time = time(|| runmat_runtime::elementwise_add(&a, &b))?;\n    let cpu_per_elem = cpu_time.as_secs_f64() / elements as f64;\n    update_cpu_cost(cpu_cost_slot, cpu_per_elem);\n    if let Some(model) = profile_cost_model() {\n        if let Some(gpu_time) = model.estimate_elemwise(elements) {\n            trace!(\n                \"Elemwise calibration ({} elems): cpu={:?}, gpu_est={:?}\",\n                elements,\n                cpu_time,\n                gpu_time\n            );\n            return Ok(Some(gpu_time < cpu_time));\n        }\n    }\n    let view = HostTensorView {\n        data: &data,\n        shape: &[elements, 1],\n    };\n    let ha = provider.upload(&view).map_err(|e| anyhow!(e.to_string()))?;\n    let hb = provider.upload(&view).map_err(|e| anyhow!(e.to_string()))?;\n    let start = Instant::now();\n    let hc = match provider.elem_add(&ha, &hb) {\n        Ok(h) => h,\n        Err(_) => {\n            let _ = provider.free(&ha);\n            let _ = provider.free(&hb);\n            return Ok(None);\n        }\n    };\n    let gpu_time = start.elapsed();\n    let _ = provider.free(&ha);\n    let _ = provider.free(&hb);\n    let _ = provider.free(&hc);\n    Ok(Some(gpu_time < cpu_time))\n}\n\nfn calibrate_reduction(\n    provider: &'static dyn AccelProvider,\n    cfg: &mut ThresholdConfig,\n) -> Option<Result<usize>> {\n    let sizes = [256usize, 1_024, 4_096, 16_384, 65_536];\n    for size in sizes {\n        match compare_reduction(provider, size, &mut cfg.cpu_reduction_per_elem) {\n            Ok(Some(true)) => return Some(Ok(size)),\n            Ok(Some(false)) => continue,\n            Ok(None) => return None,\n            Err(e) => return Some(Err(e)),\n        }\n    }\n    Some(Ok(usize::MAX))\n}\n\nfn compare_reduction(\n    provider: &'static dyn AccelProvider,\n    elements: usize,\n    cpu_cost_slot: &mut f64,\n) -> Result<Option<bool>> {\n    let data: Vec<f64> = (0..elements).map(|i| i as f64).collect();\n    let tensor = Tensor::new(data.clone(), vec![elements, 1]).map_err(|e| anyhow!(e))?;\n    let value = Value::Tensor(tensor.clone());\n    let cpu_time = time(|| runmat_runtime::call_builtin(\"sum\", &[value.clone()]))?;\n    let cpu_per_elem = cpu_time.as_secs_f64() / elements as f64;\n    update_cpu_cost(cpu_cost_slot, cpu_per_elem);\n    if let Some(model) = profile_cost_model() {\n        if let Some(gpu_time) = model.estimate_reduction(elements) {\n            trace!(\n                \"Reduction calibration ({} elems): cpu={:?}, gpu_est={:?}\",\n                elements,\n                cpu_time,\n                gpu_time\n            );\n            return Ok(Some(gpu_time < cpu_time));\n        }\n    }\n    let view = HostTensorView {\n        data: &data,\n        shape: &[elements, 1],\n    };\n    let h = provider.upload(&view).map_err(|e| anyhow!(e.to_string()))?;\n    let start = Instant::now();\n    let out = match provider.reduce_sum(&h) {\n        Ok(hc) => hc,\n        Err(_) => {\n            provider.free(&h).ok();\n            return Ok(None);\n        }\n    };\n    let gpu_time = start.elapsed();\n    let _ = provider.free(&h);\n    let _ = provider.free(&out);\n    Ok(Some(gpu_time < cpu_time))\n}\n\nfn calibrate_matmul(\n    provider: &'static dyn AccelProvider,\n    cfg: &mut ThresholdConfig,\n) -> Option<Result<usize>> {\n    let dims = [32usize, 64, 96, 128, 192];\n    for n in dims {\n        match compare_matmul(provider, n, &mut cfg.cpu_matmul_per_flop) {\n            Ok(Some(true)) => {\n                let flops = n * n * n;\n                return Some(Ok(flops));\n            }\n            Ok(Some(false)) => continue,\n            Ok(None) => return None,\n            Err(e) => return Some(Err(e)),\n        }\n    }\n    Some(Ok(usize::MAX))\n}\n\nfn compare_matmul(\n    provider: &'static dyn AccelProvider,\n    n: usize,\n    cpu_cost_slot: &mut f64,\n) -> Result<Option<bool>> {\n    if n == 0 {\n        return Ok(Some(false));\n    }\n    let total = n * n;\n    let data_a: Vec<f64> = (0..total).map(|i| (i % 13) as f64).collect();\n    let data_b: Vec<f64> = (0..total).map(|i| (i % 7) as f64).collect();\n    let ta = Tensor::new(data_a.clone(), vec![n, n]).map_err(|e| anyhow!(e))?;\n    let tb = Tensor::new(data_b.clone(), vec![n, n]).map_err(|e| anyhow!(e))?;\n    let a = Value::Tensor(ta.clone());\n    let b = Value::Tensor(tb.clone());\n    let cpu_time = time(|| runmat_runtime::matrix::value_matmul(&a, &b))?;\n    let flops = (n * n * n) as f64;\n    update_cpu_cost(cpu_cost_slot, cpu_time.as_secs_f64() / flops);\n    if let Some(model) = profile_cost_model() {\n        if let Some(gpu_time) = model.estimate_matmul(n, n, n) {\n            trace!(\n                \"Matmul calibration ({}^3 flops): cpu={:?}, gpu_est={:?}\",\n                n,\n                cpu_time,\n                gpu_time\n            );\n            return Ok(Some(gpu_time < cpu_time));\n        }\n    }\n    let view_a = HostTensorView {\n        data: &data_a,\n        shape: &[n, n],\n    };\n    let view_b = HostTensorView {\n        data: &data_b,\n        shape: &[n, n],\n    };\n    let ha = provider\n        .upload(&view_a)\n        .map_err(|e| anyhow!(e.to_string()))?;\n    let hb = provider\n        .upload(&view_b)\n        .map_err(|e| anyhow!(e.to_string()))?;\n    let start = Instant::now();\n    let hc = match provider.matmul(&ha, &hb) {\n        Ok(h) => h,\n        Err(_) => {\n            let _ = provider.free(&ha);\n            let _ = provider.free(&hb);\n            return Ok(None);\n        }\n    };\n    let gpu_time = start.elapsed();\n    let _ = provider.free(&ha);\n    let _ = provider.free(&hb);\n    let _ = provider.free(&hc);\n    Ok(Some(gpu_time < cpu_time))\n}\n\nfn time<F, T>(mut f: F) -> Result<Duration>\nwhere\n    F: FnMut() -> Result<T, String>,\n{\n    let start = Instant::now();\n    let _ = f().map_err(|e| anyhow!(e))?;\n    Ok(start.elapsed())\n}\n\n#[allow(dead_code)]\n#[derive(Clone, Deserialize, Debug)]\nstruct ProfileDurationSummary {\n    #[serde(default)]\n    avg_ms: f64,\n    #[serde(default)]\n    min_ms: f64,\n    #[serde(default)]\n    max_ms: f64,\n}\n\n#[allow(dead_code)]\n#[derive(Clone, Deserialize, Debug)]\nstruct ProfileReport {\n    name: String,\n    category: String,\n    #[serde(default)]\n    detail: String,\n    #[serde(default)]\n    input_shapes: Vec<Vec<usize>>,\n    #[serde(default)]\n    iterations: usize,\n    upload_ms: ProfileDurationSummary,\n    compute_ms: ProfileDurationSummary,\n    download_ms: ProfileDurationSummary,\n    total_ms: ProfileDurationSummary,\n    #[serde(default)]\n    notes: Vec<String>,\n}\n\n#[derive(Clone, Copy, Default, Debug)]\nstruct LinearModel {\n    slope: f64,\n    intercept: f64,\n}\n\nimpl LinearModel {\n    fn estimate(&self, x: f64) -> Option<Duration> {\n        if !self.slope.is_finite() || self.slope <= 0.0 {\n            return None;\n        }\n        let total = self.intercept + self.slope * x;\n        if total.is_finite() && total > 0.0 {\n            Some(Duration::from_secs_f64(total))\n        } else {\n            None\n        }\n    }\n}\n\n#[derive(Default)]\nstruct ProfileCostModel {\n    elem: Option<LinearModel>,\n    reduction: Option<LinearModel>,\n    transpose: Option<LinearModel>,\n    matmul: Option<LinearModel>,\n}\n\nimpl ProfileCostModel {\n    fn from_reports(reports: &[ProfileReport]) -> Self {\n        let mut elem_samples = Vec::<(f64, f64)>::new();\n        let mut reduction_samples = Vec::<(f64, f64)>::new();\n        let mut transpose_samples = Vec::<(f64, f64)>::new();\n        let mut matmul_samples = Vec::<(f64, f64)>::new();\n\n        for report in reports {\n            let total_secs = report.total_ms.avg_ms / 1_000.0;\n            match report.category.as_str() {\n                \"elementwise\" | \"reduction\" | \"transpose\" => {\n                    if let Some(shape) = report.input_shapes.first() {\n                        let elems: usize = shape.iter().copied().product();\n                        if elems == 0 {\n                            continue;\n                        }\n                        let sample = (elems as f64, total_secs);\n                        match report.category.as_str() {\n                            \"elementwise\" => elem_samples.push(sample),\n                            \"reduction\" => reduction_samples.push(sample),\n                            \"transpose\" => transpose_samples.push(sample),\n                            _ => {}\n                        }\n                    }\n                }\n                \"matmul\" => {\n                    if report.input_shapes.len() >= 2 {\n                        let a = &report.input_shapes[0];\n                        let b = &report.input_shapes[1];\n                        if a.len() == 2 && b.len() == 2 {\n                            let m = a[0];\n                            let k = a[1];\n                            let n = b[1];\n                            let flops = m.checked_mul(k).and_then(|val| val.checked_mul(n));\n                            if let Some(flops) = flops {\n                                matmul_samples.push((flops as f64, total_secs));\n                            }\n                        }\n                    }\n                }\n                _ => {}\n            }\n        }\n\n        ProfileCostModel {\n            elem: fit_linear_model(&elem_samples),\n            reduction: fit_linear_model(&reduction_samples),\n            transpose: fit_linear_model(&transpose_samples),\n            matmul: fit_linear_model(&matmul_samples),\n        }\n    }\n\n    fn estimate_elemwise(&self, elements: usize) -> Option<Duration> {\n        self.elem.and_then(|model| model.estimate(elements as f64))\n    }\n\n    fn estimate_reduction(&self, elements: usize) -> Option<Duration> {\n        self.reduction\n            .and_then(|model| model.estimate(elements as f64))\n    }\n\n    fn estimate_matmul(&self, m: usize, k: usize, n: usize) -> Option<Duration> {\n        let flops = m.checked_mul(k)?.checked_mul(n)?;\n        self.matmul.and_then(|model| model.estimate(flops as f64))\n    }\n\n    fn estimate_matmul_flops(&self, flops: usize) -> Option<Duration> {\n        self.matmul.and_then(|model| model.estimate(flops as f64))\n    }\n\n    #[allow(dead_code)]\n    fn estimate_transpose(&self, elements: usize) -> Option<Duration> {\n        self.transpose\n            .and_then(|model| model.estimate(elements as f64))\n    }\n}\n\nfn fit_linear_model(samples: &[(f64, f64)]) -> Option<LinearModel> {\n    if samples.is_empty() {\n        return None;\n    }\n    if samples.len() == 1 {\n        let (x, y) = samples[0];\n        if x > 0.0 {\n            return Some(LinearModel {\n                slope: (y / x).max(0.0),\n                intercept: 0.0,\n            });\n        }\n        return None;\n    }\n\n    let sum_x: f64 = samples.iter().map(|(x, _)| *x).sum();\n    let sum_y: f64 = samples.iter().map(|(_, y)| *y).sum();\n    let sum_xx: f64 = samples.iter().map(|(x, _)| x * x).sum();\n    let sum_xy: f64 = samples.iter().map(|(x, y)| x * y).sum();\n    let n = samples.len() as f64;\n    let denom = (n * sum_xx) - (sum_x * sum_x);\n    if denom.abs() < f64::EPSILON {\n        return None;\n    }\n    let slope = ((n * sum_xy) - (sum_x * sum_y)) / denom;\n    let mean_x = sum_x / n;\n    let mean_y = sum_y / n;\n    let mut intercept = mean_y - slope * mean_x;\n    if intercept < 0.0 {\n        intercept = 0.0;\n    }\n    if !slope.is_finite() || slope <= 0.0 {\n        return None;\n    }\n    Some(LinearModel { slope, intercept })\n}\n\nfn profile_cost_model() -> Option<&'static ProfileCostModel> {\n    PROFILE_MODEL\n        .get_or_init(|| load_profile_cost_model())\n        .as_ref()\n}\n\nfn load_profile_cost_model() -> Option<ProfileCostModel> {\n    let mut candidates = Vec::new();\n    if let Ok(path) = env::var(\"RUNMAT_ACCEL_PROFILE\") {\n        candidates.push(PathBuf::from(path));\n    }\n    if let Some(path) = auto_offload_options().profile_path.clone() {\n        candidates.push(path);\n    }\n    candidates.push(PathBuf::from(\"benchmarks/wgpu_profile/mac_m2.json\"));\n    candidates.push(PathBuf::from(\"wgpu_profile.json\"));\n\n    for path in candidates {\n        if !path.exists() {\n            continue;\n        }\n        match fs::read_to_string(&path) {\n            Ok(contents) => match serde_json::from_str::<Vec<ProfileReport>>(&contents) {\n                Ok(reports) => {\n                    debug!(\n                        \"Loaded {} GPU profile reports from {}\",\n                        reports.len(),\n                        path.display()\n                    );\n                    return Some(ProfileCostModel::from_reports(&reports));\n                }\n                Err(err) => {\n                    debug!(\"Failed to parse GPU profile {}: {err}\", path.display());\n                }\n            },\n            Err(err) => {\n                debug!(\"Failed to read GPU profile {}: {err}\", path.display());\n            }\n        }\n    }\n    None\n}\n\npub fn promote_binary(op: BinaryOp, a: &Value, b: &Value) -> Result<(Value, Value)> {\n    if !auto_enabled() {\n        return Ok((a.clone(), b.clone()));\n    }\n    if let Some(auto) = global() {\n        auto.promote_binary(op, a, b)\n    } else {\n        Ok((a.clone(), b.clone()))\n    }\n}\n\npub fn promote_unary(op: UnaryOp, value: &Value) -> Result<Value> {\n    if !auto_enabled() {\n        return Ok(value.clone());\n    }\n    if let Some(auto) = global() {\n        auto.promote_unary(op, value)\n    } else {\n        Ok(value.clone())\n    }\n}\n\npub fn prepare_builtin_args(name: &str, args: &[Value]) -> Result<Vec<Value>> {\n    if let Some(policy) = builtin_policy(name) {\n        if policy.is_sink {\n            return gather_args(args);\n        }\n    }\n    if !auto_enabled() {\n        return Ok(args.to_vec());\n    }\n    if let Some(auto) = global() {\n        auto.prepare_builtin(name, args)\n    } else {\n        Ok(args.to_vec())\n    }\n}\n\npub fn is_sink(name: &str) -> bool {\n    builtin_policy(name).map(|p| p.is_sink).unwrap_or(false)\n}\n\npub fn promote_reduction_args(op: ReductionOp, args: &[Value]) -> Result<Vec<Value>> {\n    if !auto_enabled() {\n        return Ok(args.to_vec());\n    }\n    if let Some(auto) = global() {\n        auto.promote_reduction(op, args)\n    } else {\n        Ok(args.to_vec())\n    }\n}\n\n===== crates/runmat-accelerate/src/fusion.rs =====\nuse std::cell::RefCell;\nuse std::collections::{HashMap, HashSet};\nuse std::sync::{Arc, RwLock, Weak};\n\nuse once_cell::sync::Lazy;\nuse runmat_builtins::Value;\nuse serde::{Deserialize, Serialize};\n\nuse crate::graph::{\n    AccelGraph, AccelNode, AccelNodeLabel, InstrSpan, NodeId, PrimitiveOp, ShapeInfo, ValueId,\n    ValueOrigin,\n};\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub enum FusionKind {\n    ElementwiseChain,\n    Reduction,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FusionGroup {\n    pub id: usize,\n    pub kind: FusionKind,\n    pub nodes: Vec<NodeId>,\n    pub shape: ShapeInfo,\n    pub span: InstrSpan,\n}\n\npub fn detect_fusion_groups(graph: &AccelGraph) -> Vec<FusionGroup> {\n    if graph.nodes.is_empty() {\n        return Vec::new();\n    }\n\n    let consumer_map = build_consumer_map(graph);\n    let mut assigned: HashSet<NodeId> = HashSet::new();\n    let mut groups = Vec::new();\n    let mut group_id = 0usize;\n\n    for node in &graph.nodes {\n        // Elementwise chains\n        if !node.is_elementwise() || assigned.contains(&node.id) {\n            continue;\n        }\n        if node.outputs.is_empty() {\n            continue;\n        }\n        let mut current_shape = node_output_shape(graph, node);\n        if matches!(current_shape, ShapeInfo::Unknown) {\n            continue;\n        }\n        let mut chain: Vec<NodeId> = Vec::new();\n        let mut frontier = node.id;\n        let mut local_seen: HashSet<NodeId> = HashSet::new();\n\n        loop {\n            if !local_seen.insert(frontier) {\n                break;\n            }\n            chain.push(frontier);\n            let next = find_next_elementwise(\n                graph,\n                frontier,\n                &assigned,\n                &local_seen,\n                &consumer_map,\n                &current_shape,\n            );\n            match next {\n                Some((next_id, next_shape)) => {\n                    frontier = next_id;\n                    current_shape = next_shape;\n                }\n                None => break,\n            }\n        }\n\n        if chain.len() > 1 {\n            for id in &chain {\n                assigned.insert(*id);\n            }\n            let span = group_span(graph, &chain);\n            groups.push(FusionGroup {\n                id: group_id,\n                kind: FusionKind::ElementwiseChain,\n                nodes: chain,\n                shape: current_shape.clone(),\n                span,\n            });\n            group_id += 1;\n        }\n    }\n\n    // Reduction singletons (basic grouping; future: include eligible producers)\n    for node in &graph.nodes {\n        if !node.is_reduction() || assigned.contains(&node.id) {\n            continue;\n        }\n        let span = InstrSpan {\n            start: node.span.start,\n            end: node.span.end,\n        };\n        groups.push(FusionGroup {\n            id: group_id,\n            kind: FusionKind::Reduction,\n            nodes: vec![node.id],\n            shape: node_output_shape(graph, node),\n            span,\n        });\n        group_id += 1;\n    }\n\n    groups\n}\n\nfn build_consumer_map(graph: &AccelGraph) -> HashMap<ValueId, HashSet<NodeId>> {\n    let mut map: HashMap<ValueId, HashSet<NodeId>> = HashMap::new();\n    for node in &graph.nodes {\n        for &input in &node.inputs {\n            if let Some(value) = graph.value(input) {\n                if matches!(value.origin, crate::graph::ValueOrigin::NodeOutput { .. }) {\n                    map.entry(input).or_default().insert(node.id);\n                }\n            }\n        }\n    }\n    map\n}\n\nfn node_output_shape(graph: &AccelGraph, node: &AccelNode) -> ShapeInfo {\n    let mut shape = ShapeInfo::Scalar;\n    for &output in &node.outputs {\n        if let Some(info) = graph.value(output) {\n            shape = shape.unify(&info.shape);\n        }\n    }\n    shape\n}\n\nfn find_next_elementwise(\n    graph: &AccelGraph,\n    node_id: NodeId,\n    assigned: &HashSet<NodeId>,\n    local_seen: &HashSet<NodeId>,\n    consumer_map: &HashMap<ValueId, HashSet<NodeId>>,\n    current_shape: &ShapeInfo,\n) -> Option<(NodeId, ShapeInfo)> {\n    let node = graph.node(node_id)?;\n    let mut candidate: Option<(NodeId, ShapeInfo)> = None;\n\n    for &output in &node.outputs {\n        let consumers = consumer_map.get(&output)?;\n        if consumers.len() != 1 {\n            return None;\n        }\n        let next_id = *consumers.iter().next()?;\n        if next_id <= node_id || assigned.contains(&next_id) || local_seen.contains(&next_id) {\n            return None;\n        }\n        let next_node = graph.node(next_id)?;\n        if !next_node.is_elementwise() {\n            return None;\n        }\n        // Ensure the edge we follow is actually used by next node\n        if !next_node.inputs.contains(&output) {\n            continue;\n        }\n        let next_shape = node_output_shape(graph, next_node);\n        if matches!(next_shape, ShapeInfo::Unknown) {\n            return None;\n        }\n        let unified = current_shape.unify(&next_shape);\n        if matches!(unified, ShapeInfo::Unknown) {\n            return None;\n        }\n        candidate = Some((next_id, unified));\n        break;\n    }\n\n    candidate\n}\n\nfn group_span(graph: &AccelGraph, nodes: &[NodeId]) -> InstrSpan {\n    let mut start = usize::MAX;\n    let mut end = 0usize;\n    for &id in nodes {\n        if let Some(node) = graph.node(id) {\n            start = start.min(node.span.start);\n            end = end.max(node.span.end);\n        }\n    }\n    if start == usize::MAX {\n        start = 0;\n    }\n    InstrSpan { start, end }\n}\n\n#[derive(Debug, Clone)]\npub struct FusionPlan {\n    pub groups: Vec<FusionGroupPlan>,\n}\n\n#[derive(Debug, Clone)]\npub struct FusionGroupPlan {\n    pub index: usize,\n    pub group: FusionGroup,\n    pub operations: Vec<FusionOp>,\n    pub inputs: Vec<ValueId>,\n    pub stack_pattern: Vec<usize>,\n    pub constants: HashMap<usize, Value>,\n    pub output: Option<ValueId>,\n    pub kernel: FusionKernelSpec,\n}\n\n#[derive(Debug, Clone)]\npub enum FusionOp {\n    Primitive {\n        op: PrimitiveOp,\n        inputs: Vec<ValueId>,\n        output: Option<ValueId>,\n    },\n    Builtin {\n        name: String,\n        inputs: Vec<ValueId>,\n        output: Option<ValueId>,\n    },\n}\n\n#[derive(Debug, Clone)]\npub struct FusionKernelSpec {\n    pub kind: FusionKind,\n    pub supported: bool,\n}\n\nimpl FusionKernelSpec {\n    fn new(kind: FusionKind, supported: bool) -> Self {\n        Self { kind, supported }\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct ActiveFusion {\n    pub kind: FusionKind,\n    pub span: InstrSpan,\n    pub element_count: Option<usize>,\n    pub supported: bool,\n}\n\nstruct ActiveContext {\n    plan: Arc<FusionPlan>,\n    active_group: Option<usize>,\n}\n\nstatic PLAN_CACHE: Lazy<RwLock<HashMap<usize, Weak<FusionPlan>>>> =\n    Lazy::new(|| RwLock::new(HashMap::new()));\n\nthread_local! {\n    static ACTIVE_PLAN: RefCell<Option<ActiveContext>> = const { RefCell::new(None) };\n}\n\npub fn prepare_fusion_plan(\n    graph: Option<&AccelGraph>,\n    groups: &[FusionGroup],\n) -> Option<Arc<FusionPlan>> {\n    let graph = graph?;\n    if groups.is_empty() {\n        return None;\n    }\n    let key = graph as *const AccelGraph as usize;\n    if let Some(plan) = PLAN_CACHE\n        .read()\n        .ok()\n        .and_then(|guard| guard.get(&key).and_then(|weak| weak.upgrade()))\n    {\n        return Some(plan);\n    }\n\n    let plan = FusionPlan::from_graph(graph, groups);\n    let plan = Arc::new(plan);\n    if let Ok(mut guard) = PLAN_CACHE.write() {\n        guard.insert(key, Arc::downgrade(&plan));\n    }\n    Some(plan)\n}\n\npub fn activate_fusion_plan(plan: Option<Arc<FusionPlan>>) {\n    ACTIVE_PLAN.with(|ctx| {\n        let mut slot = ctx.borrow_mut();\n        *slot = plan.map(|plan| ActiveContext {\n            plan,\n            active_group: None,\n        });\n    });\n}\n\npub fn deactivate_fusion_plan() {\n    ACTIVE_PLAN.with(|ctx| {\n        ctx.borrow_mut().take();\n    });\n}\n\npub fn set_current_pc(pc: usize) {\n    ACTIVE_PLAN.with(|ctx| {\n        if let Some(context) = ctx.borrow_mut().as_mut() {\n            context.active_group = context.plan.group_for_pc(pc);\n        }\n    });\n}\n\npub fn active_fusion() -> Option<ActiveFusion> {\n    ACTIVE_PLAN.with(|ctx| {\n        ctx.borrow()\n            .as_ref()\n            .and_then(|context| {\n                context\n                    .active_group\n                    .and_then(|idx| context.plan.groups.get(idx))\n            })\n            .map(|plan| ActiveFusion {\n                kind: plan.group.kind.clone(),\n                span: plan.group.span.clone(),\n                element_count: plan.element_count(),\n                supported: plan.kernel.supported,\n            })\n    })\n}\n\npub fn active_group_plan_clone() -> Option<FusionGroupPlan> {\n    ACTIVE_PLAN.with(|ctx| {\n        ctx.borrow().as_ref().and_then(|context| {\n            context\n                .active_group\n                .and_then(|idx| context.plan.groups.get(idx).cloned())\n        })\n    })\n}\n\nimpl FusionPlan {\n    pub fn from_graph(graph: &AccelGraph, groups: &[FusionGroup]) -> Self {\n        let plans = groups\n            .iter()\n            .enumerate()\n            .map(|(idx, group)| FusionGroupPlan::new(idx, group.clone(), graph))\n            .collect();\n        Self { groups: plans }\n    }\n\n    fn group_for_pc(&self, pc: usize) -> Option<usize> {\n        self.groups\n            .iter()\n            .find(|plan| pc >= plan.group.span.start && pc <= plan.group.span.end)\n            .map(|plan| plan.index)\n    }\n}\n\nimpl From<Vec<FusionGroupPlan>> for FusionPlan {\n    fn from(groups: Vec<FusionGroupPlan>) -> Self {\n        Self { groups }\n    }\n}\n\nimpl FusionGroupPlan {\n    fn new(index: usize, group: FusionGroup, graph: &AccelGraph) -> Self {\n        let node_set: HashSet<NodeId> = group.nodes.iter().copied().collect();\n        let mut seen_inputs: HashMap<ValueId, usize> = HashMap::new();\n        let mut inputs: Vec<ValueId> = Vec::new();\n        let mut stack_pattern: Vec<usize> = Vec::new();\n        let mut constants: HashMap<usize, Value> = HashMap::new();\n        let mut operations = Vec::new();\n        let mut output: Option<ValueId> = None;\n\n        for node_id in &group.nodes {\n            let Some(node) = graph.node(*node_id) else {\n                continue;\n            };\n            for input in &node.inputs {\n                let (external, is_variable, maybe_constant) = match graph.value(*input) {\n                    Some(info) => match &info.origin {\n                        ValueOrigin::NodeOutput { node: origin, .. }\n                            if node_set.contains(origin) =>\n                        {\n                            (false, false, None)\n                        }\n                        ValueOrigin::Variable { .. } => (true, true, None),\n                        ValueOrigin::Constant => (true, false, info.constant.clone()),\n                        _ => (true, false, None),\n                    },\n                    None => (true, false, None),\n                };\n                if external {\n                    let input_idx = if let Some(idx) = seen_inputs.get(input) {\n                        *idx\n                    } else {\n                        let idx = inputs.len();\n                        inputs.push(*input);\n                        seen_inputs.insert(*input, idx);\n                        idx\n                    };\n\n                    if let Some(constant) = maybe_constant.clone() {\n                        constants.insert(input_idx, constant);\n                    } else if !is_variable {\n                        stack_pattern.push(input_idx);\n                    }\n                }\n            }\n\n            let op = match &node.label {\n                AccelNodeLabel::Primitive(p) => FusionOp::Primitive {\n                    op: *p,\n                    inputs: node.inputs.clone(),\n                    output: node.outputs.get(0).copied(),\n                },\n                AccelNodeLabel::Builtin { name } => FusionOp::Builtin {\n                    name: name.clone(),\n                    inputs: node.inputs.clone(),\n                    output: node.outputs.get(0).copied(),\n                },\n                AccelNodeLabel::Unknown => FusionOp::Primitive {\n                    op: PrimitiveOp::UPlus,\n                    inputs: node.inputs.clone(),\n                    output: node.outputs.get(0).copied(),\n                },\n            };\n            operations.push(op);\n\n            if let Some(out) = node.outputs.get(0).copied() {\n                output = Some(out);\n            }\n        }\n\n        let kind = group.kind.clone();\n        let mut plan = Self {\n            index,\n            group,\n            operations,\n            stack_pattern,\n            constants,\n            inputs,\n            output,\n            kernel: FusionKernelSpec::new(kind, true),\n        };\n\n        let supported = if plan.kernel.kind.is_elementwise() {\n            plan.generate_wgsl(\"f32\").is_some()\n        } else if plan.kernel.kind.is_reduction() {\n            plan.generate_reduction_wgsl(\"f32\").is_some()\n        } else {\n            false\n        };\n        plan.kernel.supported = supported;\n        plan\n    }\n\n    pub fn element_count(&self) -> Option<usize> {\n        self.group.element_count()\n    }\n\n    pub fn constant_shape(&self, len: usize) -> Vec<usize> {\n        match &self.group.shape {\n            ShapeInfo::Tensor(dims) if !dims.is_empty() && dims.iter().all(|dim| dim.is_some()) => {\n                dims.iter().map(|dim| dim.unwrap()).collect()\n            }\n            _ => vec![len],\n        }\n    }\n\n    pub fn generate_wgsl(&self, scalar_ty: &str) -> Option<String> {\n        if !self.kernel.kind.is_elementwise() {\n            return None;\n        }\n        if !self.kernel.supported {\n            return None;\n        }\n        let output_id = self.output?;\n        let mut exprs: HashMap<ValueId, String> = HashMap::new();\n        for (idx, input_id) in self.inputs.iter().enumerate() {\n            exprs.insert(*input_id, format!(\"input{idx}.data[idx]\"));\n        }\n\n        let mut body = String::new();\n        for (node_idx, op) in self.operations.iter().enumerate() {\n            let tmp_name = format!(\"tmp{node_idx}\");\n            match op {\n                FusionOp::Primitive { op, inputs, output } => {\n                    let expr = primitive_expr(*op, inputs, &exprs)?;\n                    body.push_str(&format!(\"    let {tmp_name}: {scalar_ty} = {expr};\\n\"));\n                    if let Some(out) = output {\n                        exprs.insert(*out, tmp_name.clone());\n                    }\n                }\n                FusionOp::Builtin {\n                    name,\n                    inputs,\n                    output,\n                } => {\n                    let expr = builtin_expr(name, inputs, &exprs, scalar_ty)?;\n                    body.push_str(&format!(\"    let {tmp_name}: {scalar_ty} = {expr};\\n\"));\n                    if let Some(out) = output {\n                        exprs.insert(*out, tmp_name.clone());\n                    }\n                }\n            }\n        }\n\n        let final_expr = exprs.get(&output_id)?.clone();\n\n        let mut shader = String::new();\n        shader.push_str(&format!(\"struct Tensor {{ data: array<{scalar_ty}>; }}\\n\"));\n        shader.push_str(\"struct Params {\\n    len: u32;\\n    _pad0: u32;\\n    _pad1: u32;\\n    _pad2: u32;\\n}\\n\\n\");\n        for (idx, _) in self.inputs.iter().enumerate() {\n            shader.push_str(&format!(\n                \"@group(0) @binding({}) var<storage, read> input{}: Tensor;\\n\",\n                idx, idx\n            ));\n        }\n        shader.push_str(&format!(\n            \"@group(0) @binding({}) var<storage, read_write> output: Tensor;\\n\",\n            self.inputs.len()\n        ));\n        shader.push_str(&format!(\n            \"@group(0) @binding({}) var<uniform> params: Params;\\n\\n\",\n            self.inputs.len() + 1\n        ));\n        shader.push_str(\"@compute @workgroup_size(256)\\nfn main(@builtin(global_invocation_id) gid: vec3<u32>) {\\n\");\n        shader.push_str(\n            \"    let idx = gid.x;\\n    if (idx >= params.len) {\\n        return;\\n    }\\n\",\n        );\n        shader.push_str(&body);\n        shader.push_str(&format!(\"    output.data[idx] = {final_expr};\\n}}\\n\"));\n        Some(shader)\n    }\n\n    pub fn generate_reduction_wgsl(&self, scalar_ty: &str) -> Option<String> {\n        if !self.kernel.kind.is_reduction() {\n            return None;\n        }\n        // Minimal column-major reduction kernel template (single workgroup per slice).\n        // Assumes first input is the tensor to reduce; ignores additional inputs for now.\n        if self.inputs.is_empty() {\n            return None;\n        }\n        // Determine axis from constants: inputs[1] if present; 1-based dim -> 0 for columns, 1 for rows.\n        let mut axis = 0usize;\n        if let Some(Value::Num(n)) = self.constants.get(&1) {\n            if *n >= 1.0 { axis = (*n as usize).saturating_sub(1); }\n        } else if let Some(Value::Int(i)) = self.constants.get(&1) {\n            let v = i.to_f64(); if v >= 1.0 { axis = (v as usize).saturating_sub(1); }\n        }\n\n        // Detect omitnan constant (compile-time selection)\n        let omitnan = self.constants.values().any(|v| match v {\n            Value::String(s) => s.eq_ignore_ascii_case(\"omitnan\"),\n            _ => false,\n        });\n\n        let mut shader = String::new();\n        shader.push_str(&format!(\"struct Tensor {{ data: array<{scalar_ty}>; }}\\n\"));\n        shader.push_str(\"struct MParams { nrows: u32, ncols: u32, ld: u32, flags: u32 }\\n\\n\");\n        shader.push_str(&format!(\"@group(0) @binding(0) var<storage, read> input0: Tensor;\\n\"));\n        shader.push_str(&format!(\"@group(0) @binding(1) var<storage, read_write> output: Tensor;\\n\"));\n        shader.push_str(&format!(\"@group(0) @binding(2) var<uniform> params: MParams;\\n\\n\"));\n        shader.push_str(&format!(\"const OMITNAN: bool = {}bool({});\\n\\n\", if scalar_ty==\"f64\" { \"\" } else { \"\" }, if omitnan { \"true\" } else { \"false\" }));\n        shader.push_str(\"@compute @workgroup_size(256)\\n\");\n        if axis == 0 {\n            // Column-wise: reduce over rows; one output per column (ncols)\n            shader.push_str(\n                \"fn main(@builtin(local_invocation_id) lid: vec3<u32>, @builtin(workgroup_id) wid: vec3<u32>) {\\n\",\n            );\n            shader.push_str(\"  let col = wid.x;\\n  if (col >= params.ncols) { return; }\\n\");\n            shader.push_str(&format!(\"  var acc: {scalar_ty} = {}0.0;\\n\", if scalar_ty == \"f64\" { \"f64(\" } else { \"\" }));\n            if scalar_ty == \"f64\" { shader.push_str(\"  // close cast for f64 literal\\n\"); }\n            shader.push_str(\"  var saw_nan: bool = false;\\n  var r = lid.x;\\n\");\n            shader.push_str(\"  while (r < params.nrows) {\\n    let v = input0.data[ (col * params.ld) + r ];\\n    if (OMITNAN) { if (!isNan(v)) { acc = acc + v; } } else { if (isNan(v)) { saw_nan = true; } else { acc = acc + v; } }\\n    r += 256u;\\n  }\\n\");\n        } else {\n            // Row-wise: reduce over cols; one output per row (nrows)\n            shader.push_str(\n                \"fn main(@builtin(local_invocation_id) lid: vec3<u32>, @builtin(workgroup_id) wid: vec3<u32>) {\\n\",\n            );\n            shader.push_str(\"  let row = wid.x;\\n  if (row >= params.nrows) { return; }\\n\");\n            shader.push_str(&format!(\"  var acc: {scalar_ty} = {}0.0;\\n\", if scalar_ty == \"f64\" { \"f64(\" } else { \"\" }));\n            if scalar_ty == \"f64\" { shader.push_str(\"  // close cast for f64 literal\\n\"); }\n            shader.push_str(\"  var saw_nan: bool = false;\\n  var c = lid.x;\\n\");\n            shader.push_str(\"  while (c < params.ncols) {\\n    let v = input0.data[ row + (c * params.ld) ];\\n    if (OMITNAN) { if (!isNan(v)) { acc = acc + v; } } else { if (isNan(v)) { saw_nan = true; } else { acc = acc + v; } }\\n    c += 256u;\\n  }\\n\");\n        }\n        shader.push_str(\"  var<workgroup> tile: array<f32, 256u>;\\n  tile[lid.x] = acc;\\n  workgroupBarrier();\\n\");\n        shader.push_str(\n            \"  var off = 128u;\\n  loop { if (off == 0u) { break; } if (lid.x < off) {\\n    let a = tile[lid.x]; let b = tile[lid.x + off];\\n    if (!is_omitnan(params.flags) && (isNan(a) || isNan(b))) { tile[lid.x] = f32(NaN); } else { tile[lid.x] = a + b; }\\n  } workgroupBarrier(); off = off / 2u; }\\n\",\n        );\n        if axis == 0 {\n            shader.push_str(\"  if (lid.x == 0u) { output.data[col] = tile[0u]; }\\n}\\n\");\n        } else {\n            shader.push_str(\"  if (lid.x == 0u) { output.data[row] = tile[0u]; }\\n}\\n\");\n        }\n        Some(shader)\n    }\n}\n\nimpl FusionGroup {\n    pub fn element_count(&self) -> Option<usize> {\n        match &self.shape {\n            ShapeInfo::Scalar => Some(1),\n            ShapeInfo::Tensor(dims) => dims.iter().try_fold(1usize, |acc, dim| {\n                if let Some(size) = dim.and_then(|d| acc.checked_mul(d)) {\n                    Some(size)\n                } else {\n                    None\n                }\n            }),\n            ShapeInfo::Unknown => None,\n        }\n    }\n}\n\nimpl FusionKind {\n    pub fn is_elementwise(&self) -> bool {\n        matches!(self, FusionKind::ElementwiseChain)\n    }\n\n    pub fn is_reduction(&self) -> bool {\n        matches!(self, FusionKind::Reduction)\n    }\n}\n\nfn primitive_expr(\n    op: PrimitiveOp,\n    inputs: &[ValueId],\n    exprs: &HashMap<ValueId, String>,\n) -> Option<String> {\n    let binary = |exprs: &HashMap<ValueId, String>| -> Option<(String, String)> {\n        let lhs = exprs.get(inputs.get(0)?).cloned()?;\n        let rhs = exprs.get(inputs.get(1)?).cloned()?;\n        Some((lhs, rhs))\n    };\n    match op {\n        PrimitiveOp::Add => {\n            let (lhs, rhs) = binary(exprs)?;\n            Some(format!(\"({lhs} + {rhs})\"))\n        }\n        PrimitiveOp::Sub => {\n            let (lhs, rhs) = binary(exprs)?;\n            Some(format!(\"({lhs} - {rhs})\"))\n        }\n        PrimitiveOp::Mul | PrimitiveOp::ElemMul => {\n            let (lhs, rhs) = binary(exprs)?;\n            Some(format!(\"({lhs} * {rhs})\"))\n        }\n        PrimitiveOp::Div | PrimitiveOp::ElemDiv | PrimitiveOp::ElemLeftDiv => {\n            let (lhs, rhs) = binary(exprs)?;\n            Some(format!(\"({lhs} / {rhs})\"))\n        }\n        PrimitiveOp::Pow | PrimitiveOp::ElemPow => {\n            let (lhs, rhs) = binary(exprs)?;\n            Some(format!(\"pow({lhs}, {rhs})\"))\n        }\n        PrimitiveOp::Neg => {\n            let arg = exprs.get(inputs.get(0)?).cloned()?;\n            Some(format!(\"(-{arg})\"))\n        }\n        PrimitiveOp::UPlus => {\n            let arg = exprs.get(inputs.get(0)?).cloned()?;\n            Some(format!(\"(+{arg})\"))\n        }\n        _ => None,\n    }\n}\n\nfn builtin_expr(\n    name: &str,\n    inputs: &[ValueId],\n    exprs: &HashMap<ValueId, String>,\n    scalar_ty: &str,\n) -> Option<String> {\n    let func = match name.to_ascii_lowercase().as_str() {\n        \"sin\" => \"sin\",\n        \"cos\" => \"cos\",\n        \"tan\" => \"tan\",\n        \"asin\" => \"asin\",\n        \"acos\" => \"acos\",\n        \"atan\" => \"atan\",\n        \"atan2\" => return builtin_binary(\"atan2\", inputs, exprs),\n        \"sinh\" => \"sinh\",\n        \"cosh\" => \"cosh\",\n        \"tanh\" => \"tanh\",\n        \"exp\" => \"exp\",\n        \"log\" => \"log\",\n        \"log2\" => \"log2\",\n        \"sqrt\" => \"sqrt\",\n        \"abs\" => \"abs\",\n        \"exp2\" => \"exp2\",\n        \"floor\" => \"floor\",\n        \"ceil\" => \"ceil\",\n        \"round\" => \"round\",\n        \"trunc\" => \"trunc\",\n        _ => {\n            return match name.to_ascii_lowercase().as_str() {\n                \"log10\" => {\n                    let arg = exprs.get(inputs.get(0)?).cloned()?;\n                    let constant = cast_literal(scalar_ty, \"0.4342944819032518\");\n                    Some(format!(\"(log({arg}) * {constant})\"))\n                }\n                \"log1p\" => {\n                    let arg = exprs.get(inputs.get(0)?).cloned()?;\n                    let one = cast_literal(scalar_ty, \"1.0\");\n                    Some(format!(\"log({arg} + {one})\"))\n                }\n                \"expm1\" => {\n                    let arg = exprs.get(inputs.get(0)?).cloned()?;\n                    let one = cast_literal(scalar_ty, \"1.0\");\n                    Some(format!(\"(exp({arg}) - {one})\"))\n                }\n                _ => None,\n            }\n        }\n    };\n    let arg = exprs.get(inputs.get(0)?).cloned()?;\n    Some(format!(\"{func}({arg})\"))\n}\n\nfn builtin_binary(\n    func: &str,\n    inputs: &[ValueId],\n    exprs: &HashMap<ValueId, String>,\n) -> Option<String> {\n    let lhs = exprs.get(inputs.get(0)?).cloned()?;\n    let rhs = exprs.get(inputs.get(1)?).cloned()?;\n    Some(format!(\"{func}({lhs}, {rhs})\"))\n}\n\nfn cast_literal(scalar_ty: &str, literal: &str) -> String {\n    if scalar_ty == \"f64\" {\n        format!(\"{scalar_ty}({literal})\")\n    } else {\n        literal.to_string()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::graph::{\n        AccelGraph, AccelGraphTag, AccelNode, AccelNodeLabel, AccelOpCategory, InstrSpan,\n        PrimitiveOp, ValueId, ValueInfo, ValueOrigin, VarKind,\n    };\n    use runmat_builtins::Type;\n    use std::collections::HashMap as StdHashMap;\n\n    fn simple_elementwise_graph() -> AccelGraph {\n        let mut values = Vec::new();\n        // Value 0: input tensor\n        values.push(ValueInfo {\n            id: 0,\n            origin: ValueOrigin::Variable {\n                kind: VarKind::Global,\n                index: 0,\n            },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4), Some(4)]),\n            constant: None,\n        });\n        // Node 0 output value (value id 1)\n        values.push(ValueInfo {\n            id: 1,\n            origin: ValueOrigin::NodeOutput { node: 0, output: 0 },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4), Some(4)]),\n            constant: None,\n        });\n        // Node 1 output value (value id 2)\n        values.push(ValueInfo {\n            id: 2,\n            origin: ValueOrigin::NodeOutput { node: 1, output: 0 },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4), Some(4)]),\n            constant: None,\n        });\n\n        let node0 = AccelNode {\n            id: 0,\n            label: AccelNodeLabel::Primitive(PrimitiveOp::ElemMul),\n            category: AccelOpCategory::Elementwise,\n            inputs: vec![0, 0],\n            outputs: vec![1],\n            span: InstrSpan { start: 10, end: 10 },\n            tags: vec![AccelGraphTag::Elementwise],\n        };\n        let node1 = AccelNode {\n            id: 1,\n            label: AccelNodeLabel::Primitive(PrimitiveOp::ElemMul),\n            category: AccelOpCategory::Elementwise,\n            inputs: vec![1, 0],\n            outputs: vec![2],\n            span: InstrSpan { start: 11, end: 11 },\n            tags: vec![AccelGraphTag::Elementwise],\n        };\n\n        AccelGraph {\n            nodes: vec![node0, node1],\n            values,\n        }\n    }\n\n    #[test]\n    fn detects_chain() {\n        let graph = simple_elementwise_graph();\n        let groups = detect_fusion_groups(&graph);\n        assert_eq!(groups.len(), 1);\n        let group = &groups[0];\n        assert_eq!(group.nodes, vec![0, 1]);\n        assert_eq!(group.kind, FusionKind::ElementwiseChain);\n    }\n\n    #[test]\n    fn builds_plan_and_template() {\n        let graph = simple_elementwise_graph();\n        let groups = detect_fusion_groups(&graph);\n        let plan = FusionPlan::from_graph(&graph, &groups);\n        assert_eq!(plan.groups.len(), 1);\n        let group_plan = &plan.groups[0];\n        assert!(group_plan.kernel.supported);\n        let wgsl = group_plan.generate_wgsl(\"f32\").expect(\"wgsl\");\n        assert!(wgsl.contains(\"@compute\"));\n        assert!(group_plan.group.element_count().is_some());\n    }\n\n    #[test]\n    fn stack_pattern_tracks_repeated_constants() {\n        let mut values = Vec::new();\n        values.push(ValueInfo {\n            id: 0,\n            origin: ValueOrigin::Variable {\n                kind: VarKind::Global,\n                index: 0,\n            },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4)]),\n            constant: None,\n        });\n        values.push(ValueInfo {\n            id: 1,\n            origin: ValueOrigin::Constant,\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4)]),\n            constant: None,\n        });\n        values.push(ValueInfo {\n            id: 2,\n            origin: ValueOrigin::NodeOutput { node: 0, output: 0 },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4)]),\n            constant: None,\n        });\n        values.push(ValueInfo {\n            id: 3,\n            origin: ValueOrigin::NodeOutput { node: 1, output: 0 },\n            ty: Type::tensor(),\n            shape: ShapeInfo::Tensor(vec![Some(4)]),\n            constant: None,\n        });\n\n        let node0 = AccelNode {\n            id: 0,\n            label: AccelNodeLabel::Primitive(PrimitiveOp::Add),\n            category: AccelOpCategory::Elementwise,\n            inputs: vec![0, 1],\n            outputs: vec![2],\n            span: InstrSpan { start: 5, end: 5 },\n            tags: vec![AccelGraphTag::Elementwise],\n        };\n        let node1 = AccelNode {\n            id: 1,\n            label: AccelNodeLabel::Primitive(PrimitiveOp::Add),\n            category: AccelOpCategory::Elementwise,\n            inputs: vec![2, 1],\n            outputs: vec![3],\n            span: InstrSpan { start: 6, end: 6 },\n            tags: vec![AccelGraphTag::Elementwise],\n        };\n\n        let graph = AccelGraph {\n            nodes: vec![node0, node1],\n            values,\n        };\n\n        let groups = detect_fusion_groups(&graph);\n        assert_eq!(groups.len(), 1);\n        let plan = FusionPlan::from_graph(&graph, &groups);\n        let group_plan = &plan.groups[0];\n        assert_eq!(group_plan.inputs.len(), 2);\n        assert_eq!(group_plan.stack_pattern.len(), 2);\n        assert!(group_plan.stack_pattern.iter().all(|idx| *idx == 1));\n    }\n\n    #[test]\n    fn builtin_expr_supports_extended_set() {\n        let mut exprs: StdHashMap<ValueId, String> = StdHashMap::new();\n        exprs.insert(0, \"v0\".to_string());\n        exprs.insert(1, \"v1\".to_string());\n\n        let log1p = super::builtin_expr(\"log1p\", &[0], &exprs, \"f32\");\n        assert!(log1p.is_some());\n\n        let log10 = super::builtin_expr(\"log10\", &[0], &exprs, \"f64\");\n        assert!(log10.unwrap().contains(\"log\"));\n\n        let expm1 = super::builtin_expr(\"expm1\", &[0], &exprs, \"f32\");\n        assert!(expm1.unwrap().contains(\"exp\"));\n\n        let floor = super::builtin_expr(\"floor\", &[0], &exprs, \"f32\");\n        assert_eq!(floor.unwrap(), \"floor(v0)\");\n\n        let atan2 = super::builtin_expr(\"atan2\", &[0, 1], &exprs, \"f32\");\n        assert_eq!(atan2.unwrap(), \"atan2(v0, v1)\");\n    }\n}\n\n===== crates/runmat-builtins/src/lib.rs =====\npub use inventory;\nuse runmat_gc_api::GcPtr;\nuse std::collections::HashMap;\nuse std::convert::TryFrom;\nuse std::fmt;\n\n#[derive(Debug, Clone, PartialEq)]\npub enum Value {\n    Int(IntValue),\n    Num(f64),\n    /// Complex scalar value represented as (re, im)\n    Complex(f64, f64),\n    Bool(bool),\n    // Logical array (N-D of booleans). Scalars use Bool.\n    LogicalArray(LogicalArray),\n    String(String),\n    // String array (R2016b+): N-D array of string scalars\n    StringArray(StringArray),\n    // Char array (single-quoted): 2-D character array (rows x cols)\n    CharArray(CharArray),\n    Tensor(Tensor),\n    /// Complex numeric array; same column-major shape semantics as `Tensor`\n    ComplexTensor(ComplexTensor),\n    Cell(CellArray),\n    // Struct (scalar or nested). Struct arrays are represented in higher layers;\n    // this variant holds a single struct's fields.\n    Struct(StructValue),\n    // GPU-resident tensor handle (opaque; buffer managed by backend)\n    GpuTensor(runmat_accelerate_api::GpuTensorHandle),\n    // Simple object instance until full class system lands\n    Object(ObjectInstance),\n    /// Handle-object wrapper providing identity semantics and validity tracking\n    HandleObject(HandleRef),\n    /// Event listener handle for events\n    Listener(Listener),\n    // Function handle pointing to a named function (builtin or user)\n    FunctionHandle(String),\n    Closure(Closure),\n    ClassRef(String),\n    MException(MException),\n}\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum IntValue {\n    I8(i8),\n    I16(i16),\n    I32(i32),\n    I64(i64),\n    U8(u8),\n    U16(u16),\n    U32(u32),\n    U64(u64),\n}\n\nimpl IntValue {\n    pub fn to_i64(&self) -> i64 {\n        match self {\n            IntValue::I8(v) => *v as i64,\n            IntValue::I16(v) => *v as i64,\n            IntValue::I32(v) => *v as i64,\n            IntValue::I64(v) => *v,\n            IntValue::U8(v) => *v as i64,\n            IntValue::U16(v) => *v as i64,\n            IntValue::U32(v) => *v as i64,\n            IntValue::U64(v) => {\n                if *v > i64::MAX as u64 {\n                    i64::MAX\n                } else {\n                    *v as i64\n                }\n            }\n        }\n    }\n    pub fn to_f64(&self) -> f64 {\n        self.to_i64() as f64\n    }\n    pub fn is_zero(&self) -> bool {\n        self.to_i64() == 0\n    }\n    pub fn class_name(&self) -> &'static str {\n        match self {\n            IntValue::I8(_) => \"int8\",\n            IntValue::I16(_) => \"int16\",\n            IntValue::I32(_) => \"int32\",\n            IntValue::I64(_) => \"int64\",\n            IntValue::U8(_) => \"uint8\",\n            IntValue::U16(_) => \"uint16\",\n            IntValue::U32(_) => \"uint32\",\n            IntValue::U64(_) => \"uint64\",\n        }\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct StructValue {\n    pub fields: HashMap<String, Value>,\n}\n\nimpl StructValue {\n    pub fn new() -> Self {\n        Self {\n            fields: HashMap::new(),\n        }\n    }\n}\n\nimpl Default for StructValue {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct Tensor {\n    pub data: Vec<f64>,\n    pub shape: Vec<usize>, // Column-major layout\n    pub rows: usize,       // Compatibility for 2D usage\n    pub cols: usize,       // Compatibility for 2D usage\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct ComplexTensor {\n    pub data: Vec<(f64, f64)>,\n    pub shape: Vec<usize>,\n    pub rows: usize,\n    pub cols: usize,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct StringArray {\n    pub data: Vec<String>,\n    pub shape: Vec<usize>,\n    pub rows: usize,\n    pub cols: usize,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct LogicalArray {\n    pub data: Vec<u8>, // 0 or 1 values; compact bitset can come later\n    pub shape: Vec<usize>,\n}\n\nimpl LogicalArray {\n    pub fn new(data: Vec<u8>, shape: Vec<usize>) -> Result<Self, String> {\n        let expected: usize = shape.iter().product();\n        if data.len() != expected {\n            return Err(format!(\n                \"LogicalArray data length {} doesn't match shape {:?} ({} elements)\",\n                data.len(),\n                shape,\n                expected\n            ));\n        }\n        // Normalize to 0/1\n        let mut d = data;\n        for v in &mut d {\n            *v = if *v != 0 { 1 } else { 0 };\n        }\n        Ok(LogicalArray { data: d, shape })\n    }\n    pub fn zeros(shape: Vec<usize>) -> Self {\n        let expected: usize = shape.iter().product();\n        LogicalArray {\n            data: vec![0u8; expected],\n            shape,\n        }\n    }\n    pub fn len(&self) -> usize {\n        self.data.len()\n    }\n    pub fn is_empty(&self) -> bool {\n        self.data.is_empty()\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct CharArray {\n    pub data: Vec<char>,\n    pub rows: usize,\n    pub cols: usize,\n}\n\nimpl CharArray {\n    pub fn new_row(s: &str) -> Self {\n        CharArray {\n            data: s.chars().collect(),\n            rows: 1,\n            cols: s.chars().count(),\n        }\n    }\n    pub fn new(data: Vec<char>, rows: usize, cols: usize) -> Result<Self, String> {\n        if rows * cols != data.len() {\n            return Err(format!(\n                \"Char data length {} doesn't match dimensions {}x{}\",\n                data.len(),\n                rows,\n                cols\n            ));\n        }\n        Ok(CharArray { data, rows, cols })\n    }\n}\n\nimpl StringArray {\n    pub fn new(data: Vec<String>, shape: Vec<usize>) -> Result<Self, String> {\n        let expected: usize = shape.iter().product();\n        if data.len() != expected {\n            return Err(format!(\n                \"StringArray data length {} doesn't match shape {:?} ({} elements)\",\n                data.len(),\n                shape,\n                expected\n            ));\n        }\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        Ok(StringArray {\n            data,\n            shape,\n            rows,\n            cols,\n        })\n    }\n    pub fn new_2d(data: Vec<String>, rows: usize, cols: usize) -> Result<Self, String> {\n        Self::new(data, vec![rows, cols])\n    }\n    pub fn rows(&self) -> usize {\n        self.shape.first().copied().unwrap_or(1)\n    }\n    pub fn cols(&self) -> usize {\n        self.shape.get(1).copied().unwrap_or(1)\n    }\n}\n\n// GpuTensorHandle now lives in runmat-accel-api\n\nimpl Tensor {\n    pub fn new(data: Vec<f64>, shape: Vec<usize>) -> Result<Self, String> {\n        let expected: usize = shape.iter().product();\n        if data.len() != expected {\n            return Err(format!(\n                \"Tensor data length {} doesn't match shape {:?} ({} elements)\",\n                data.len(),\n                shape,\n                expected\n            ));\n        }\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        Ok(Tensor {\n            data,\n            shape,\n            rows,\n            cols,\n        })\n    }\n\n    pub fn new_2d(data: Vec<f64>, rows: usize, cols: usize) -> Result<Self, String> {\n        Self::new(data, vec![rows, cols])\n    }\n\n    pub fn zeros(shape: Vec<usize>) -> Self {\n        let size: usize = shape.iter().product();\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        Tensor {\n            data: vec![0.0; size],\n            shape,\n            rows,\n            cols,\n        }\n    }\n\n    pub fn ones(shape: Vec<usize>) -> Self {\n        let size: usize = shape.iter().product();\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        Tensor {\n            data: vec![1.0; size],\n            shape,\n            rows,\n            cols,\n        }\n    }\n\n    // 2D helpers for transitional call sites\n    pub fn zeros2(rows: usize, cols: usize) -> Self {\n        Self::zeros(vec![rows, cols])\n    }\n    pub fn ones2(rows: usize, cols: usize) -> Self {\n        Self::ones(vec![rows, cols])\n    }\n\n    pub fn rows(&self) -> usize {\n        self.shape.first().copied().unwrap_or(1)\n    }\n    pub fn cols(&self) -> usize {\n        self.shape.get(1).copied().unwrap_or(1)\n    }\n\n    pub fn get2(&self, row: usize, col: usize) -> Result<f64, String> {\n        let rows = self.rows();\n        let cols = self.cols();\n        if row >= rows || col >= cols {\n            return Err(format!(\n                \"Index ({row}, {col}) out of bounds for {rows}x{cols} tensor\"\n            ));\n        }\n        // Column-major linearization: lin = row + col*rows\n        Ok(self.data[row + col * rows])\n    }\n\n    pub fn set2(&mut self, row: usize, col: usize, value: f64) -> Result<(), String> {\n        let rows = self.rows();\n        let cols = self.cols();\n        if row >= rows || col >= cols {\n            return Err(format!(\n                \"Index ({row}, {col}) out of bounds for {rows}x{cols} tensor\"\n            ));\n        }\n        // Column-major linearization\n        self.data[row + col * rows] = value;\n        Ok(())\n    }\n\n    pub fn scalar_to_tensor2(scalar: f64, rows: usize, cols: usize) -> Tensor {\n        Tensor {\n            data: vec![scalar; rows * cols],\n            shape: vec![rows, cols],\n            rows,\n            cols,\n        }\n    }\n    // No-compat constructors: prefer new/new_2d/zeros/zeros2/ones/ones2\n}\n\nimpl ComplexTensor {\n    pub fn new(data: Vec<(f64, f64)>, shape: Vec<usize>) -> Result<Self, String> {\n        let expected: usize = shape.iter().product();\n        if data.len() != expected {\n            return Err(format!(\n                \"ComplexTensor data length {} doesn't match shape {:?} ({} elements)\",\n                data.len(),\n                shape,\n                expected\n            ));\n        }\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        Ok(ComplexTensor {\n            data,\n            shape,\n            rows,\n            cols,\n        })\n    }\n    pub fn new_2d(data: Vec<(f64, f64)>, rows: usize, cols: usize) -> Result<Self, String> {\n        Self::new(data, vec![rows, cols])\n    }\n    pub fn zeros(shape: Vec<usize>) -> Self {\n        let size: usize = shape.iter().product();\n        let (rows, cols) = if shape.len() >= 2 {\n            (shape[0], shape[1])\n        } else if shape.len() == 1 {\n            (1, shape[0])\n        } else {\n            (0, 0)\n        };\n        ComplexTensor {\n            data: vec![(0.0, 0.0); size],\n            shape,\n            rows,\n            cols,\n        }\n    }\n}\n\nimpl fmt::Display for Tensor {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self.shape.len() {\n            0 | 1 => {\n                // Treat as row vector for display\n                write!(f, \"[\")?;\n                for (i, v) in self.data.iter().enumerate() {\n                    if i > 0 {\n                        write!(f, \" \")?;\n                    }\n                    write!(f, \"{}\", format_number_short_g(*v))?;\n                }\n                write!(f, \"]\")\n            }\n            2 => {\n                let rows = self.rows();\n                let cols = self.cols();\n                write!(f, \"[\")?;\n                for r in 0..rows {\n                    for c in 0..cols {\n                        if c > 0 {\n                            write!(f, \" \")?;\n                        }\n                        let v = self.data[r + c * rows];\n                        write!(f, \"{}\", format_number_short_g(v))?;\n                    }\n                    if r + 1 < rows {\n                        write!(f, \"; \")?;\n                    }\n                }\n                write!(f, \"]\")\n            }\n            _ => write!(f, \"Tensor(shape={:?})\", self.shape),\n        }\n    }\n}\n\nimpl fmt::Display for StringArray {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self.shape.len() {\n            0 | 1 => {\n                write!(f, \"[\")?;\n                for (i, v) in self.data.iter().enumerate() {\n                    if i > 0 {\n                        write!(f, \" \")?;\n                    }\n                    let escaped = v.replace('\"', \"\\\\\\\"\");\n                    write!(f, \"\\\"{escaped}\\\"\")?;\n                }\n                write!(f, \"]\")\n            }\n            2 => {\n                let rows = self.rows();\n                let cols = self.cols();\n                write!(f, \"[\")?;\n                for r in 0..rows {\n                    for c in 0..cols {\n                        if c > 0 {\n                            write!(f, \" \")?;\n                        }\n                        let v = &self.data[r + c * rows];\n                        let escaped = v.replace('\"', \"\\\\\\\"\");\n                        write!(f, \"\\\"{escaped}\\\"\")?;\n                    }\n                    if r + 1 < rows {\n                        write!(f, \"; \")?;\n                    }\n                }\n                write!(f, \"]\")\n            }\n            _ => write!(f, \"StringArray(shape={:?})\", self.shape),\n        }\n    }\n}\n\nimpl fmt::Display for LogicalArray {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self.shape.len() {\n            0 => write!(f, \"[]\"),\n            1 => {\n                write!(f, \"[\")?;\n                for (i, v) in self.data.iter().enumerate() {\n                    if i > 0 {\n                        write!(f, \" \")?;\n                    }\n                    write!(f, \"{}\", if *v != 0 { 1 } else { 0 })?;\n                }\n                write!(f, \"]\")\n            }\n            2 => {\n                let rows = self.shape[0];\n                let cols = self.shape[1];\n                write!(f, \"[\")?;\n                for r in 0..rows {\n                    for c in 0..cols {\n                        if c > 0 {\n                            write!(f, \" \")?;\n                        }\n                        let idx = r + c * rows;\n                        write!(f, \"{}\", if self.data[idx] != 0 { 1 } else { 0 })?;\n                    }\n                    if r + 1 < rows {\n                        write!(f, \"; \")?;\n                    }\n                }\n                write!(f, \"]\")\n            }\n            _ => write!(f, \"LogicalArray(shape={:?})\", self.shape),\n        }\n    }\n}\n\nimpl fmt::Display for CharArray {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        // Display as single-quoted rows separated by ;\n        write!(f, \"[\")?;\n        for r in 0..self.rows {\n            if r > 0 {\n                write!(f, \"; \")?;\n            }\n            write!(f, \"'\")?;\n            for c in 0..self.cols {\n                let ch = self.data[r * self.cols + c];\n                if ch == '\\'' {\n                    write!(f, \"''\")?;\n                } else {\n                    write!(f, \"{ch}\")?;\n                }\n            }\n            write!(f, \"'\")?;\n        }\n        write!(f, \"]\")\n    }\n}\n\n// From implementations for Value\nimpl From<i32> for Value {\n    fn from(i: i32) -> Self {\n        Value::Int(IntValue::I32(i))\n    }\n}\nimpl From<i64> for Value {\n    fn from(i: i64) -> Self {\n        Value::Int(IntValue::I64(i))\n    }\n}\nimpl From<u32> for Value {\n    fn from(i: u32) -> Self {\n        Value::Int(IntValue::U32(i))\n    }\n}\nimpl From<u64> for Value {\n    fn from(i: u64) -> Self {\n        Value::Int(IntValue::U64(i))\n    }\n}\nimpl From<i16> for Value {\n    fn from(i: i16) -> Self {\n        Value::Int(IntValue::I16(i))\n    }\n}\nimpl From<i8> for Value {\n    fn from(i: i8) -> Self {\n        Value::Int(IntValue::I8(i))\n    }\n}\nimpl From<u16> for Value {\n    fn from(i: u16) -> Self {\n        Value::Int(IntValue::U16(i))\n    }\n}\nimpl From<u8> for Value {\n    fn from(i: u8) -> Self {\n        Value::Int(IntValue::U8(i))\n    }\n}\n\nimpl From<f64> for Value {\n    fn from(f: f64) -> Self {\n        Value::Num(f)\n    }\n}\n\nimpl From<bool> for Value {\n    fn from(b: bool) -> Self {\n        Value::Bool(b)\n    }\n}\n\nimpl From<String> for Value {\n    fn from(s: String) -> Self {\n        Value::String(s)\n    }\n}\n\nimpl From<&str> for Value {\n    fn from(s: &str) -> Self {\n        Value::String(s.to_string())\n    }\n}\n\nimpl From<Tensor> for Value {\n    fn from(m: Tensor) -> Self {\n        Value::Tensor(m)\n    }\n}\n\n// Remove blanket From<Vec<Value>> to avoid losing shape information\n\n// TryFrom implementations for extracting native types\nimpl TryFrom<&Value> for i32 {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::Int(i) => Ok(i.to_i64() as i32),\n            Value::Num(n) => Ok(*n as i32),\n            _ => Err(format!(\"cannot convert {v:?} to i32\")),\n        }\n    }\n}\n\nimpl TryFrom<&Value> for f64 {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::Num(n) => Ok(*n),\n            Value::Int(i) => Ok(i.to_f64()),\n            _ => Err(format!(\"cannot convert {v:?} to f64\")),\n        }\n    }\n}\n\nimpl TryFrom<&Value> for bool {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::Bool(b) => Ok(*b),\n            Value::Int(i) => Ok(!i.is_zero()),\n            Value::Num(n) => Ok(*n != 0.0),\n            _ => Err(format!(\"cannot convert {v:?} to bool\")),\n        }\n    }\n}\n\nimpl TryFrom<&Value> for String {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::String(s) => Ok(s.clone()),\n            Value::StringArray(sa) => {\n                if sa.data.len() == 1 {\n                    Ok(sa.data[0].clone())\n                } else {\n                    Err(\"cannot convert string array to scalar string\".to_string())\n                }\n            }\n            Value::CharArray(ca) => {\n                // Convert full char array to one string if it is a single row; else error\n                if ca.rows == 1 {\n                    Ok(ca.data.iter().collect())\n                } else {\n                    Err(\"cannot convert multi-row char array to scalar string\".to_string())\n                }\n            }\n            Value::Int(i) => Ok(i.to_i64().to_string()),\n            Value::Num(n) => Ok(n.to_string()),\n            Value::Bool(b) => Ok(b.to_string()),\n            _ => Err(format!(\"cannot convert {v:?} to String\")),\n        }\n    }\n}\n\nimpl TryFrom<&Value> for Tensor {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::Tensor(m) => Ok(m.clone()),\n            _ => Err(format!(\"cannot convert {v:?} to Tensor\")),\n        }\n    }\n}\n\nimpl TryFrom<&Value> for Value {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        Ok(v.clone())\n    }\n}\n\nimpl TryFrom<&Value> for Vec<Value> {\n    type Error = String;\n    fn try_from(v: &Value) -> Result<Self, Self::Error> {\n        match v {\n            Value::Cell(c) => Ok(c.data.iter().map(|p| (**p).clone()).collect()),\n            _ => Err(format!(\"cannot convert {v:?} to Vec<Value>\")),\n        }\n    }\n}\n\nuse serde::{Deserialize, Serialize};\n\n/// Enhanced type system used throughout RunMat for HIR and builtin functions\n/// Designed to mirror Value variants for better type inference and LSP support\n#[derive(Debug, PartialEq, Eq, Clone, Serialize, Deserialize)]\npub enum Type {\n    /// Integer number type\n    Int,\n    /// Floating-point number type  \n    Num,\n    /// Boolean type\n    Bool,\n    /// Logical array type (N-D boolean array)\n    Logical,\n    /// String type\n    String,\n    /// Tensor type with optional shape information (column-major semantics in runtime)\n    Tensor {\n        /// Optional full shape; None means unknown/dynamic; individual dims can be omitted by using None\n        shape: Option<Vec<Option<usize>>>,\n    },\n    /// Cell array type with optional element type information\n    Cell {\n        /// Optional element type (None means mixed/unknown)\n        element_type: Option<Box<Type>>,\n        /// Optional length (None means unknown/dynamic)\n        length: Option<usize>,\n    },\n    /// Function type with parameter and return types\n    Function {\n        /// Parameter types\n        params: Vec<Type>,\n        /// Return type\n        returns: Box<Type>,\n    },\n    /// Void type (no value)\n    Void,\n    /// Unknown type (for type inference)\n    Unknown,\n    /// Union type (multiple possible types)\n    Union(Vec<Type>),\n    /// Struct-like type with optional known field set (purely for inference)\n    Struct {\n        /// Optional set of known field names observed via control-flow (None = unknown fields)\n        known_fields: Option<Vec<String>>, // kept sorted unique for deterministic Eq\n    },\n}\n\nimpl Type {\n    /// Create a tensor type with unknown shape\n    pub fn tensor() -> Self {\n        Type::Tensor { shape: None }\n    }\n\n    /// Create a tensor type with known shape\n    pub fn tensor_with_shape(shape: Vec<usize>) -> Self {\n        Type::Tensor {\n            shape: Some(shape.into_iter().map(Some).collect()),\n        }\n    }\n\n    /// Create a cell array type with unknown element type\n    pub fn cell() -> Self {\n        Type::Cell {\n            element_type: None,\n            length: None,\n        }\n    }\n\n    /// Create a cell array type with known element type\n    pub fn cell_of(element_type: Type) -> Self {\n        Type::Cell {\n            element_type: Some(Box::new(element_type)),\n            length: None,\n        }\n    }\n\n    /// Check if this type is compatible with another type\n    pub fn is_compatible_with(&self, other: &Type) -> bool {\n        match (self, other) {\n            (Type::Unknown, _) | (_, Type::Unknown) => true,\n            (Type::Int, Type::Num) | (Type::Num, Type::Int) => true, // Number compatibility\n            (Type::Tensor { .. }, Type::Tensor { .. }) => true, // Tensor compatibility regardless of dims for now\n            (a, b) => a == b,\n        }\n    }\n\n    /// Get the most specific common type between two types\n    pub fn unify(&self, other: &Type) -> Type {\n        match (self, other) {\n            (Type::Unknown, t) | (t, Type::Unknown) => t.clone(),\n            (Type::Int, Type::Num) | (Type::Num, Type::Int) => Type::Num,\n            (Type::Tensor { .. }, Type::Tensor { .. }) => Type::tensor(), // Lose shape info for now\n            (Type::Struct { known_fields: a }, Type::Struct { known_fields: b }) => match (a, b) {\n                (None, None) => Type::Struct { known_fields: None },\n                (Some(ka), None) | (None, Some(ka)) => Type::Struct {\n                    known_fields: Some(ka.clone()),\n                },\n                (Some(ka), Some(kb)) => {\n                    let mut set: std::collections::BTreeSet<String> = ka.iter().cloned().collect();\n                    set.extend(kb.iter().cloned());\n                    Type::Struct {\n                        known_fields: Some(set.into_iter().collect()),\n                    }\n                }\n            },\n            (a, b) if a == b => a.clone(),\n            _ => Type::Union(vec![self.clone(), other.clone()]),\n        }\n    }\n\n    /// Infer type from a Value\n    pub fn from_value(value: &Value) -> Type {\n        match value {\n            Value::Int(_) => Type::Int,\n            Value::Num(_) => Type::Num,\n            Value::Complex(_, _) => Type::Num, // treat as numeric double (complex) in type system for now\n            Value::Bool(_) => Type::Bool,\n            Value::LogicalArray(_) => Type::Logical,\n            Value::String(_) => Type::String,\n            Value::StringArray(_sa) => {\n                // Model as Cell of String for type system for now\n                Type::cell_of(Type::String)\n            }\n            Value::Tensor(t) => Type::Tensor {\n                shape: Some(t.shape.iter().map(|&d| Some(d)).collect()),\n            },\n            Value::ComplexTensor(t) => Type::Tensor {\n                shape: Some(t.shape.iter().map(|&d| Some(d)).collect()),\n            },\n            Value::Cell(cells) => {\n                if cells.data.is_empty() {\n                    Type::cell()\n                } else {\n                    // Infer element type from first element\n                    let element_type = Type::from_value(&cells.data[0]);\n                    Type::Cell {\n                        element_type: Some(Box::new(element_type)),\n                        length: Some(cells.data.len()),\n                    }\n                }\n            }\n            Value::GpuTensor(h) => Type::Tensor {\n                shape: Some(h.shape.iter().map(|&d| Some(d)).collect()),\n            },\n            Value::Object(_) => Type::Unknown,\n            Value::HandleObject(_) => Type::Unknown,\n            Value::Listener(_) => Type::Unknown,\n            Value::Struct(_) => Type::Struct { known_fields: None },\n            Value::FunctionHandle(_) => Type::Function {\n                params: vec![Type::Unknown],\n                returns: Box::new(Type::Unknown),\n            },\n            Value::Closure(_) => Type::Function {\n                params: vec![Type::Unknown],\n                returns: Box::new(Type::Unknown),\n            },\n            Value::ClassRef(_) => Type::Unknown,\n            Value::MException(_) => Type::Unknown,\n            Value::CharArray(ca) => {\n                // Treat as cell of char for type purposes; or a 2-D char matrix conceptually\n                Type::Cell {\n                    element_type: Some(Box::new(Type::String)),\n                    length: Some(ca.rows * ca.cols),\n                }\n            }\n        }\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct Closure {\n    pub function_name: String,\n    pub captures: Vec<Value>,\n}\n\n/// Acceleration metadata describing GPU-friendly characteristics of a builtin.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum AccelTag {\n    Unary,\n    Elementwise,\n    Reduction,\n    MatMul,\n    Transpose,\n}\n\n/// Simple builtin function definition using the unified type system\n#[derive(Debug, Clone)]\npub struct BuiltinFunction {\n    pub name: &'static str,\n    pub description: &'static str,\n    pub category: &'static str,\n    pub doc: &'static str,\n    pub examples: &'static str,\n    pub param_types: Vec<Type>,\n    pub return_type: Type,\n    pub implementation: fn(&[Value]) -> Result<Value, String>,\n    pub accel_tags: &'static [AccelTag],\n    pub is_sink: bool,\n}\n\nimpl BuiltinFunction {\n    #[allow(clippy::too_many_arguments)]\n    pub fn new(\n        name: &'static str,\n        description: &'static str,\n        category: &'static str,\n        doc: &'static str,\n        examples: &'static str,\n        param_types: Vec<Type>,\n        return_type: Type,\n        implementation: fn(&[Value]) -> Result<Value, String>,\n        accel_tags: &'static [AccelTag],\n        is_sink: bool,\n    ) -> Self {\n        Self {\n            name,\n            description,\n            category,\n            doc,\n            examples,\n            param_types,\n            return_type,\n            implementation,\n            accel_tags,\n            is_sink,\n        }\n    }\n}\n\n/// A constant value that can be accessed as a variable\n#[derive(Clone)]\npub struct Constant {\n    pub name: &'static str,\n    pub value: Value,\n}\n\nimpl std::fmt::Debug for Constant {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"Constant {{ name: {:?}, value: {:?} }}\",\n            self.name, self.value\n        )\n    }\n}\n\ninventory::collect!(BuiltinFunction);\ninventory::collect!(Constant);\n\npub fn builtin_functions() -> Vec<&'static BuiltinFunction> {\n    inventory::iter::<BuiltinFunction>().collect()\n}\n\npub fn constants() -> Vec<&'static Constant> {\n    inventory::iter::<Constant>().collect()\n}\n\n// ----------------------\n// Builtin documentation metadata (optional, registered by macros)\n// ----------------------\n\n#[derive(Debug)]\npub struct BuiltinDoc {\n    pub name: &'static str,\n    pub category: Option<&'static str>,\n    pub summary: Option<&'static str>,\n    pub keywords: Option<&'static str>,\n    pub errors: Option<&'static str>,\n    pub related: Option<&'static str>,\n    pub introduced: Option<&'static str>,\n    pub status: Option<&'static str>,\n    pub examples: Option<&'static str>,\n}\n\ninventory::collect!(BuiltinDoc);\n\npub fn builtin_docs() -> Vec<&'static BuiltinDoc> {\n    inventory::iter::<BuiltinDoc>().collect()\n}\n\n// ----------------------\n// Display implementations\n// ----------------------\n\nfn format_number_short_g(value: f64) -> String {\n    if value.is_nan() {\n        return \"NaN\".to_string();\n    }\n    if value.is_infinite() {\n        return if value.is_sign_negative() {\n            \"-Inf\"\n        } else {\n            \"Inf\"\n        }\n        .to_string();\n    }\n    // Normalize -0.0 to 0\n    let mut v = value;\n    if v == 0.0 {\n        v = 0.0;\n    }\n\n    let abs = v.abs();\n    if abs == 0.0 {\n        return \"0\".to_string();\n    }\n\n    // Decide between fixed and scientific notation roughly like short g\n    let use_scientific = !(1e-4..1e6).contains(&abs);\n\n    if use_scientific {\n        // 5 significant digits in scientific notation for short g style\n        let s = format!(\"{v:.5e}\");\n        // Trim trailing zeros in fraction part\n        if let Some(idx) = s.find('e') {\n            let (mut mantissa, exp) = s.split_at(idx);\n            // mantissa like \"-1.23450\"\n            if let Some(dot_idx) = mantissa.find('.') {\n                // Trim trailing zeros\n                let mut end = mantissa.len();\n                while end > dot_idx + 1 && mantissa.as_bytes()[end - 1] == b'0' {\n                    end -= 1;\n                }\n                if end > 0 && mantissa.as_bytes()[end - 1] == b'.' {\n                    end -= 1;\n                }\n                mantissa = &mantissa[..end];\n            }\n            return format!(\"{mantissa}{exp}\");\n        }\n        return s;\n    }\n\n    // Fixed notation with up to 12 significant digits, trim trailing zeros\n    // Compute number of decimals to retain to reach ~12 significant digits\n    let exp10 = abs.log10().floor() as i32; // position of most significant digit\n    let sig_digits: i32 = 12;\n    let decimals = (sig_digits - 1 - exp10).clamp(0, 12) as usize;\n    // Round to that many decimals\n    let pow = 10f64.powi(decimals as i32);\n    let rounded = (v * pow).round() / pow;\n    let mut s = format!(\"{rounded:.decimals$}\");\n    if let Some(dot) = s.find('.') {\n        // Trim trailing zeros\n        let mut end = s.len();\n        while end > dot + 1 && s.as_bytes()[end - 1] == b'0' {\n            end -= 1;\n        }\n        if end > 0 && s.as_bytes()[end - 1] == b'.' {\n            end -= 1;\n        }\n        s.truncate(end);\n    }\n    if s.is_empty() || s == \"-0\" {\n        s = \"0\".to_string();\n    }\n    s\n}\n\n// -------- Exception type --------\n#[derive(Debug, Clone, PartialEq)]\npub struct MException {\n    pub identifier: String,\n    pub message: String,\n    pub stack: Vec<String>,\n}\n\nimpl MException {\n    pub fn new(identifier: String, message: String) -> Self {\n        Self {\n            identifier,\n            message,\n            stack: Vec::new(),\n        }\n    }\n}\n\n/// Reference to a GC-allocated object providing language handle semantics\n#[derive(Debug, Clone)]\npub struct HandleRef {\n    pub class_name: String,\n    pub target: GcPtr<Value>,\n    pub valid: bool,\n}\n\nimpl PartialEq for HandleRef {\n    fn eq(&self, other: &Self) -> bool {\n        let a = unsafe { self.target.as_raw() } as usize;\n        let b = unsafe { other.target.as_raw() } as usize;\n        a == b\n    }\n}\n\n/// Event listener handle for events\n#[derive(Debug, Clone, PartialEq)]\npub struct Listener {\n    pub id: u64,\n    pub target: GcPtr<Value>,\n    pub event_name: String,\n    pub callback: GcPtr<Value>,\n    pub enabled: bool,\n    pub valid: bool,\n}\n\nimpl Listener {\n    pub fn class_name(&self) -> String {\n        match unsafe { &*self.target.as_raw() } {\n            Value::Object(o) => o.class_name.clone(),\n            Value::HandleObject(h) => h.class_name.clone(),\n            _ => String::new(),\n        }\n    }\n}\n\nimpl fmt::Display for Value {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Value::Int(i) => write!(f, \"{}\", i.to_i64()),\n            Value::Num(n) => write!(f, \"{}\", format_number_short_g(*n)),\n            Value::Complex(re, im) => {\n                if *im == 0.0 {\n                    write!(f, \"{}\", format_number_short_g(*re))\n                } else if *re == 0.0 {\n                    write!(f, \"{}i\", format_number_short_g(*im))\n                } else if *im < 0.0 {\n                    write!(\n                        f,\n                        \"{}-{}i\",\n                        format_number_short_g(*re),\n                        format_number_short_g(im.abs())\n                    )\n                } else {\n                    write!(\n                        f,\n                        \"{}+{}i\",\n                        format_number_short_g(*re),\n                        format_number_short_g(*im)\n                    )\n                }\n            }\n            Value::Bool(b) => write!(f, \"{}\", if *b { 1 } else { 0 }),\n            Value::LogicalArray(la) => write!(f, \"{la}\"),\n            Value::String(s) => write!(f, \"'{s}'\"),\n            Value::StringArray(sa) => write!(f, \"{sa}\"),\n            Value::CharArray(ca) => write!(f, \"{ca}\"),\n            Value::Tensor(m) => write!(f, \"{m}\"),\n            Value::ComplexTensor(m) => write!(f, \"{m}\"),\n            Value::Cell(ca) => ca.fmt(f),\n\n            Value::GpuTensor(h) => write!(\n                f,\n                \"GpuTensor(shape={:?}, device={}, buffer={})\",\n                h.shape, h.device_id, h.buffer_id\n            ),\n            Value::Object(obj) => write!(f, \"{}(props={})\", obj.class_name, obj.properties.len()),\n            Value::HandleObject(h) => {\n                let ptr = unsafe { h.target.as_raw() } as usize;\n                write!(\n                    f,\n                    \"<handle {} @0x{:x} valid={}>\",\n                    h.class_name, ptr, h.valid\n                )\n            }\n            Value::Listener(l) => {\n                let ptr = unsafe { l.target.as_raw() } as usize;\n                write!(\n                    f,\n                    \"<listener id={} {}@0x{:x} '{}' enabled={} valid={}>\",\n                    l.id,\n                    l.class_name(),\n                    ptr,\n                    l.event_name,\n                    l.enabled,\n                    l.valid\n                )\n            }\n            Value::Struct(st) => write!(f, \"struct(fields={})\", st.fields.len()),\n            Value::FunctionHandle(name) => write!(f, \"@{name}\"),\n            Value::Closure(c) => write!(\n                f,\n                \"<closure {} captures={}>\",\n                c.function_name,\n                c.captures.len()\n            ),\n            Value::ClassRef(name) => write!(f, \"<class {name}>\"),\n            Value::MException(e) => write!(\n                f,\n                \"MException(identifier='{}', message='{}')\",\n                e.identifier, e.message\n            ),\n        }\n    }\n}\n\nimpl fmt::Display for ComplexTensor {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self.shape.len() {\n            0 | 1 => {\n                write!(f, \"[\")?;\n                for (i, (re, im)) in self.data.iter().enumerate() {\n                    if i > 0 {\n                        write!(f, \" \")?;\n                    }\n                    let s = Value::Complex(*re, *im).to_string();\n                    write!(f, \"{s}\")?;\n                }\n                write!(f, \"]\")\n            }\n            2 => {\n                let rows = self.rows;\n                let cols = self.cols;\n                write!(f, \"[\")?;\n                for r in 0..rows {\n                    for c in 0..cols {\n                        if c > 0 {\n                            write!(f, \" \")?;\n                        }\n                        let (re, im) = self.data[r + c * rows];\n                        let s = Value::Complex(re, im).to_string();\n                        write!(f, \"{s}\")?;\n                    }\n                    if r + 1 < rows {\n                        write!(f, \"; \")?;\n                    }\n                }\n                write!(f, \"]\")\n            }\n            _ => write!(f, \"ComplexTensor(shape={:?})\", self.shape),\n        }\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct CellArray {\n    pub data: Vec<GcPtr<Value>>,\n    pub rows: usize,\n    pub cols: usize,\n}\n\nimpl CellArray {\n    pub fn new_handles(\n        handles: Vec<GcPtr<Value>>,\n        rows: usize,\n        cols: usize,\n    ) -> Result<Self, String> {\n        if rows * cols != handles.len() {\n            return Err(format!(\n                \"Cell data length {} doesn't match dimensions {}x{}\",\n                handles.len(),\n                rows,\n                cols\n            ));\n        }\n        Ok(CellArray {\n            data: handles,\n            rows,\n            cols,\n        })\n    }\n    pub fn new(data: Vec<Value>, rows: usize, cols: usize) -> Result<Self, String> {\n        if rows * cols != data.len() {\n            return Err(format!(\n                \"Cell data length {} doesn't match dimensions {}x{}\",\n                data.len(),\n                rows,\n                cols\n            ));\n        }\n        // Note: data will be allocated into GC handles by callers (runtime/ignition) to avoid builtins↔gc cycles\n        let handles: Vec<GcPtr<Value>> = data\n            .into_iter()\n            .map(|v| unsafe { GcPtr::from_raw(Box::into_raw(Box::new(v))) })\n            .collect();\n        Ok(CellArray {\n            data: handles,\n            rows,\n            cols,\n        })\n    }\n\n    pub fn get(&self, row: usize, col: usize) -> Result<Value, String> {\n        if row >= self.rows || col >= self.cols {\n            return Err(format!(\n                \"Cell index ({row}, {col}) out of bounds for {}x{} cell array\",\n                self.rows, self.cols\n            ));\n        }\n        Ok((*self.data[row * self.cols + col]).clone())\n    }\n}\n\nimpl fmt::Display for CellArray {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        write!(f, \"{{\")?;\n        for r in 0..self.rows {\n            for c in 0..self.cols {\n                if c > 0 {\n                    write!(f, \", \")?;\n                }\n                let v = &self.data[r * self.cols + c];\n                write!(f, \"{}\", **v)?;\n            }\n            if r + 1 < self.rows {\n                write!(f, \"; \")?;\n            }\n        }\n        write!(f, \"}}\")\n    }\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub struct ObjectInstance {\n    pub class_name: String,\n    pub properties: HashMap<String, Value>,\n}\n\nimpl ObjectInstance {\n    pub fn new(class_name: String) -> Self {\n        Self {\n            class_name,\n            properties: HashMap::new(),\n        }\n    }\n}\n\n// -------- Class registry (scaffolding) --------\n#[derive(Debug, Clone, PartialEq, Eq)]\npub enum Access {\n    Public,\n    Private,\n}\n\n#[derive(Debug, Clone)]\npub struct PropertyDef {\n    pub name: String,\n    pub is_static: bool,\n    pub is_dependent: bool,\n    pub get_access: Access,\n    pub set_access: Access,\n    pub default_value: Option<Value>,\n}\n\n#[derive(Debug, Clone)]\npub struct MethodDef {\n    pub name: String,\n    pub is_static: bool,\n    pub access: Access,\n    pub function_name: String, // bound runtime builtin/user func name\n}\n\n#[derive(Debug, Clone)]\npub struct ClassDef {\n    pub name: String, // namespaced e.g. pkg.Point\n    pub parent: Option<String>,\n    pub properties: HashMap<String, PropertyDef>,\n    pub methods: HashMap<String, MethodDef>,\n}\n\nuse std::sync::{Mutex, OnceLock};\n\nstatic CLASS_REGISTRY: OnceLock<Mutex<HashMap<String, ClassDef>>> = OnceLock::new();\nstatic STATIC_VALUES: OnceLock<Mutex<HashMap<(String, String), Value>>> = OnceLock::new();\n\nfn registry() -> &'static Mutex<HashMap<String, ClassDef>> {\n    CLASS_REGISTRY.get_or_init(|| Mutex::new(HashMap::new()))\n}\n\npub fn register_class(def: ClassDef) {\n    let mut m = registry().lock().unwrap();\n    m.insert(def.name.clone(), def);\n}\n\npub fn get_class(name: &str) -> Option<ClassDef> {\n    registry().lock().unwrap().get(name).cloned()\n}\n\n/// Resolve a property through the inheritance chain, returning the property definition and\n/// the name of the class where it was defined.\npub fn lookup_property(class_name: &str, prop: &str) -> Option<(PropertyDef, String)> {\n    let reg = registry().lock().unwrap();\n    let mut current = Some(class_name.to_string());\n    let guard: Option<std::sync::MutexGuard<'_, std::collections::HashMap<String, ClassDef>>> =\n        None;\n    drop(guard);\n    while let Some(name) = current {\n        if let Some(cls) = reg.get(&name) {\n            if let Some(p) = cls.properties.get(prop) {\n                return Some((p.clone(), name));\n            }\n            current = cls.parent.clone();\n        } else {\n            break;\n        }\n    }\n    None\n}\n\n/// Resolve a method through the inheritance chain, returning the method definition and\n/// the name of the class where it was defined.\npub fn lookup_method(class_name: &str, method: &str) -> Option<(MethodDef, String)> {\n    let reg = registry().lock().unwrap();\n    let mut current = Some(class_name.to_string());\n    while let Some(name) = current {\n        if let Some(cls) = reg.get(&name) {\n            if let Some(m) = cls.methods.get(method) {\n                return Some((m.clone(), name));\n            }\n            current = cls.parent.clone();\n        } else {\n            break;\n        }\n    }\n    None\n}\n\nfn static_values() -> &'static Mutex<HashMap<(String, String), Value>> {\n    STATIC_VALUES.get_or_init(|| Mutex::new(HashMap::new()))\n}\n\npub fn get_static_property_value(class_name: &str, prop: &str) -> Option<Value> {\n    static_values()\n        .lock()\n        .unwrap()\n        .get(&(class_name.to_string(), prop.to_string()))\n        .cloned()\n}\n\npub fn set_static_property_value(class_name: &str, prop: &str, value: Value) {\n    static_values()\n        .lock()\n        .unwrap()\n        .insert((class_name.to_string(), prop.to_string()), value);\n}\n\n/// Set a static property, resolving the defining ancestor class for storage.\npub fn set_static_property_value_in_owner(\n    class_name: &str,\n    prop: &str,\n    value: Value,\n) -> Result<(), String> {\n    if let Some((_p, owner)) = lookup_property(class_name, prop) {\n        set_static_property_value(&owner, prop, value);\n        Ok(())\n    } else {\n        Err(format!(\"Unknown static property '{class_name}.{prop}'\"))\n    }\n}\n\n===== crates/runmat-runtime/BUILTIN_PACKAGING.md =====\n# Builtin Packaging & Authoring Blueprint\n\nThis document captures the end-state we want for builtin authoring, GPU integration, documentation, and automation. It is the go-to reference when wiring new builtins or extending the RunMat Function Manager tooling.\n\n## Goals\n- One Rust source file per builtin containing code, long-form documentation, GPU/fusion specs, and unit tests.\n- Inventory-backed metadata that fuels both the runtime (Ignition/Turbine + Accelerate) and authoring tools.\n- First-class support for scalar and variadic signatures, GPU offload, fusion planning, and BLAS/LAPACK fallbacks.\n- Tooling that can emit structured docs for the Next.js site and drive Codex-based authoring sessions.\n\n## Source Layout\n```\ncrates/runmat-runtime/\n  src/\n    builtins/\n      mod.rs              # category re-exports, shared helpers\n      common/             # shared utilities (complex math, GPU helpers, test support)\n      math/\n        sin.rs\n        sum.rs\n        ...\n      array/\n        zeros.rs\n        ...\n      accel/\n        gpu_array.rs\n        ...\n      ...                 # other categories (io, introspection, strings, etc.)\n```\n- `builtins/mod.rs` exposes category modules and re-exports existing function symbols to keep downstream code compiling.\n- Shared helpers live under `builtins/common/`. They must never perform registration; builtin files call into them explicitly.\n- Each builtin file is self-contained: documentation constant, specs, one or more `#[runtime_builtin]` annotated functions, helper routines, and tests.\n\n## Builtin Template Checklist\n1. `//!` file doc comment summarising the builtin.\n2. `use` statements scoped to required helpers.\n3. `pub const DOC_MD: &str = r#\"...\"#;` containing YAML frontmatter + Markdown (details below).\n4. Optional `pub const GPU_SPEC: BuiltinGpuSpec` and `pub const FUSION_SPEC: BuiltinFusionSpec`, registered via helper macros.\n5. One or more `#[runtime_builtin(..., doc_md = DOC_MD, ...)]` functions. Variadic signatures use a trailing `Vec<Value>` parameter, e.g. `rest: Vec<Value>`. The runtime macro already detects this pattern and passes the remaining arguments through.\n6. Helper functions (private) to keep the annotated functions concise. Host/GPU split helpers are common.\n7. `#[cfg(test)] mod tests` covering: scalars, array/broadcast, variadic combinations, GPU provider execution (under `feature = \"native-accel\"`), and doc example smoke tests via the shared test harness.\n\n## Inline Documentation Expectations\n- YAML frontmatter should cover `title`, `category`, `keywords`, `summary`, `references`, `gpu_support`, `fusion`, `tested`, and any flags relevant to BLAS/LAPACK usage.\n- Markdown body should explain numerics, broadcasting, error behaviour, GPU semantics (including how Accelerate fuses kernels and manages residency), and thorough examples of usage within the MATLAB language syntax. Aim for 5-10 examples, and pick examples based on the most common use cases that would be searched on a search engine for when using the function.\n- Encourage users to understand GPU offload: describe gpuArray creation, gather, and the lazy execution model (Ignition + Accelerate detect fusion opportunities, queue kernels, and execute on demand).\n\n## GPU & Fusion Spec Types\n- Authoritative implementations of types live in `crates/runmat-runtime/src/builtins/common/spec.rs`. The Function Manager links against the same module to stay in sync.\n- `register_builtin_*` macros submit the specs into inventory so Accelerate and the Function Manager can discover them.\n- Provider hooks map to methods exposed by `runmat-accelerate-api`. For reductions, add `ProviderHook::Reduction { name: \"reduce_sum\" }`.\n- `notes` must stay concise (one or two sentences) and focus on actionable implementation details: provider prerequisites, fallbacks, precision caveats, or residency expectations. Avoid repeating long-form documentation that already exists in `DOC_MD`.\n\n### Planner Constants and Fusion\n- Builtins should document any constants the planner will inline (e.g. `dim`, `'omitnan'`).\n- Use `constant_strategy: InlineLiteral` for small immediates; prefer uniform buffers for runtime switches across kernels.\n- Reductions must clearly define output shape for `dim=1` (column-wise) vs `dim=2` (row-wise), and behavior for dims > ndims.\n\n### Two-Pass Reductions\n- When `reduce_len` exceeds the workgroup size, providers should use a two-pass kernel:\n  - Pass 1: per-slice partial reductions (one workgroup per slice × chunks).\n  - Pass 2: reduce partials across chunks.\n- Builtins should set `two_pass_threshold` and optionally `workgroup_size` to guide generation.\n- NaN handling (`omitnan`/`includenan`) must be honored consistently across passes.\n\n### Testing and Benchmarks\n- Provide CPU vs GPU parity tests covering:\n  - dim=1 and dim=2; include/omitnan; empty and scalar edge cases.\n  - Fused producer → reduction (e.g., `sin(X).*X + 2` → `sum(..., dim)`).\n  - Large shapes to validate two-pass speedup; include warmup to amortize pipeline compile.\n\n### FunMatFunc Authoring Hints\n- Include a minimal checklist in each builtin for argument parsing, planner constants, GPU spec fields, fusion WGSL body, and examples.\n- Keep `DOC_MD` examples executable; prefer small, deterministic inputs for CI.\n\n## BLAS / LAPACK Integration Points\n- BLAS/LAPACK-backed builtins live under `src/blas.rs` and `src/lapack.rs` guarded by `#[cfg(feature = \"blas-lapack\")]`.\n- When authoring builtins that rely on these crates, mention the feature flag in `DOC_MD` (`requires_feature: blas-lapack`) and in the GPU spec notes if relevant.\n- The Function Manager should check cargo features and warn when attempting to run tests that require BLAS/LAPACK but the feature is disabled.\n\n## RunMat Function Manager Snapshot\n- Binary crate at `tools/runmatfunc/`.\n- Responsibilities: discover builtin manifests, assemble authoring contexts, launch interactive/headless Codex sessions, run targeted tests, emit documentation bundles, and manage job queues.\n- CLI surface (initial):\n  - `runmatfunc builtin <name> [--headless] [--model ...]`\n  - `runmatfunc browse`\n  - `runmatfunc docs emit`\n  - `runmatfunc queue add/run`\n  - `runmatfunc list`\n- Documentation export writes `docs/generated/builtins.json` and `docs/generated/builtins.d.ts` for the Next.js site.\n\n## Legacy Module Cleanup\n- Legacy modules are located in `crates/runmat-runtime/src/builtins/*.rs` (e.g. not in a subdirectory).\n- Once you are finished authoring a module, search the legacy modules for any legacy implementations of the function and remove them, and clean up any references to them in the codebase.\n\n===== crates/runmat-runtime/src/builtins/common/spec.rs =====\nuse std::fmt;\n\n/// Supported scalar precisions that GPU kernels may target.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ScalarType {\n    F32,\n    F64,\n    I32,\n    Bool,\n}\n\n/// High-level GPU operation kind for builtin categorisation.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum GpuOpKind {\n    Elementwise,\n    Reduction,\n    MatMul,\n    Transpose,\n    Custom(&'static str),\n}\n\n/// Broadcast semantics supported by the builtin.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum BroadcastSemantics {\n    Matlab,\n    ScalarOnly,\n    None,\n}\n\n/// Hook names that providers may implement for specialised kernels.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ProviderHook {\n    Unary {\n        name: &'static str,\n    },\n    Binary {\n        name: &'static str,\n        commutative: bool,\n    },\n    Reduction {\n        name: &'static str,\n    },\n    Custom(&'static str),\n}\n\n/// Strategy used when embedding constants in fused kernels.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ConstantStrategy {\n    InlineLiteral,\n    UniformBuffer,\n    WorkgroupMemory,\n}\n\n/// Residency policy for builtin outputs.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ResidencyPolicy {\n    InheritInputs,\n    NewHandle,\n    GatherImmediately,\n}\n\n/// How reductions should treat NaN values.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ReductionNaN {\n    Include,\n    Omit,\n}\n\n/// Shape requirements for fused kernels.\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ShapeRequirements {\n    BroadcastCompatible,\n    Exact(&'static [usize]),\n    Any,\n}\n\n/// Context provided to fusion expression builders.\npub struct FusionExprContext<'a> {\n    pub scalar_ty: ScalarType,\n    pub inputs: &'a [&'a str],\n    pub constants: &'a [&'a str],\n}\n\n/// Builder used to generate WGSL expressions.\npub type FusionExprBuilder = fn(&FusionExprContext) -> Result<String, FusionError>;\n\n/// Description of a fusion kernel template.\n#[derive(Clone)]\npub struct FusionKernelTemplate {\n    pub scalar_precisions: &'static [ScalarType],\n    pub wgsl_body: FusionExprBuilder,\n}\n\n/// Possible errors emitted by a fusion builder.\n#[derive(Debug)]\npub enum FusionError {\n    MissingInput(usize),\n    UnsupportedPrecision(ScalarType),\n    Message(&'static str),\n}\n\n/// GPU metadata registered alongside builtin functions.\n#[derive(Debug, Clone, Copy)]\npub struct BuiltinGpuSpec {\n    pub name: &'static str,\n    pub op_kind: GpuOpKind,\n    pub supported_precisions: &'static [ScalarType],\n    pub broadcast: BroadcastSemantics,\n    pub provider_hooks: &'static [ProviderHook],\n    pub constant_strategy: ConstantStrategy,\n    pub residency: ResidencyPolicy,\n    pub nan_mode: ReductionNaN,\n    /// If set, reductions with reduce_len greater than this should prefer a two-pass kernel.\n    pub two_pass_threshold: Option<usize>,\n    /// Optional workgroup size hint for generated kernels.\n    pub workgroup_size: Option<u32>,\n    /// Whether the provider hook (if used) supports device-side omitnan handling.\n    pub accepts_nan_mode: bool,\n    pub notes: &'static str,\n}\n\nimpl fmt::Display for FusionError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            FusionError::MissingInput(idx) => write!(f, \"missing input {}\", idx),\n            FusionError::UnsupportedPrecision(ty) => write!(f, \"unsupported precision {:?}\", ty),\n            FusionError::Message(msg) => write!(f, \"{msg}\"),\n        }\n    }\n}\n\nimpl std::error::Error for FusionError {}\n\n/// Fusion metadata registered alongside builtin functions.\n#[derive(Clone)]\npub struct BuiltinFusionSpec {\n    pub name: &'static str,\n    pub shape: ShapeRequirements,\n    pub constant_strategy: ConstantStrategy,\n    pub elementwise: Option<FusionKernelTemplate>,\n    pub reduction: Option<FusionKernelTemplate>,\n    pub emits_nan: bool,\n    pub notes: &'static str,\n}\n\n/// Inventory wrapper for GPU specs.\npub struct GpuSpecInventory {\n    pub spec: &'static BuiltinGpuSpec,\n}\n\n/// Inventory wrapper for fusion specs.\npub struct FusionSpecInventory {\n    pub spec: &'static BuiltinFusionSpec,\n}\n\ninventory::collect!(GpuSpecInventory);\ninventory::collect!(FusionSpecInventory);\n\n/// Iterate all registered GPU specs.\npub fn builtin_gpu_specs() -> impl Iterator<Item = &'static BuiltinGpuSpec> {\n    inventory::iter::<GpuSpecInventory>()\n        .into_iter()\n        .map(|entry| entry.spec)\n}\n\n/// Iterate all registered fusion specs.\npub fn builtin_fusion_specs() -> impl Iterator<Item = &'static BuiltinFusionSpec> {\n    inventory::iter::<FusionSpecInventory>()\n        .into_iter()\n        .map(|entry| entry.spec)\n}\n\nimpl fmt::Debug for BuiltinFusionSpec {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"BuiltinFusionSpec\")\n            .field(\"name\", &self.name)\n            .field(\"shape\", &self.shape)\n            .field(\"emits_nan\", &self.emits_nan)\n            .finish()\n    }\n}\n\n#[macro_export]\nmacro_rules! register_builtin_gpu_spec {\n    ($spec:expr) => {\n        inventory::submit! {\n            $crate::builtins::common::spec::GpuSpecInventory { spec: &$spec }\n        }\n    };\n}\n\n#[macro_export]\nmacro_rules! register_builtin_fusion_spec {\n    ($spec:expr) => {\n        inventory::submit! {\n            $crate::builtins::common::spec::FusionSpecInventory { spec: &$spec }\n        }\n    };\n}\n\n===== crates/runmat-runtime/src/builtins/math/reduction/sum.rs =====\n//! Elementwise and reduction sum builtin for RunMat.\n//!\n//! This implementation mirrors MATLAB semantics, including optional `'omitnan'` handling,\n//! tiered GPU execution through RunMat Accelerate, and fusion metadata for the native planner.\n\nuse runmat_accelerate_api::GpuTensorHandle;\nuse runmat_builtins::{Tensor, Value};\nuse runmat_macros::runtime_builtin;\n\nuse crate::builtins::common::spec::{\n    BroadcastSemantics, BuiltinFusionSpec, BuiltinGpuSpec, ConstantStrategy, FusionError,\n    FusionExprContext, FusionKernelTemplate, GpuOpKind, ProviderHook, ReductionNaN,\n    ResidencyPolicy, ScalarType, ShapeRequirements,\n};\nuse crate::builtins::common::{gpu_helpers, tensor};\nuse crate::{register_builtin_fusion_spec, register_builtin_gpu_spec};\n\n#[cfg_attr(not(test), allow(dead_code))]\npub const DOC_MD: &str = r#\"---\ntitle: \"sum\"\ncategory: \"math/reduction\"\nkeywords: [\"sum\", \"reduction\", \"gpu\", \"omitnan\"]\nsummary: \"Sum elements of scalars, vectors, matrices, or N-D tensors.\"\nreferences: []\ngpu_support:\n  elementwise: false\n  reduction: true\n  precisions: [\"f32\", \"f64\"]\n  broadcasting: \"matlab\"\n  notes: \"Falls back to host when omitnan is requested or the active provider lacks reduction hooks.\"\nfusion:\n  elementwise: false\n  reduction: true\n  max_inputs: 1\n  constants: \"inline\"\nrequires_feature: null\ntested:\n  unit: \"builtins::math::reduction::sum::tests\"\n  integration: \"builtins::math::reduction::sum::tests::sum_gpu_provider_roundtrip\"\n---\n\n# MATLAB / RunMat `sum` Function\n`sum(x)` adds together elements of scalars, vectors, matrices, and higher-dimensional tensors.\nWhen no dimension is supplied, the reduction runs along the first non-singleton dimension.\n\n## Behaviour\n- `sum(X)` on an `m × n` matrix returns a row vector (`1 × n`) with column sums.\n- `sum(X, 2)` returns a column vector (`m × 1`) containing row sums.\n- Logical inputs are promoted to double precision (`true → 1.0`, `false → 0.0`).\n- `sum(..., 'omitnan')` ignores `NaN` values; if all entries are `NaN`, the result becomes `0`.\n- `sum(..., 'includenan')` (default) propagates `NaN` when any element in the slice is `NaN`.\n- Empty slices return zeros with MATLAB-compatible shape semantics.\n- Dimensions larger than `ndims(X)` leave the input unchanged.\n\n## GPU Execution\nWhen RunMat Accelerate is active, tensors that already reside on the device stay on the GPU.\nProviders may implement `reduce_sum_dim` or `reduce_sum` for fast execution; otherwise\nRunMat transparently gathers the data and falls back to the host implementation.\n`'omitnan'` always uses the host path today because providers do not yet accept NaN policies.\n\n## Examples\n\n```matlab\nA = [1 2 3; 4 5 6];\ncolSums = sum(A);      % [5 7 9]\nrowSums = sum(A, 2);   % [6; 15]\n```\n\n```matlab\nvalues = [1 NaN 3];\ntotal = sum(values, 'omitnan');   % 4\n```\n\n```matlab\nG = gpuArray(rand(1024, 1024));\nenergy = sum(G .^ 2);\nresult = gather(energy);          % column sums computed on the device when supported\n```\n\n## See Also\n[`prod`], [`mean`], [`cumsum`], [`gpuArray`], [`gather`]\n\"#;\n\npub const GPU_SPEC: BuiltinGpuSpec = BuiltinGpuSpec {\n    name: \"sum\",\n    op_kind: GpuOpKind::Reduction,\n    supported_precisions: &[ScalarType::F32, ScalarType::F64],\n    broadcast: BroadcastSemantics::Matlab,\n    provider_hooks: &[ProviderHook::Reduction {\n            name: \"reduce_sum_dim\",\n    }],\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    residency: ResidencyPolicy::NewHandle,\n    nan_mode: ReductionNaN::Include,\n    two_pass_threshold: Some(256),\n    workgroup_size: Some(256),\n    accepts_nan_mode: true,\n    notes:\n        \"Providers may specialise reduce_sum_dim / reduce_sum; omitnan falls back to the CPU path.\",\n};\n\nregister_builtin_gpu_spec!(GPU_SPEC);\n\npub const FUSION_SPEC: BuiltinFusionSpec = BuiltinFusionSpec {\n    name: \"sum\",\n    shape: ShapeRequirements::BroadcastCompatible,\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    elementwise: None,\n    reduction: Some(FusionKernelTemplate {\n        scalar_precisions: &[ScalarType::F32, ScalarType::F64],\n        wgsl_body: |ctx: &FusionExprContext| {\n            let input = ctx.inputs.get(0).ok_or(FusionError::MissingInput(0))?;\n            Ok(format!(\"accumulator += {input};\"))\n        },\n    }),\n    emits_nan: false,\n    notes: \"Planner emits a standard column-major reduction template; providers can substitute custom kernels.\",\n};\n\nregister_builtin_fusion_spec!(FUSION_SPEC);\n\n#[runtime_builtin(\n    name = \"sum\",\n    category = \"math/reduction\",\n    summary = \"Sum elements of scalars, vectors, matrices, or N-D tensors.\",\n    keywords = \"sum,reduction,gpu,omitnan\",\n    accel = \"reduction\"\n)]\nfn sum_builtin(value: Value, rest: Vec<Value>) -> Result<Value, String> {\n    let (dim, nan_mode) = parse_arguments(&rest)?;\n    match value {\n        Value::GpuTensor(handle) => sum_gpu(handle, dim, nan_mode),\n        other => sum_host(other, dim, nan_mode),\n    }\n}\n\nfn parse_arguments(args: &[Value]) -> Result<(Option<usize>, ReductionNaN), String> {\n    match args.len() {\n        0 => Ok((None, ReductionNaN::Include)),\n        1 => {\n            if let Some(mode) = parse_nan_mode(&args[0])? {\n                Ok((None, mode))\n            } else {\n                let dim = tensor::parse_dimension(&args[0], \"sum\")?;\n                Ok((Some(dim), ReductionNaN::Include))\n            }\n        }\n        2 => {\n            // Accept either order: (dim, mode) or (mode, dim)\n            if let Some(mode) = parse_nan_mode(&args[0])? {\n                // (mode, dim)\n                let dim = tensor::parse_dimension(&args[1], \"sum\")?;\n                Ok((Some(dim), mode))\n            } else {\n                // (dim, mode)\n                let dim = tensor::parse_dimension(&args[0], \"sum\")?;\n                if let Some(mode) = parse_nan_mode(&args[1])? {\n                    Ok((Some(dim), mode))\n                } else {\n                    Err(\"sum: expected 'omitnan' or 'includenan' as the third argument\".to_string())\n                }\n            }\n        }\n        _ => Err(\"sum: unsupported arguments\".to_string()),\n    }\n}\n\nfn parse_nan_mode(value: &Value) -> Result<Option<ReductionNaN>, String> {\n    let text = match value {\n        Value::String(s) => Some(s.clone()),\n        Value::StringArray(sa) if sa.data.len() == 1 => Some(sa.data[0].clone()),\n        Value::CharArray(ca) if ca.rows == 1 => Some(ca.data.iter().collect()),\n        _ => None,\n    };\n    let Some(text) = text else {\n        return Ok(None);\n    };\n    let trimmed = text.trim();\n    let lowered = trimmed.to_ascii_lowercase();\n    match lowered.as_str() {\n        \"omitnan\" => Ok(Some(ReductionNaN::Omit)),\n        \"includenan\" => Ok(Some(ReductionNaN::Include)),\n        _ => Err(format!(\"sum: unknown reduction mode '{trimmed}'\")),\n    }\n}\n\nfn sum_host(value: Value, dim: Option<usize>, nan_mode: ReductionNaN) -> Result<Value, String> {\n    let tensor = tensor::value_into_tensor(value)?;\n    let target_dim = dim.unwrap_or_else(|| default_dimension(&tensor));\n    let reduced = reduce_tensor_dim(&tensor, target_dim, nan_mode)?;\n    Ok(tensor::tensor_into_value(reduced))\n}\n\nfn sum_gpu(\n    handle: GpuTensorHandle,\n    dim: Option<usize>,\n    nan_mode: ReductionNaN,\n) -> Result<Value, String> {\n    let target_dim = dim.unwrap_or_else(|| default_dimension_from_shape(&handle.shape));\n\n    if target_dim == 0 {\n        return Err(\"sum: dimension must be >= 1\".to_string());\n    }\n\n    let Some(target_shape) = reduction_shape(&handle.shape, target_dim) else {\n        return Ok(Value::GpuTensor(handle));\n    };\n\n    if nan_mode == ReductionNaN::Include {\n        if let Some(provider) = runmat_accelerate_api::provider() {\n            let zero_based = target_dim.saturating_sub(1);\n            if zero_based < handle.shape.len() {\n                if let Ok(device_result) = provider.reduce_sum_dim(&handle, zero_based) {\n                    return Ok(Value::GpuTensor(device_result));\n                }\n            }\n\n            if tensor::element_count(&target_shape) == 1 {\n                if let Ok(device_result) = provider.reduce_sum(&handle) {\n                    return Ok(Value::GpuTensor(device_result));\n                }\n            }\n        }\n    } else if nan_mode == ReductionNaN::Omit {\n        // Device path via generic fused reduction with omitnan baked into the shader.\n        if let Some(provider) = runmat_accelerate_api::provider() {\n            let axis = target_dim.saturating_sub(1);\n            if handle.shape.len() == 2 && axis <= 1 {\n                let rows = *handle.shape.get(0).unwrap_or(&1);\n                let cols = *handle.shape.get(1).unwrap_or(&1);\n                let (reduce_len, num_slices, axis_is_row) = if axis == 0 {\n                    (rows, cols, false)\n                } else {\n                    (cols, rows, true)\n                };\n                let output_shape = reduction_shape(&handle.shape, target_dim)\n                    .unwrap_or_else(|| vec![num_slices]);\n                let scalar_ty = match provider.precision() {\n                    runmat_accelerate_api::ProviderPrecision::F32 => \"f32\",\n                    runmat_accelerate_api::ProviderPrecision::F64 => \"f64\",\n                };\n                // Minimal WGSL with omitnan=true\n                let mut shader = String::new();\n                shader.push_str(&format!(\"struct Tensor {{ data: array<{scalar_ty}>; }}\\n\"));\n                shader.push_str(\"struct MParams { nrows: u32, ncols: u32, ld: u32, flags: u32 }\\n\\n\");\n                shader.push_str(\"@group(0) @binding(0) var<storage, read> input0: Tensor;\\n\");\n                shader.push_str(\"@group(0) @binding(1) var<storage, read_write> output: Tensor;\\n\");\n                shader.push_str(\"@group(0) @binding(2) var<uniform> params: MParams;\\n\\n\");\n                shader.push_str(\"@compute @workgroup_size(256)\\n\");\n                if axis_is_row {\n                    // Row-wise: one output per row; reduce over columns\n                    shader.push_str(\n                        \"fn main(@builtin(local_invocation_id) lid: vec3<u32>, @builtin(workgroup_id) wid: vec3<u32>) {\\n\",\n                    );\n                    shader.push_str(\"  let row = wid.x; if (row >= params.nrows) { return; }\\n\");\n                    shader.push_str(&format!(\"  var acc: {scalar_ty} = {}0.0;\\n\", if scalar_ty==\"f64\" { \"f64(\" } else { \"\" }));\n                    if scalar_ty==\"f64\" { shader.push_str(\"  // close f64 literal\\n\"); }\n                    shader.push_str(\"  var c = lid.x;\\n  while (c < params.ncols) {\\n    let v = input0.data[row + (c * params.ld)];\\n    if (!isNan(v)) { acc = acc + v; }\\n    c += 256u;\\n  }\\n\");\n                } else {\n                    // Column-wise: one output per column; reduce over rows\n                    shader.push_str(\n                        \"fn main(@builtin(local_invocation_id) lid: vec3<u32>, @builtin(workgroup_id) wid: vec3<u32>) {\\n\",\n                    );\n                    shader.push_str(\"  let col = wid.x; if (col >= params.ncols) { return; }\\n\");\n                    shader.push_str(&format!(\"  var acc: {scalar_ty} = {}0.0;\\n\", if scalar_ty==\"f64\" { \"f64(\" } else { \"\" }));\n                    if scalar_ty==\"f64\" { shader.push_str(\"  // close f64 literal\\n\"); }\n                    shader.push_str(\"  var r = lid.x;\\n  while (r < params.nrows) {\\n    let v = input0.data[(col * params.ld) + r];\\n    if (!isNan(v)) { acc = acc + v; }\\n    r += 256u;\\n  }\\n\");\n                }\n                shader.push_str(\"  var<workgroup> tile: array<f32, 256u>;\\n  tile[lid.x] = acc;\\n  workgroupBarrier();\\n\");\n                shader.push_str(\"  var off = 128u;\\n  loop { if (off == 0u) { break; } if (lid.x < off) {\\n    let a = tile[lid.x]; let b = tile[lid.x + off];\\n    tile[lid.x] = a + b;\\n  } workgroupBarrier(); off = off / 2u; }\\n\");\n                if axis_is_row {\n                    shader.push_str(\"  if (lid.x == 0u) { output.data[row] = tile[0u]; }\\n}\\n\");\n                } else {\n                    shader.push_str(\"  if (lid.x == 0u) { output.data[col] = tile[0u]; }\\n}\\n\");\n                }\n\n                if let Ok(device_result) = provider.fused_reduction(\n                    &shader,\n                    std::slice::from_ref(&handle),\n                    &output_shape,\n                    reduce_len,\n                    num_slices,\n                    256,\n                ) {\n                    return Ok(Value::GpuTensor(device_result));\n                }\n            }\n        }\n    }\n\n    let gathered = gpu_helpers::gather_tensor(&handle)?;\n    let fallback_dim = dim.unwrap_or_else(|| default_dimension(&gathered));\n    let reduced = reduce_tensor_dim(&gathered, fallback_dim, nan_mode)?;\n    Ok(tensor::tensor_into_value(reduced))\n}\n\nfn reduce_tensor_dim(\n    tensor: &Tensor,\n    dim: usize,\n    nan_mode: ReductionNaN,\n) -> Result<Tensor, String> {\n    if dim == 0 {\n        return Err(\"sum: dimension must be >= 1\".to_string());\n    }\n\n    if tensor.data.is_empty() {\n        if let Some(shape) = reduction_shape(&tensor.shape, dim) {\n            let zeros = vec![0.0; tensor::element_count(&shape)];\n            return Tensor::new(zeros, shape).map_err(|e| format!(\"sum: {e}\"));\n        } else {\n            return Ok(tensor.clone());\n        }\n    }\n\n    if tensor.shape.is_empty() {\n        let value = tensor.data[0];\n        let reduced = match nan_mode {\n            ReductionNaN::Include => value,\n            ReductionNaN::Omit => {\n                if value.is_nan() {\n                    0.0\n    } else {\n                    value\n                }\n            }\n        };\n        return Tensor::new(vec![reduced], vec![1, 1]).map_err(|e| format!(\"sum: {e}\"));\n    }\n\n    let Some(output_shape) = reduction_shape(&tensor.shape, dim) else {\n        return Ok(tensor.clone());\n    };\n\n    let dim_index = dim - 1;\n    let reduce_len = tensor.shape[dim_index];\n    let stride_before = dim_product(&tensor.shape[..dim_index]);\n    let stride_after = dim_product(&tensor.shape[dim..]);\n\n    let out_len = tensor::element_count(&output_shape);\n    let mut output = vec![0.0f64; out_len];\n\n    for after in 0..stride_after {\n        for before in 0..stride_before {\n            let mut sum = 0.0;\n            let mut any_value = false;\n            let mut saw_nan = false;\n\n            for k in 0..reduce_len {\n                let idx = before + k * stride_before + after * stride_before * reduce_len;\n                let value = tensor.data[idx];\n                match nan_mode {\n                    ReductionNaN::Include => {\n                        if value.is_nan() {\n                            saw_nan = true;\n                            break;\n                        }\n                        sum += value;\n                        any_value = true;\n                    }\n                    ReductionNaN::Omit => {\n                        if value.is_nan() {\n                            continue;\n                        }\n                        sum += value;\n                        any_value = true;\n                    }\n                }\n            }\n\n            let out_idx = after * stride_before + before;\n            output[out_idx] = match nan_mode {\n                ReductionNaN::Include => {\n                    if saw_nan {\n                        f64::NAN\n                    } else if any_value {\n                        sum\n                    } else {\n                        0.0\n                    }\n                }\n                ReductionNaN::Omit => {\n                    if any_value {\n                        sum\n            } else {\n                0.0\n            }\n        }\n            };\n        }\n    }\n\n    Tensor::new(output, output_shape).map_err(|e| format!(\"sum: {e}\"))\n}\n\nfn reduction_shape(shape: &[usize], dim: usize) -> Option<Vec<usize>> {\n    if dim == 0 {\n        return None;\n    }\n    if shape.is_empty() {\n        if dim == 1 {\n            return Some(vec![1, 1]);\n        }\n        return None;\n    }\n    if dim > shape.len() {\n        return None;\n    }\n    let mut out = shape.to_vec();\n    out[dim - 1] = 1;\n    Some(out)\n}\n\nfn dim_product(dims: &[usize]) -> usize {\n    dims.iter()\n        .copied()\n        .fold(1usize, |acc, v| acc.saturating_mul(v))\n}\n\nfn default_dimension(tensor: &Tensor) -> usize {\n    default_dimension_from_shape(&tensor.shape)\n}\n\nfn default_dimension_from_shape(shape: &[usize]) -> usize {\n    if shape.is_empty() {\n        return 1;\n    }\n    shape\n        .iter()\n        .position(|&extent| extent != 1)\n        .map(|idx| idx + 1)\n        .unwrap_or(1)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::builtins::common::test_support;\n    use runmat_builtins::IntValue;\n\n    #[test]\n    fn sum_scalar_num() {\n        let result = sum_builtin(Value::Num(5.0), Vec::new()).expect(\"sum\");\n        assert_eq!(result, Value::Num(5.0));\n    }\n\n    #[test]\n    fn sum_matrix_default_dimension() {\n        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n        let result = sum_builtin(Value::Tensor(tensor), Vec::new()).expect(\"sum\");\n        match result {\n            Value::Tensor(out) => {\n                assert_eq!(out.shape, vec![1, 3]);\n                assert_eq!(out.data, vec![5.0, 7.0, 9.0]);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sum_matrix_dimension_two() {\n        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n        let result =\n            sum_builtin(Value::Tensor(tensor), vec![Value::Int(IntValue::I32(2))]).expect(\"sum\");\n        match result {\n            Value::Tensor(out) => {\n                assert_eq!(out.shape, vec![2, 1]);\n                assert_eq!(out.data, vec![6.0, 15.0]);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sum_with_omit_nan_default_dimension() {\n        let tensor = Tensor::new(vec![1.0, f64::NAN, 3.0], vec![3, 1]).unwrap();\n        let result = sum_builtin(Value::Tensor(tensor), vec![Value::from(\"omitnan\")]).expect(\"sum\");\n        assert_eq!(result, Value::Num(4.0));\n    }\n\n    #[test]\n    fn sum_with_include_nan_propagates() {\n        let tensor = Tensor::new(vec![1.0, f64::NAN, 3.0], vec![3, 1]).unwrap();\n        let result = sum_builtin(Value::Tensor(tensor), Vec::new()).expect(\"sum\");\n        match result {\n            Value::Num(n) => assert!(n.is_nan()),\n            other => panic!(\"expected scalar NaN, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sum_dimension_greater_than_ndims_returns_input() {\n        let tensor = Tensor::new(vec![1.0, 2.0, 3.0], vec![3, 1]).unwrap();\n        let original = tensor.clone();\n        let value = Value::Tensor(tensor);\n        let result = sum_builtin(value, vec![Value::Int(IntValue::I32(5))]).expect(\"sum\");\n        match result {\n            Value::Tensor(out) => assert_eq!(out, original),\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sum_gpu_provider_roundtrip() {\n        test_support::with_test_provider(|provider| {\n            let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n            let view = runmat_accelerate_api::HostTensorView {\n                data: &tensor.data,\n                shape: &tensor.shape,\n            };\n            let handle = provider.upload(&view).expect(\"upload\");\n            let result = sum_builtin(Value::GpuTensor(handle), Vec::new()).expect(\"sum\");\n            let gathered = test_support::gather(result).expect(\"gather\");\n            assert_eq!(gathered.shape, vec![1, 3]);\n            assert_eq!(gathered.data, vec![5.0, 7.0, 9.0]);\n        });\n    }\n\n    #[test]\n    fn sum_gpu_omit_nan_falls_back_to_host() {\n        test_support::with_test_provider(|provider| {\n            let tensor = Tensor::new(vec![f64::NAN, 2.0, f64::NAN, 4.0], vec![2, 2]).unwrap();\n            let view = runmat_accelerate_api::HostTensorView {\n                data: &tensor.data,\n                shape: &tensor.shape,\n            };\n            let handle = provider.upload(&view).expect(\"upload\");\n            let result =\n                sum_builtin(Value::GpuTensor(handle), vec![Value::from(\"omitnan\")]).expect(\"sum\");\n            let gathered = test_support::gather(result).expect(\"gather\");\n            assert_eq!(gathered.shape, vec![1, 2]);\n            assert_eq!(gathered.data, vec![2.0, 4.0]);\n        });\n    }\n\n    #[test]\n    fn doc_examples_present() {\n        let blocks = test_support::doc_examples(DOC_MD);\n        assert!(!blocks.is_empty());\n    }\n}\n\n===== crates/runmat-runtime/src/builtins/math/reduction/mean.rs =====\n//! Column-major mean reduction builtin for RunMat.\n//!\n//! Provides MATLAB-compatible behaviour, including optional `'omitnan'`\n//! handling, GPU-aware execution via RunMat Accelerate, and fusion/GPU\n//! metadata for the native planner.\n\nuse runmat_accelerate_api::GpuTensorHandle;\nuse runmat_builtins::{Tensor, Value};\nuse runmat_macros::runtime_builtin;\n\nuse crate::builtins::common::spec::{\n    BroadcastSemantics, BuiltinFusionSpec, BuiltinGpuSpec, ConstantStrategy, GpuOpKind,\n    ProviderHook, ReductionNaN, ResidencyPolicy, ScalarType, ShapeRequirements,\n};\nuse crate::builtins::common::{gpu_helpers, tensor};\nuse crate::{register_builtin_fusion_spec, register_builtin_gpu_spec};\n\n#[cfg_attr(not(test), allow(dead_code))]\npub const DOC_MD: &str = r#\"---\ntitle: \"mean\"\ncategory: \"math/reduction\"\nkeywords: [\"mean\", \"average\", \"reduction\", \"gpu\", \"omitnan\"]\nsummary: \"Average elements of scalars, vectors, matrices, or N-D tensors.\"\nreferences: []\ngpu_support:\n  elementwise: false\n  reduction: true\n  precisions: [\"f32\", \"f64\"]\n  broadcasting: \"matlab\"\n  notes: \"Falls back to host whenever 'omitnan' is requested or the active provider lacks mean reductions.\"\nfusion:\n  elementwise: false\n  reduction: false\n  max_inputs: 1\n  constants: \"inline\"\nrequires_feature: null\ntested:\n  unit: \"builtins::math::reduction::mean::tests\"\n  integration: \"builtins::math::reduction::mean::tests::mean_gpu_provider_roundtrip\"\n---\n\n# MATLAB / RunMat `mean` Function\n`mean(x)` computes the arithmetic mean of scalars, vectors, matrices, and higher-dimensional tensors.\nWhen no dimension is supplied, the reduction runs along the first non-singleton dimension.\n\n## Behaviour\n- `mean(X)` on an `m × n` matrix returns a row vector (`1 × n`) with column averages.\n- `mean(X, 2)` returns a column vector (`m × 1`) containing row averages.\n- Logical inputs are promoted to double precision (`true → 1.0`, `false → 0.0`).\n- `mean(..., 'omitnan')` ignores `NaN` values; if every element in the slice is `NaN`, the result is `NaN`.\n- `mean(..., 'includenan')` (default) propagates `NaN` when any element in the slice is `NaN`.\n- Empty slices produce `NaN` outputs that follow MATLAB's shape semantics.\n- Dimensions larger than `ndims(X)` leave the input unchanged.\n\n## GPU Execution\nWhen RunMat Accelerate is active, tensors that already reside on the device stay on the GPU.\nProviders may implement `reduce_mean_dim` or `reduce_mean` for fast execution; otherwise RunMat\ntransparently gathers the data and falls back to the host implementation. `'omitnan'` always\nuses the host path today because providers do not yet accept the NaN policy.\n\n## Examples\n\n```matlab\nA = [1 2 3; 4 5 6];\ncolMeans = mean(A);      % [2.5 3.5 4.5]\nrowMeans = mean(A, 2);   % [2; 5]\n```\n\n```matlab\nvalues = [1 NaN 3];\navg = mean(values, 'omitnan');   % 2\n```\n\n```matlab\nG = gpuArray(rand(1024, 1024));\nenergy = mean(G .^ 2);\nresult = gather(energy);          % column means computed on the device when supported\n```\n\n## RunMat vs MATLAB behavior\n- Matches MATLAB for numeric, logical, empty, and `'omitnan'` cases, including propagation rules.\n- GPU execution is transparent: RunMat keeps tensors resident on the device and fuses upstream work when possible.\n- When providers do not expose mean reductions, RunMat falls back to host execution automatically.\n\n## Source & Feedback\n- Implementation: `crates/runmat-runtime/src/builtins/math/reduction/mean.rs`\n- Issues & feature requests: https://github.com/runmat-org/runmat/issues/new/choose\n\n## See Also\n[`sum`], [`median`], [`cumsum`], [`gpuArray`], [`gather`]\n\"#;\n\npub const GPU_SPEC: BuiltinGpuSpec = BuiltinGpuSpec {\n    name: \"mean\",\n    op_kind: GpuOpKind::Reduction,\n    supported_precisions: &[ScalarType::F32, ScalarType::F64],\n    broadcast: BroadcastSemantics::Matlab,\n    provider_hooks: &[\n        ProviderHook::Reduction { name: \"reduce_mean_dim\" },\n        ProviderHook::Reduction { name: \"reduce_mean\" },\n    ],\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    residency: ResidencyPolicy::NewHandle,\n    nan_mode: ReductionNaN::Include,\n    two_pass_threshold: Some(256),\n    workgroup_size: Some(256),\n    accepts_nan_mode: true,\n    notes: \"Providers can specialise mean reductions; omitnan currently falls back to the host.\",\n};\n\nregister_builtin_gpu_spec!(GPU_SPEC);\n\npub const FUSION_SPEC: BuiltinFusionSpec = BuiltinFusionSpec {\n    name: \"mean\",\n    shape: ShapeRequirements::BroadcastCompatible,\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    elementwise: None,\n    reduction: None,\n    emits_nan: true,\n    notes: \"Fusion fallback currently gathers to host; future kernels will divide the accumulated sum by slice size.\",\n};\n\nregister_builtin_fusion_spec!(FUSION_SPEC);\n\n#[runtime_builtin(\n    name = \"mean\",\n    category = \"math/reduction\",\n    summary = \"Average elements of scalars, vectors, matrices, or N-D tensors.\",\n    keywords = \"mean,average,reduction,gpu,omitnan\",\n    accel = \"reduction\"\n)]\nfn mean_builtin(value: Value, rest: Vec<Value>) -> Result<Value, String> {\n    let (dim, nan_mode) = parse_arguments(&rest)?;\n    match value {\n        Value::GpuTensor(handle) => mean_gpu(handle, dim, nan_mode),\n        other => mean_host(other, dim, nan_mode),\n    }\n}\n\nfn parse_arguments(args: &[Value]) -> Result<(Option<usize>, ReductionNaN), String> {\n    match args.len() {\n        0 => Ok((None, ReductionNaN::Include)),\n        1 => {\n            if let Some(mode) = parse_nan_mode(&args[0])? {\n                Ok((None, mode))\n            } else {\n                let dim = tensor::parse_dimension(&args[0], \"mean\")?;\n                Ok((Some(dim), ReductionNaN::Include))\n            }\n        }\n        2 => {\n            let dim = tensor::parse_dimension(&args[0], \"mean\")?;\n            if let Some(mode) = parse_nan_mode(&args[1])? {\n                Ok((Some(dim), mode))\n            } else {\n                Err(\"mean: expected 'omitnan' or 'includenan' as the third argument\".to_string())\n            }\n        }\n        _ => Err(\"mean: unsupported arguments\".to_string()),\n    }\n}\n\nfn parse_nan_mode(value: &Value) -> Result<Option<ReductionNaN>, String> {\n    let text = match value {\n        Value::String(s) => Some(s.clone()),\n        Value::StringArray(sa) if sa.data.len() == 1 => Some(sa.data[0].clone()),\n        Value::CharArray(ca) if ca.rows == 1 => Some(ca.data.iter().collect()),\n        _ => None,\n    };\n    let Some(text) = text else {\n        return Ok(None);\n    };\n    let trimmed = text.trim();\n    let lowered = trimmed.to_ascii_lowercase();\n    match lowered.as_str() {\n        \"omitnan\" => Ok(Some(ReductionNaN::Omit)),\n        \"includenan\" => Ok(Some(ReductionNaN::Include)),\n        _ => Err(format!(\"mean: unknown reduction mode '{trimmed}'\")),\n    }\n}\n\nfn mean_host(value: Value, dim: Option<usize>, nan_mode: ReductionNaN) -> Result<Value, String> {\n    let tensor = tensor::value_into_tensor_for(\"mean\", value)?;\n    let target_dim = dim.unwrap_or_else(|| default_dimension(&tensor));\n    let reduced = reduce_tensor_mean_dim(&tensor, target_dim, nan_mode)?;\n    Ok(tensor::tensor_into_value(reduced))\n}\n\nfn mean_gpu(\n    handle: GpuTensorHandle,\n    dim: Option<usize>,\n    nan_mode: ReductionNaN,\n) -> Result<Value, String> {\n    let target_dim = dim.unwrap_or_else(|| default_dimension_from_shape(&handle.shape));\n\n    if target_dim == 0 {\n        return Err(\"mean: dimension must be >= 1\".to_string());\n    }\n\n    let Some(target_shape) = reduction_shape(&handle.shape, target_dim) else {\n        return Ok(Value::GpuTensor(handle));\n    };\n\n    if nan_mode == ReductionNaN::Include {\n        if let Some(provider) = runmat_accelerate_api::provider() {\n            let zero_based = target_dim.saturating_sub(1);\n            if zero_based < handle.shape.len() {\n                if let Ok(device_result) = provider.reduce_mean_dim(&handle, zero_based) {\n                    return Ok(Value::GpuTensor(device_result));\n                }\n            }\n            if tensor::element_count(&target_shape) == 1 {\n                if let Ok(device_result) = provider.reduce_mean(&handle) {\n                    return Ok(Value::GpuTensor(device_result));\n                }\n            }\n        }\n    }\n\n    let gathered = gpu_helpers::gather_tensor(&handle)?;\n    let fallback_dim = dim.unwrap_or_else(|| default_dimension(&gathered));\n    let reduced = reduce_tensor_mean_dim(&gathered, fallback_dim, nan_mode)?;\n    Ok(tensor::tensor_into_value(reduced))\n}\n\nfn reduce_tensor_mean_dim(\n    tensor: &Tensor,\n    dim: usize,\n    nan_mode: ReductionNaN,\n) -> Result<Tensor, String> {\n    if dim == 0 {\n        return Err(\"mean: dimension must be >= 1\".to_string());\n    }\n\n    if tensor.shape.is_empty() {\n        let value = tensor.data.get(0).copied().unwrap_or(f64::NAN);\n        let result = match nan_mode {\n            ReductionNaN::Include => value,\n            ReductionNaN::Omit => {\n                if value.is_nan() {\n                    f64::NAN\n                } else {\n                    value\n                }\n            }\n        };\n        return Tensor::new(vec![result], vec![1, 1]).map_err(|e| format!(\"mean: {e}\"));\n    }\n\n    let Some(output_shape) = reduction_shape(&tensor.shape, dim) else {\n        return Ok(tensor.clone());\n    };\n\n    if tensor.data.is_empty() {\n        let fill = vec![f64::NAN; tensor::element_count(&output_shape)];\n        return Tensor::new(fill, output_shape).map_err(|e| format!(\"mean: {e}\"));\n    }\n\n    let dim_index = dim - 1;\n    let reduce_len = tensor.shape[dim_index];\n    let stride_before = dim_product(&tensor.shape[..dim_index]);\n    let stride_after = dim_product(&tensor.shape[dim..]);\n    let out_len = tensor::element_count(&output_shape);\n    let mut output = vec![0.0f64; out_len];\n\n    for after in 0..stride_after {\n        for before in 0..stride_before {\n            let mut sum = 0.0;\n            let mut count = 0usize;\n            let mut saw_nan = false;\n\n            for k in 0..reduce_len {\n                let idx = before + k * stride_before + after * stride_before * reduce_len;\n                let value = tensor.data[idx];\n                match nan_mode {\n                    ReductionNaN::Include => {\n                        if value.is_nan() {\n                            saw_nan = true;\n                            break;\n                        }\n                        sum += value;\n                    }\n                    ReductionNaN::Omit => {\n                        if value.is_nan() {\n                            continue;\n                        }\n                        sum += value;\n                        count += 1;\n                    }\n                }\n            }\n\n            let out_idx = after * stride_before + before;\n            output[out_idx] = match nan_mode {\n                ReductionNaN::Include => {\n                    if reduce_len == 0 || saw_nan {\n                        f64::NAN\n                    } else {\n                        sum / (reduce_len as f64)\n                    }\n                }\n                ReductionNaN::Omit => {\n                    if count == 0 {\n                        f64::NAN\n                    } else {\n                        sum / (count as f64)\n                    }\n                }\n            };\n        }\n    }\n\n    Tensor::new(output, output_shape).map_err(|e| format!(\"mean: {e}\"))\n}\n\nfn reduction_shape(shape: &[usize], dim: usize) -> Option<Vec<usize>> {\n    if dim == 0 {\n        return None;\n    }\n    if shape.is_empty() {\n        if dim == 1 {\n            return Some(vec![1, 1]);\n        }\n        return None;\n    }\n    if dim > shape.len() {\n        return None;\n    }\n    let mut out = shape.to_vec();\n    out[dim - 1] = 1;\n    Some(out)\n}\n\nfn dim_product(dims: &[usize]) -> usize {\n    dims.iter()\n        .copied()\n        .fold(1usize, |acc, v| acc.saturating_mul(v))\n}\n\nfn default_dimension(tensor: &Tensor) -> usize {\n    default_dimension_from_shape(&tensor.shape)\n}\n\nfn default_dimension_from_shape(shape: &[usize]) -> usize {\n    if shape.is_empty() {\n        return 1;\n    }\n    shape\n        .iter()\n        .position(|&extent| extent != 1)\n        .map(|idx| idx + 1)\n        .unwrap_or(1)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::builtins::common::test_support;\n    use runmat_builtins::IntValue;\n\n    #[test]\n    fn mean_scalar_num() {\n        let result = mean_builtin(Value::Num(6.0), Vec::new()).expect(\"mean\");\n        assert_eq!(result, Value::Num(6.0));\n    }\n\n    #[test]\n    fn mean_matrix_default_dimension() {\n        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n        let result = mean_builtin(Value::Tensor(tensor), Vec::new()).expect(\"mean\");\n        match result {\n            Value::Tensor(out) => {\n                assert_eq!(out.shape, vec![1, 3]);\n                assert_eq!(out.data, vec![2.5, 3.5, 4.5]);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_matrix_dimension_two() {\n        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n        let result =\n            mean_builtin(Value::Tensor(tensor), vec![Value::Int(IntValue::I32(2))]).expect(\"mean\");\n        match result {\n            Value::Tensor(out) => {\n                assert_eq!(out.shape, vec![2, 1]);\n                assert_eq!(out.data, vec![2.0, 5.0]);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_with_omit_nan_default_dimension() {\n        let tensor = Tensor::new(vec![1.0, f64::NAN, 5.0], vec![3, 1]).unwrap();\n        let result =\n            mean_builtin(Value::Tensor(tensor), vec![Value::from(\"omitnan\")]).expect(\"mean\");\n        match result {\n            Value::Num(v) => assert!((v - 3.0).abs() < 1e-12),\n            other => panic!(\"expected scalar result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_with_omit_nan_all_nan_returns_nan() {\n        let tensor = Tensor::new(vec![f64::NAN, f64::NAN], vec![2, 1]).unwrap();\n        let result =\n            mean_builtin(Value::Tensor(tensor), vec![Value::from(\"omitnan\")]).expect(\"mean\");\n        match result {\n            Value::Num(v) => assert!(v.is_nan()),\n            other => panic!(\"expected NaN result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_with_include_nan_propagates_nan() {\n        let tensor = Tensor::new(vec![1.0, f64::NAN, 3.0], vec![3, 1]).unwrap();\n        let result =\n            mean_builtin(Value::Tensor(tensor), vec![Value::from(\"includenan\")]).expect(\"mean\");\n        match result {\n            Value::Num(v) => assert!(v.is_nan()),\n            other => panic!(\"expected NaN result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_dimension_greater_than_ndims_returns_input() {\n        let tensor = Tensor::new(vec![1.0, 2.0, 3.0], vec![3, 1]).unwrap();\n        let original = tensor.clone();\n        let result =\n            mean_builtin(Value::Tensor(tensor), vec![Value::Int(IntValue::I32(5))]).expect(\"mean\");\n        match result {\n            Value::Tensor(out) => assert_eq!(out, original),\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_dimension_with_omit_nan() {\n        let tensor =\n            Tensor::new(vec![1.0, f64::NAN, 3.0, 4.0], vec![2, 2]).expect(\"tensor construction\");\n        let result = mean_builtin(\n            Value::Tensor(tensor),\n            vec![Value::Int(IntValue::I32(1)), Value::from(\"omitnan\")],\n        )\n        .expect(\"mean\");\n        match result {\n            Value::Tensor(out) => {\n                assert_eq!(out.shape, vec![1, 2]);\n                assert_eq!(out.data, vec![1.0, 3.5]);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn mean_gpu_provider_roundtrip() {\n        test_support::with_test_provider(|provider| {\n            let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();\n            let view = runmat_accelerate_api::HostTensorView {\n                data: &tensor.data,\n                shape: &tensor.shape,\n            };\n            let handle = provider.upload(&view).expect(\"upload\");\n            let result = mean_builtin(Value::GpuTensor(handle), Vec::new()).expect(\"mean\");\n            let gathered = test_support::gather(result).expect(\"gather\");\n            assert_eq!(gathered.shape, vec![1, 3]);\n            assert_eq!(gathered.data, vec![2.5, 3.5, 4.5]);\n        });\n    }\n\n    #[test]\n    fn mean_gpu_omit_nan_falls_back_to_host() {\n        test_support::with_test_provider(|provider| {\n            let tensor = Tensor::new(vec![f64::NAN, 2.0, f64::NAN, 4.0], vec![2, 2]).unwrap();\n            let view = runmat_accelerate_api::HostTensorView {\n                data: &tensor.data,\n                shape: &tensor.shape,\n            };\n            let handle = provider.upload(&view).expect(\"upload\");\n            let result =\n                mean_builtin(Value::GpuTensor(handle), vec![Value::from(\"omitnan\")]).expect(\"mean\");\n            let gathered = test_support::gather(result).expect(\"gather\");\n            assert_eq!(gathered.shape, vec![1, 2]);\n            assert_eq!(gathered.data, vec![2.0, 4.0]);\n        });\n    }\n\n    #[test]\n    fn doc_examples_present() {\n        let blocks = test_support::doc_examples(DOC_MD);\n        assert!(!blocks.is_empty());\n    }\n}\n\n===== crates/runmat-runtime/src/builtins/math/trigonometry/sin.rs =====\n//! Elementwise sine builtin for RunMat.\n//!\n//! Provides MATLAB-compatible behaviour for scalars, tensors, complex numbers,\n//! logical arrays, and GPU-resident tensors, including rich documentation and\n//! metadata for the Accelerate planner.\n\nuse runmat_accelerate_api::GpuTensorHandle;\nuse runmat_builtins::{CharArray, ComplexTensor, Tensor, Value};\nuse runmat_macros::runtime_builtin;\n\nuse crate::builtins::common::spec::{\n    BroadcastSemantics, BuiltinFusionSpec, BuiltinGpuSpec, ConstantStrategy, FusionError,\n    FusionExprContext, FusionKernelTemplate, GpuOpKind, ProviderHook, ReductionNaN,\n    ResidencyPolicy, ScalarType, ShapeRequirements,\n};\nuse crate::builtins::common::{gpu_helpers, tensor};\nuse crate::{register_builtin_fusion_spec, register_builtin_gpu_spec};\n\n#[cfg_attr(not(test), allow(dead_code))]\npub const DOC_MD: &str = r#\"---\ntitle: \"sin\"\ncategory: \"math/trigonometry\"\nkeywords: [\"sin\", \"sine\", \"trigonometry\", \"gpu\"]\nsummary: \"Sine of scalars, vectors, matrices, or N-D tensors (element-wise).\"\nreferences: []\ngpu_support:\n  elementwise: true\n  reduction: false\n  precisions: [\"f32\", \"f64\"]\n  broadcasting: \"matlab\"\n  notes: \"Falls back to the host implementation when the active provider lacks unary_sin.\"\nfusion:\n  elementwise: true\n  reduction: false\n  max_inputs: 1\n  constants: \"inline\"\nrequires_feature: null\ntested:\n  unit: \"builtins::math::trigonometry::sin::tests\"\n  integration: \"builtins::math::trigonometry::sin::tests::sin_gpu_provider_roundtrip\"\n---\n\n# MATLAB / RunMat `sin` Function\n`y = sin(x)` computes the sine of every element in `x`, with angles specified in radians.\n\n## Behaviour\n- Works on scalars, vectors, matrices, and N-D tensors with MATLAB broadcasting semantics.\n- Logical inputs are converted to double precision (`true → 1.0`, `false → 0.0`) before applying sine.\n- Complex inputs follow MATLAB's definition: `sin(a + bi) = sin(a)cosh(b) + i·cos(a)sinh(b)`.\n- Character arrays are treated as their numeric code points and return a double array of the same size.\n\n## GPU Execution\nWhen RunMat Accelerate is active, tensors that already reside on the GPU remain on the device.\nIf the selected provider implements `unary_sin`, the operation executes directly on the GPU. Otherwise,\nRunMat gathers the data back to the host and uses the CPU implementation automatically to preserve behaviour.\n\n## Examples\n\n```matlab\nx = linspace(0, 2*pi, 5);\ny = sin(x);\n```\n\n```matlab\nz = sin(1 + 2i);     % complex input\n```\n\n```matlab\ny = sin(pi/2);\n```\n\n## See Also\n[`cos`], [`tan`], [`gpuArray`], [`gather`]\n\"#;\n\npub const GPU_SPEC: BuiltinGpuSpec = BuiltinGpuSpec {\n    name: \"sin\",\n    op_kind: GpuOpKind::Elementwise,\n    supported_precisions: &[ScalarType::F32, ScalarType::F64],\n    broadcast: BroadcastSemantics::Matlab,\n    provider_hooks: &[ProviderHook::Unary { name: \"unary_sin\" }],\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    residency: ResidencyPolicy::NewHandle,\n    nan_mode: ReductionNaN::Include,\n    two_pass_threshold: None,\n    workgroup_size: None,\n    accepts_nan_mode: false,\n    notes:\n        \"Providers may execute sin in-place on the device; runtimes gather to host when unary_sin is unavailable.\",\n};\n\nregister_builtin_gpu_spec!(GPU_SPEC);\n\npub const FUSION_SPEC: BuiltinFusionSpec = BuiltinFusionSpec {\n    name: \"sin\",\n    shape: ShapeRequirements::BroadcastCompatible,\n    constant_strategy: ConstantStrategy::InlineLiteral,\n    elementwise: Some(FusionKernelTemplate {\n        scalar_precisions: &[ScalarType::F32, ScalarType::F64],\n        wgsl_body: |ctx: &FusionExprContext| {\n            let input = ctx.inputs.get(0).ok_or(FusionError::MissingInput(0))?;\n            Ok(format!(\"sin({input})\"))\n        },\n    }),\n    reduction: None,\n    emits_nan: false,\n    notes: \"Fusion planner emits WGSL `sin` calls; providers may override via fused elementwise kernels.\",\n};\n\nregister_builtin_fusion_spec!(FUSION_SPEC);\n\n#[runtime_builtin(\n    name = \"sin\",\n    category = \"math/trigonometry\",\n    summary = \"Sine of scalars, vectors, matrices, or N-D tensors (element-wise).\",\n    keywords = \"sin,sine,trigonometry,gpu\",\n    accel = \"unary\"\n)]\nfn sin_builtin(value: Value) -> Result<Value, String> {\n    match value {\n        Value::GpuTensor(handle) => sin_gpu(handle),\n        Value::Complex(re, im) => Ok(Value::Complex(sin_complex_re(re, im), sin_complex_im(re, im))),\n        Value::ComplexTensor(ct) => sin_complex_tensor(ct),\n        Value::CharArray(ca) => sin_char_array(ca),\n        Value::String(_) | Value::StringArray(_) => {\n            Err(\"sin: expected numeric input\".to_string())\n        }\n        other => sin_real(other),\n    }\n}\n\nfn sin_gpu(handle: GpuTensorHandle) -> Result<Value, String> {\n    if let Some(provider) = runmat_accelerate_api::provider() {\n        if let Ok(out) = provider.unary_sin(&handle) {\n            return Ok(Value::GpuTensor(out));\n        }\n    }\n    let tensor = gpu_helpers::gather_tensor(&handle)?;\n    sin_tensor(tensor).map(tensor::tensor_into_value)\n}\n\nfn sin_real(value: Value) -> Result<Value, String> {\n    let tensor = tensor::value_into_tensor_for(\"sin\", value)?;\n    sin_tensor(tensor).map(tensor::tensor_into_value)\n}\n\nfn sin_tensor(tensor: Tensor) -> Result<Tensor, String> {\n    let data = tensor.data.iter().map(|&v| v.sin()).collect::<Vec<_>>();\n    Tensor::new(data, tensor.shape.clone()).map_err(|e| format!(\"sin: {e}\"))\n}\n\nfn sin_complex_tensor(ct: ComplexTensor) -> Result<Value, String> {\n    let mapped = ct\n        .data\n        .iter()\n        .map(|&(re, im)| (sin_complex_re(re, im), sin_complex_im(re, im)))\n        .collect::<Vec<_>>();\n    let tensor =\n        ComplexTensor::new(mapped, ct.shape.clone()).map_err(|e| format!(\"sin: {e}\"))?;\n    Ok(Value::ComplexTensor(tensor))\n}\n\nfn sin_char_array(ca: CharArray) -> Result<Value, String> {\n    let data = ca\n        .data\n        .iter()\n        .map(|&ch| (ch as u32 as f64).sin())\n        .collect::<Vec<_>>();\n    let tensor =\n        Tensor::new(data, vec![ca.rows, ca.cols]).map_err(|e| format!(\"sin: {e}\"))?;\n    Ok(Value::Tensor(tensor))\n}\n\n#[inline]\nfn sin_complex_re(re: f64, im: f64) -> f64 {\n    re.sin() * im.cosh()\n}\n\n#[inline]\nfn sin_complex_im(re: f64, im: f64) -> f64 {\n    re.cos() * im.sinh()\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use runmat_builtins::{IntValue, Tensor};\n\n    use crate::builtins::common::test_support;\n\n    #[test]\n    fn sin_scalar() {\n        let value = Value::Num(std::f64::consts::PI / 2.0);\n        let result = sin_builtin(value).expect(\"sin\");\n        match result {\n            Value::Num(v) => assert!((v - 1.0).abs() < 1e-12),\n            other => panic!(\"expected scalar result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sin_tensor_elements() {\n        let tensor = Tensor::new(vec![0.0, std::f64::consts::PI], vec![2, 1]).unwrap();\n        let result = sin_builtin(Value::Tensor(tensor)).expect(\"sin\");\n        match result {\n            Value::Tensor(t) => {\n                assert_eq!(t.shape, vec![2, 1]);\n                assert!((t.data[0] - 0.0).abs() < 1e-12);\n                assert!((t.data[1] - 0.0).abs() < 1e-12);\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sin_int_value_promotes() {\n        let value = Value::Int(IntValue::I32(1));\n        let result = sin_builtin(value).expect(\"sin\");\n        match result {\n            Value::Num(v) => assert!((v - 1.0_f64.sin()).abs() < 1e-12),\n            other => panic!(\"expected scalar result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sin_complex_scalar() {\n        let result = sin_builtin(Value::Complex(1.0, 2.0)).expect(\"sin\");\n        match result {\n            Value::Complex(re, im) => {\n                assert!((re - (1.0f64.sin() * 2.0f64.cosh())).abs() < 1e-12);\n                assert!((im - (1.0f64.cos() * 2.0f64.sinh())).abs() < 1e-12);\n            }\n            other => panic!(\"expected complex result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sin_char_array_roundtrip() {\n        let chars = CharArray::new(\"abc\".chars().collect(), 1, 3).unwrap();\n        let result = sin_builtin(Value::CharArray(chars)).expect(\"sin\");\n        match result {\n            Value::Tensor(t) => {\n                assert_eq!(t.shape, vec![1, 3]);\n                for (idx, ch) in ['a', 'b', 'c'].into_iter().enumerate() {\n                    let expected = (ch as u32 as f64).sin();\n                    assert!((t.data[idx] - expected).abs() < 1e-12);\n                }\n            }\n            other => panic!(\"expected tensor result, got {other:?}\"),\n        }\n    }\n\n    #[test]\n    fn sin_gpu_provider_roundtrip() {\n        test_support::with_test_provider(|provider| {\n            let tensor = Tensor::new(vec![0.0, 1.0, 2.0, 3.0], vec![4, 1]).unwrap();\n            let view = runmat_accelerate_api::HostTensorView {\n                data: &tensor.data,\n                shape: &tensor.shape,\n            };\n            let handle = provider.upload(&view).expect(\"upload\");\n            let result = sin_builtin(Value::GpuTensor(handle)).expect(\"sin\");\n            let gathered = test_support::gather(result).expect(\"gather\");\n            let expected: Vec<f64> = tensor.data.iter().map(|&v| v.sin()).collect();\n            assert_eq!(gathered.shape, vec![4, 1]);\n            assert_eq!(gathered.data, expected);\n        });\n    }\n\n    #[test]\n    fn doc_examples_present() {\n        let blocks = test_support::doc_examples(DOC_MD);\n        assert!(!blocks.is_empty());\n    }\n}\n\n===== Task =====\n- Implement the `rand` builtin at `crates/runmat-runtime/src/builtins/array/rand.rs`.\n- Use the references above to follow the RunMat builtin template (DOC_MD, GPU/Fusion specs, tests).\n- Ensure MATLAB-compatible semantics and document GPU fallbacks when provider hooks are incomplete.\n\n===== Implementation Notes =====\n- Create a dedicated builtin module (DOC_MD, GPU/Fusion specs, runtime registration, helper routines, tests).\n- Mirror function semantics exactly to MATLAB, as this is a MATLAB compatible runtime; \n- raise MATLAB-compatible errors and keep GPU fallbacks in sync with host code.\n- Share helpers via builtins/common rather than referencing legacy modules.\n- Document GPU fallbacks and complete any provider hooks that are incomplete that are needed \n- for the function to work correctly and completely on the GPU.\n\n===== Testing Expectations =====\n- cargo test -p runmat-runtime --lib -- rand\n- cargo test -p runmat-runtime --tests -- rand\n- Include doc example smoke tests (test_support::doc_examples)\n",
  "doc_markdown": null,
  "source_paths": [
    "crates/runmat-accelerate/README.md",
    "crates/runmat-accelerate/src/fusion.rs",
    "crates/runmat-accelerate/src/lib.rs",
    "crates/runmat-accelerate/src/native_auto.rs",
    "crates/runmat-builtins/src/lib.rs",
    "crates/runmat-runtime/BUILTIN_PACKAGING.md",
    "crates/runmat-runtime/generation-plan-2.md",
    "crates/runmat-runtime/src/builtins/array/rand.rs",
    "crates/runmat-runtime/src/builtins/common/spec.rs",
    "crates/runmat-runtime/src/builtins/math/reduction/mean.rs",
    "crates/runmat-runtime/src/builtins/math/reduction/sum.rs",
    "crates/runmat-runtime/src/builtins/math/trigonometry/sin.rs",
    "docs/ARCHITECTURE.md",
    "docs/fusion-runtime-design.md"
  ],
  "codex_summary": "[codex stderr] Reading prompt from stdin...",
  "tests": [
    {
      "label": "cargo test (unit)",
      "status": "failed",
      "skipped": false,
      "note": null,
      "stdout_log": null,
      "stderr_log": "artifacts/runmatfunc/tests/rand/00_cargo_test__unit__stderr.log"
    }
  ],
  "test_log_dir": "artifacts/runmatfunc/tests/rand",
  "created_at": "2025-10-18T19:45:24.427033Z"
}