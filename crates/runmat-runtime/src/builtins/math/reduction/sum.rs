//! MATLAB-compatible `sum` builtin.
//!
//! This implementation follows the new builtin template described in
//! `crates/runmat-runtime/BUILTIN_PACKAGING.md`. It provides documentation,
//! GPU/fusion specifications, runtime registration, helper routines, and unit tests.

use crate::builtins::common::gpu_helpers;
use crate::builtins::common::spec::{
    BroadcastSemantics, BuiltinFusionSpec, BuiltinGpuSpec, ConstantStrategy, FusionError,
    FusionExprContext, FusionKernelTemplate, GpuOpKind, ProviderHook, ResidencyPolicy,
    ScalarType, ShapeRequirements,
};
use crate::builtins::common::tensor_ops;
use crate::{register_builtin_fusion_spec, register_builtin_gpu_spec};
use runmat_accelerate_api::GpuTensorHandle;
use runmat_builtins::{CharArray, ComplexTensor, Tensor, Value};
use runmat_macros::runtime_builtin;

#[cfg_attr(not(test))]
pub const DOC_MD: &str = r#"---
title: "sum"
category: "math/reduction"
keywords: ["sum", "reduction", "accumulate", "gpu"]
summary: "Sum elements of scalars, vectors, matrices, or N-D tensors."
references: []
gpu_support:
  elementwise: false
  reduction: true
  precisions: ["f32", "f64"]
  broadcasting: "matlab"
  notes: "Uses provider reduce_sum / reduce_sum_dim hooks when available, otherwise falls back to host execution."
fusion:
  elementwise: false
  reduction: true
  max_inputs: 1
  constants: "inline"
requires_feature: null
tested:
  unit: "builtins::math::reduction::sum::tests"
  integration: null
---

# RunMat `sum`
`sum(x)` adds the elements of `x` using MATLAB-compatible semantics:

- Scalars return the original value (converted to double precision when needed).
- Vectors reduce to a scalar.
- Matrices and higher-dimensional tensors reduce along the first non-singleton dimension unless a dimension is specified explicitly with `sum(x, dim)`.
- Logical arrays are treated as `double` with `true → 1.0` and `false → 0.0`.
- Complex inputs return complex outputs using linear accumulation of real and imaginary parts.

Empty inputs follow MATLAB behaviour: reductions over empty dimensions return zeros with matching shape semantics, while fully empty scalars yield `0.0`.

## GPU Execution
RunMat Accelerate routes reductions to the active acceleration provider via `reduce_sum` and `reduce_sum_dim`. When a provider does not implement these hooks, the builtin transparently gathers the data back to host memory and performs the reduction in software. This guarantees correctness while still enabling native acceleration on capable backends.

Fusion metadata marks `sum` as a reduction that can participate in fused kernels generated by the planner (for example, an elementwise chain feeding into `sum`).

## Examples
```matlab
A = [1 2 3; 4 5 6];
sum(A)      % [5 7 9]
sum(A, 2)   % [6; 15]
```

```matlab
values = [1, 2, 3, 4];
total = sum(values);           % 10
```

```matlab
flags = logical([1 0 1 1]);
matches = sum(flags);          % 3
```

```matlab
G = gpuArray(rand(1024, 1024));
energy = sum(G .^ 2);          % stays on the GPU when the provider supports reductions
result = gather(energy);
```

## Source
- Implementation: `crates/runmat-runtime/src/builtins/math/reduction/sum.rs`
- Tests: `cargo test -p runmat-runtime --lib -- sum`
```"#;

pub const GPU_SPEC: BuiltinGpuSpec = BuiltinGpuSpec {
    name: "sum",
    op_kind: GpuOpKind::Reduction,
    supported_precisions: &[ScalarType::F32, ScalarType::F64],
    broadcast: BroadcastSemantics::Matlab,
    provider_hooks: &[
        ProviderHook::Reduction { name: "reduce_sum" },
        ProviderHook::Reduction {
            name: "reduce_sum_dim",
        },
    ],
    constant_strategy: ConstantStrategy::InlineLiteral,
    residency: ResidencyPolicy::NewHandle,
    notes: "Backends should return freshly allocated buffers representing the reduced result.",
};

register_builtin_gpu_spec!(GPU_SPEC);

pub const FUSION_SPEC: BuiltinFusionSpec = BuiltinFusionSpec {
    name: "sum",
    shape: ShapeRequirements::BroadcastCompatible,
    constant_strategy: ConstantStrategy::InlineLiteral,
    elementwise: None,
    reduction: Some(FusionKernelTemplate {
        scalar_precisions: &[ScalarType::F32, ScalarType::F64],
        wgsl_body: |ctx: &FusionExprContext<'_>| {
            let input = ctx
                .inputs
                .get(0)
                .ok_or(FusionError::MissingInput(0))?;
            Ok(format!("accumulator += {input};"))
        },
    }),
    emits_nan: false,
    notes: "Planner emits standard WGSL reduction loops and accumulates in high precision.",
};

register_builtin_fusion_spec!(FUSION_SPEC);

#[runtime_builtin(
    name = "sum",
    category = "math/reduction",
    summary = "Sum elements of scalars, vectors, matrices, or N-D tensors.",
    keywords = "sum,reduction,gpu",
    accel = "reduction"
)]
fn sum_builtin(value: Value, rest: Vec<Value>) -> Result<Value, String> {
    if rest.is_empty() {
        return sum_default(value);
    }
    if rest.len() == 1 {
        let dim = parse_dimension(&rest[0])?;
        return sum_with_dim(value, dim);
    }
    Err("sum: unsupported arguments".to_string())
}

fn sum_default(value: Value) -> Result<Value, String> {
    match value {
        Value::Tensor(t) => sum_tensor_default(&t).and_then(tensor_to_value),
        Value::ComplexTensor(ct) => sum_complex_tensor_default(&ct).and_then(complex_tensor_to_value),
        Value::LogicalArray(array) => {
            let tensor = tensor_ops::logical_to_tensor(&array)?;
            sum_tensor_default(&tensor).and_then(tensor_to_value)
        }
        Value::CharArray(array) => {
            let tensor = char_array_to_tensor(&array)?;
            sum_tensor_default(&tensor).and_then(tensor_to_value)
        }
        Value::Num(n) => Ok(Value::Num(n)),
        Value::Int(i) => Ok(Value::Num(i.to_f64())),
        Value::Bool(b) => Ok(Value::Num(if b { 1.0 } else { 0.0 })),
        Value::Complex(re, im) => Ok(Value::Complex(re, im)),
        Value::GpuTensor(handle) => gpu_sum_all(handle),
        other => Err(format!("sum: expected numeric or logical input, got {other:?}")),
    }
}

fn sum_with_dim(value: Value, dim: usize) -> Result<Value, String> {
    match value {
        Value::Tensor(tensor) => sum_tensor_with_dim(&tensor, dim).and_then(tensor_to_value),
        Value::ComplexTensor(ct) => sum_complex_tensor_with_dim(&ct, dim).and_then(complex_tensor_to_value),
        Value::LogicalArray(array) => {
            let tensor = tensor_ops::logical_to_tensor(&array)?;
            sum_tensor_with_dim(&tensor, dim).and_then(tensor_to_value)
        }
        Value::CharArray(array) => {
            let tensor = char_array_to_tensor(&array)?;
            sum_tensor_with_dim(&tensor, dim).and_then(tensor_to_value)
        }
        Value::Num(n) => Ok(Value::Num(n)),
        Value::Int(i) => Ok(Value::Num(i.to_f64())),
        Value::Bool(b) => Ok(Value::Num(if b { 1.0 } else { 0.0 })),
        Value::Complex(re, im) => Ok(Value::Complex(re, im)),
        Value::GpuTensor(handle) => gpu_sum_dim(handle, dim),
        other => Err(format!("sum: expected numeric or logical input, got {other:?}")),
    }
}

fn sum_tensor_default(tensor: &Tensor) -> Result<Tensor, String> {
    if tensor_ops::is_effectively_scalar(&tensor.shape) {
        let sum = tensor_ops::sum_all(tensor);
        return Tensor::new(vec![sum], vec![1, 1]).map_err(|e| format!("sum: {e}"));
    }
    let axis = tensor_ops::default_reduction_axis(&tensor.shape);
    tensor_ops::sum_along_dim(tensor, axis)
}

fn sum_tensor_with_dim(tensor: &Tensor, dim: usize) -> Result<Tensor, String> {
    let axis = dim.saturating_sub(1);
    tensor_ops::sum_along_dim(tensor, axis)
}

fn sum_complex_tensor_default(tensor: &ComplexTensor) -> Result<ComplexTensor, String> {
    if tensor_ops::is_effectively_scalar(&tensor.shape) {
        let (re, im) = tensor_ops::sum_all_complex(tensor);
        return ComplexTensor::new(vec![(re, im)], vec![1, 1])
            .map_err(|e| format!("sum: {e}"));
    }
    let axis = tensor_ops::default_reduction_axis(&tensor.shape);
    tensor_ops::sum_along_dim_complex(tensor, axis)
}

fn sum_complex_tensor_with_dim(tensor: &ComplexTensor, dim: usize) -> Result<ComplexTensor, String> {
    let axis = dim.saturating_sub(1);
    tensor_ops::sum_along_dim_complex(tensor, axis)
}

fn tensor_to_value(tensor: Tensor) -> Result<Value, String> {
    if tensor.data.len() == 1 {
        Ok(Value::Num(tensor.data[0]))
    } else {
        Ok(Value::Tensor(tensor))
    }
}

fn complex_tensor_to_value(tensor: ComplexTensor) -> Result<Value, String> {
    if tensor.data.len() == 1 {
        let (re, im) = tensor.data[0];
        Ok(Value::Complex(re, im))
    } else {
        Ok(Value::ComplexTensor(tensor))
    }
}

fn char_array_to_tensor(array: &CharArray) -> Result<Tensor, String> {
    let data: Vec<f64> = array.data.iter().map(|ch| *ch as u32 as f64).collect();
    Tensor::new(data, vec![array.rows, array.cols]).map_err(|e| format!("sum: {e}"))
}

fn parse_dimension(value: &Value) -> Result<usize, String> {
    let numeric = match value {
        Value::Num(n) => *n,
        Value::Int(i) => i.to_f64(),
        Value::Tensor(t) if t.data.len() == 1 => t.data[0],
        Value::LogicalArray(a) if a.len() == 1 => {
            if a.data[0] != 0 {
                1.0
            } else {
                0.0
            }
        }
        Value::GpuTensor(_) => {
            return Err("sum: dimension argument must be a positive integer scalar".to_string())
        }
        Value::CharArray(c) if c.data.len() == 1 => c.data[0] as u32 as f64,
        _ => return Err("sum: dimension argument must be a positive integer scalar".to_string()),
    };
    if !numeric.is_finite() {
        return Err("sum: dimension argument must be a positive integer scalar".to_string());
    }
    let rounded = numeric.round();
    if (rounded - numeric).abs() > 1e-9 {
        return Err("sum: dimension argument must be a positive integer scalar".to_string());
    }
    if rounded < 1.0 {
        return Err("sum: dimension argument must be a positive integer scalar".to_string());
    }
    Ok(rounded as usize)
}

fn gpu_sum_all(handle: GpuTensorHandle) -> Result<Value, String> {
    let axis = tensor_ops::default_reduction_axis(&handle.shape);
    if axis >= handle.shape.len() {
        // Nothing to reduce (scalar or degenerate dimensions); return handle unchanged.
        return Ok(Value::GpuTensor(handle));
    }
    if let Some(provider) = runmat_accelerate_api::provider() {
        if let Ok(out) = provider.reduce_sum_dim(&handle, axis) {
            return Ok(Value::GpuTensor(out));
        }
        let reduced_shape = tensor_ops::shape_with_dim_replaced(&handle.shape, axis);
        if tensor_ops::is_effectively_scalar(&reduced_shape) {
            if let Ok(out) = provider.reduce_sum(&handle) {
                return Ok(Value::GpuTensor(out));
            }
        }
    }
    // Fallback to host execution when the provider does not support reduce_sum_dim.
    let gathered = gpu_helpers::gather_tensor(&handle)?;
    sum_tensor_default(&gathered).and_then(tensor_to_value)
}

fn gpu_sum_dim(handle: GpuTensorHandle, dim: usize) -> Result<Value, String> {
    let axis = dim.saturating_sub(1);
    if axis >= handle.shape.len() {
        return Ok(Value::GpuTensor(handle));
    }
    if let Some(provider) = runmat_accelerate_api::provider() {
        if let Ok(out) = provider.reduce_sum_dim(&handle, axis) {
            return Ok(Value::GpuTensor(out));
        }
    }
    // Fallback: gather tensor and execute reduction on host.
    let gathered = gpu_helpers::gather_tensor(&handle)?;
    sum_tensor_with_dim(&gathered, dim).and_then(tensor_to_value)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::builtins::common::test_support;
    use runmat_builtins::LogicalArray;

    #[test]
    fn sum_scalar_value() {
        let result = sum_builtin(Value::Num(5.0), Vec::new()).unwrap();
        assert_eq!(result, Value::Num(5.0));
    }

    #[test]
    fn sum_row_vector_defaults_to_scalar() {
        let tensor = Tensor::new(vec![1.0, 2.0, 3.0], vec![1, 3]).unwrap();
        let result = sum_builtin(Value::Tensor(tensor), Vec::new()).unwrap();
        assert_eq!(result, Value::Num(6.0));
    }

    #[test]
    fn sum_matrix_column_reduction() {
        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();
        let result = sum_builtin(Value::Tensor(tensor), Vec::new()).unwrap();
        match result {
            Value::Tensor(out) => {
                assert_eq!(out.shape, vec![1, 3]);
                assert_eq!(out.data, vec![5.0, 7.0, 9.0]);
            }
            other => panic!("expected tensor result, got {other:?}"),
        }
    }

    #[test]
    fn sum_matrix_row_reduction_dim_two() {
        let tensor = Tensor::new(vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0], vec![2, 3]).unwrap();
        let result =
            sum_builtin(Value::Tensor(tensor), vec![Value::Num(2.0)]).unwrap();
        match result {
            Value::Tensor(out) => {
                assert_eq!(out.shape, vec![2, 1]);
                assert_eq!(out.data, vec![6.0, 15.0]);
            }
            other => panic!("expected tensor result, got {other:?}"),
        }
    }

    #[test]
    fn sum_logical_array_counts_true_values() {
        let logical =
            LogicalArray::new(vec![1, 0, 1, 1], vec![2, 2]).unwrap();
        let result = sum_builtin(Value::LogicalArray(logical), Vec::new()).unwrap();
        match result {
            Value::Tensor(out) => {
                assert_eq!(out.shape, vec![1, 2]);
                assert_eq!(out.data, vec![1.0, 2.0]);
            }
            other => panic!("expected tensor result, got {other:?}"),
        }
    }

    #[test]
    fn sum_gpu_round_trip() {
        test_support::with_test_provider(|provider| {
            let tensor = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], vec![4, 1]).unwrap();
            let handle = provider.upload_tensor(&tensor).unwrap();
            let result = sum_builtin(Value::GpuTensor(handle), Vec::new()).unwrap();
            let gathered = test_support::gather(result).unwrap();
            assert_eq!(gathered.shape, vec![1, 1]);
            assert_eq!(gathered.data, vec![10.0]);
        });
    }

    #[test]
    fn sum_dimension_validation() {
        let tensor = Tensor::new(vec![1.0, 2.0, 3.0], vec![3, 1]).unwrap();
        let err = sum_builtin(
            Value::Tensor(tensor),
            vec![Value::String("two".to_string())],
        )
        .unwrap_err();
        assert!(
            err.contains("positive integer scalar"),
            "unexpected error message: {err}"
        );
    }

    #[test]
    fn doc_examples_are_registered() {
        test_support::doc_examples(DOC_MD).unwrap();
    }
}
